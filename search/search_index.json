{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Credit: Randall Munroe (xkcd.com/2054) Welcome to Environmental Incentives' Metrics Service Line Wiki! This wiki is intended for all EI staff to better understand the services of the Metrics service line and for Metrics service line experts as an important standard reference. Don't build a giant house of cards! We can help you develop solutions that last. All EI Staff \u00b6 If you're here to better understand how the Metrics service line can support your project or internal initiative, you might want to check out our portfolio of projects , better understand how we can work with you , or review one of these helpful introductions to technologies and services available to you: Data Analysis Tool Development Databases Visualization Mapping and Spatial Analysis Metrics Service Line Experts \u00b6 Metrics service line experts will want to familiarize themselves with all of the content here. This site provides helpful training resources, captures best practices for developing metrics products, and describes our standard consulting process for internal and external clients. If you will be contributing to the development of this site, please reference this guidance . The Brown Bag \u00b6 If you missed the brown bag introduction to this wiki, check it out here .","title":"Home"},{"location":"#all-ei-staff","text":"If you're here to better understand how the Metrics service line can support your project or internal initiative, you might want to check out our portfolio of projects , better understand how we can work with you , or review one of these helpful introductions to technologies and services available to you: Data Analysis Tool Development Databases Visualization Mapping and Spatial Analysis","title":"All EI Staff"},{"location":"#metrics-service-line-experts","text":"Metrics service line experts will want to familiarize themselves with all of the content here. This site provides helpful training resources, captures best practices for developing metrics products, and describes our standard consulting process for internal and external clients. If you will be contributing to the development of this site, please reference this guidance .","title":"Metrics Service Line Experts"},{"location":"#the-brown-bag","text":"If you missed the brown bag introduction to this wiki, check it out here .","title":"The Brown Bag"},{"location":"under-construction/","text":"We're getting to it... \u00b6","title":"We're getting to it..."},{"location":"under-construction/#were-getting-to-it","text":"","title":"We're getting to it..."},{"location":"additional-resources/interview-guide/","text":"Interview Guide \u00b6 An editable interview guide is available here ; download a version of this before proceeding. Talking to real people will help us better understand our potential users and the problems they face in their efforts to meet our conservation targets. This guide is intended to help you plan and facilitate the interview. There are two sections to the interview: (1) understanding the persona and (2) understanding their problems. Within each section, a table with \u2018question topics\u2019 and \u2018example questions\u2019 is provided. Edit the example questions within each question topic prior to the interview. Use these questions as a guide, but feel free to improvise or skip questions as you see fit. Discussion during the interview should relate to our area of interest (conservation targets), but should not be leading. People are generally agreeable\u2014if you ask whether they think pollinators are important, they will probably say yes. Better to ask what their priorities are and see if they mention the problems or solutions you're testing. Towards the end of the interview, you may ask more leading questions to see why they did or did not mention our area of interest or value proposition in their earlier responses. Before conducting the interview, you should (1) develop a persona based on available data sources and (2) brainstorm the types of problems they would face in relation to your program scope, vision or conservation targets. To ensure your personas are well-developed, make sure they meet the following criteria: Real \u2013 based on actual evidence (secondary research, interviews, observations, collected data) Exact \u2013 includes sufficient detail that the persona \u2018feels\u2019 like a real person Actionable \u2013 represents people that would actually use the MBHE Clear \u2013 described well enough to be understood by others on the team Testable \u2013 evidence can be collected to substantiate and improve the persona over time Fill out the table in the interview guide for the persona to be interviewed to ensure your persona is ready and to brainstorm specific questions to ask.","title":"Interview Guide"},{"location":"additional-resources/interview-guide/#interview-guide","text":"An editable interview guide is available here ; download a version of this before proceeding. Talking to real people will help us better understand our potential users and the problems they face in their efforts to meet our conservation targets. This guide is intended to help you plan and facilitate the interview. There are two sections to the interview: (1) understanding the persona and (2) understanding their problems. Within each section, a table with \u2018question topics\u2019 and \u2018example questions\u2019 is provided. Edit the example questions within each question topic prior to the interview. Use these questions as a guide, but feel free to improvise or skip questions as you see fit. Discussion during the interview should relate to our area of interest (conservation targets), but should not be leading. People are generally agreeable\u2014if you ask whether they think pollinators are important, they will probably say yes. Better to ask what their priorities are and see if they mention the problems or solutions you're testing. Towards the end of the interview, you may ask more leading questions to see why they did or did not mention our area of interest or value proposition in their earlier responses. Before conducting the interview, you should (1) develop a persona based on available data sources and (2) brainstorm the types of problems they would face in relation to your program scope, vision or conservation targets. To ensure your personas are well-developed, make sure they meet the following criteria: Real \u2013 based on actual evidence (secondary research, interviews, observations, collected data) Exact \u2013 includes sufficient detail that the persona \u2018feels\u2019 like a real person Actionable \u2013 represents people that would actually use the MBHE Clear \u2013 described well enough to be understood by others on the team Testable \u2013 evidence can be collected to substantiate and improve the persona over time Fill out the table in the interview guide for the persona to be interviewed to ensure your persona is ready and to brainstorm specific questions to ask.","title":"Interview Guide"},{"location":"additional-resources/persona-guide/","text":"Persona Development Guide \u00b6 Personas include (1) a name, (2) a photo, (3) a screening question that will help distinguish those that fit the persona and those that don\u2019t and (4) a description of relevant motivations and constraints. You can use a template or simply write down one or two paragraphs. The persona profile should fit on a single page or PowerPoint slide. The goal is to personalize the stakeholder so you understand their perspective and allow you to advocate on their behalf while developing solutions. Here is a quick and simple process: Start by dumping a list of all the personas you think might be relevant. List them by name, such as \u2018Fred the Farmer\u2019. Ultimately, you\u2019ll need to focus on the ones that are most relevant (you can\u2019t make everyone happy), but for now more is more*. Next, rank the personas and group redundant ones. Who do you need to appeal to first to achieve your vision? Who is most likely to help or hurt your cause? Draft screening questions for each priority persona. This is one sentence that \u2018defines\u2019 who the persona is and allows you to determine if someone fits the bill (e.g., do you depend entirely on farm income for your livelihood?) Write down at least 5 people that fit the persona. You don\u2019t have to know these people, but they must exist. Use one of their photos for the persona profile. Now spend less than 30 minutes per persona to start a draft. You will find that you may be very ignorant and stereotypical at this point\u2014that\u2019s fine, overcoming that is what this exercise is all about! Next you will research and refine your personas. * Sometimes, it's best to use a different pattern for categorizing personas instead of people, which can carry connotations that you don't mean to include. For example, categorize them by their shoes (wingtips, steel toes, and tevas), where they go on vacation (snow birds, summer in france, timeshares), or something else relevant to your project. Example \u00b6 This example was used in our effort to encourage farmers to participate in a cost-share conservation project (the Monarch Habitat Exchange). Acknowledgements \u00b6 This guide was adapted from Alex Cowan's excellent guide, which you can find here .","title":"Persona Development Guide"},{"location":"additional-resources/persona-guide/#persona-development-guide","text":"Personas include (1) a name, (2) a photo, (3) a screening question that will help distinguish those that fit the persona and those that don\u2019t and (4) a description of relevant motivations and constraints. You can use a template or simply write down one or two paragraphs. The persona profile should fit on a single page or PowerPoint slide. The goal is to personalize the stakeholder so you understand their perspective and allow you to advocate on their behalf while developing solutions. Here is a quick and simple process: Start by dumping a list of all the personas you think might be relevant. List them by name, such as \u2018Fred the Farmer\u2019. Ultimately, you\u2019ll need to focus on the ones that are most relevant (you can\u2019t make everyone happy), but for now more is more*. Next, rank the personas and group redundant ones. Who do you need to appeal to first to achieve your vision? Who is most likely to help or hurt your cause? Draft screening questions for each priority persona. This is one sentence that \u2018defines\u2019 who the persona is and allows you to determine if someone fits the bill (e.g., do you depend entirely on farm income for your livelihood?) Write down at least 5 people that fit the persona. You don\u2019t have to know these people, but they must exist. Use one of their photos for the persona profile. Now spend less than 30 minutes per persona to start a draft. You will find that you may be very ignorant and stereotypical at this point\u2014that\u2019s fine, overcoming that is what this exercise is all about! Next you will research and refine your personas. * Sometimes, it's best to use a different pattern for categorizing personas instead of people, which can carry connotations that you don't mean to include. For example, categorize them by their shoes (wingtips, steel toes, and tevas), where they go on vacation (snow birds, summer in france, timeshares), or something else relevant to your project.","title":"Persona Development Guide"},{"location":"additional-resources/persona-guide/#example","text":"This example was used in our effort to encourage farmers to participate in a cost-share conservation project (the Monarch Habitat Exchange).","title":"Example"},{"location":"additional-resources/persona-guide/#acknowledgements","text":"This guide was adapted from Alex Cowan's excellent guide, which you can find here .","title":"Acknowledgements"},{"location":"additional-resources/product-definition/","text":"Product Definition \u00b6 An editable version of the Product Definition is available here . Download a copy before continuing. Product definition is the process of documenting the proposed solution in broad terms. The purpose of a product definition is to align expectations, formulate broad concepts, clarify roles, and establish a development timeline. In the next step, Product Specification , you will have a chance to provide more specifics. At the product definition stage, you still have considerable flexibility in how you will deliver the final solution. Use the guidance below to complete the product definition. The product definition for this website is available here as an example. Solution Description \u00b6 Provide a 1-2 sentence description of the proposed solution. If you'd like, include a positioning statement: For [User Name], who wants/needs to [statement of need or opportunity], this [product category] will [primary value proposition]. Unlike [next best alternative], this tool [primary difference]. Goal & Objectives \u00b6 Goal \u00b6 The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully. Objectives \u00b6 Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives Users \u00b6 Primary \u00b6 Primary users are the subset of your boundary partners who will directly interact with the tool or solution. Think of these like your customers. You might want to group users with personas . For each primary user identified, complete the table below. If you've already completed the scoping exercise , you will have a starting point. Provide specific examples of each user group and describe their perspective to help put yourself in their shoes. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Add a row to the table for each problem scenario. Secondary \u00b6 Secondary users don't directly use the tool or solution, but are affected by it. They may provide data inputs or make decisions from the outputs. Repeat the process you went through for primary users. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Uses \u00b6 Uses describe the conditions under which the tool is used and the purpose for using the tool in that case. They should be fairly broad and together encompass the list of uses you envision for the tool or solution. If you're familiar with user stories, you can think of a use case as an 'epic' user story. During product specification you will enumerate more specific user stories to motivate development of features. However, if it's helpful, you can frame use cases like user stories as \"As user X, I want to Y so I can Z.\" This helps you think through each use case (Y) as specific to a user (X) who is motivated to achieve some outcome (Z). Workflow or Product Sketch \u00b6 Describe in general terms how the tool will integrate with existing workflows or create new workflows (be careful in creating new workflows, without sufficient motivation, it's unlikely your users will fully adopt the tool). A journey map can be particularly effective in visualizing the users' workflow without the tool (before) and with the tool (after). Visualizing can really help you avoid pitfalls and identify necessary features. Be sure to jot down any insights you had. You can move the sketch to the back if breaks up the flow of the document too much. Related Products \u00b6 List any products that will integrate with, inform, or be informed by this product, especially products that produce data used by the tool, products that receive data from the tool, or products to be replaced by the tool. You'll probably have identified a few of these products in the workflow section above, here is your chance to be thorough. Add links to existing products if possible for easy reference. Considerations \u00b6 List design principles and constraints. Consider integration, hosting, support, adaptive management and deployment but don't get into details yet. Your client may have constraints related to software licensing, budget, etc. Listing them here will make sure you don't come back with a solution that isn't feasible. Conditions of Satisfaction \u00b6 Set expectations for the tool by clarifying how you will know this tool is successful. Approach \u00b6 Provide a general description of the proposed work plan. Discuss roles, timelines, and resources required.","title":"Product Definition"},{"location":"additional-resources/product-definition/#product-definition","text":"An editable version of the Product Definition is available here . Download a copy before continuing. Product definition is the process of documenting the proposed solution in broad terms. The purpose of a product definition is to align expectations, formulate broad concepts, clarify roles, and establish a development timeline. In the next step, Product Specification , you will have a chance to provide more specifics. At the product definition stage, you still have considerable flexibility in how you will deliver the final solution. Use the guidance below to complete the product definition. The product definition for this website is available here as an example.","title":"Product Definition"},{"location":"additional-resources/product-definition/#solution-description","text":"Provide a 1-2 sentence description of the proposed solution. If you'd like, include a positioning statement: For [User Name], who wants/needs to [statement of need or opportunity], this [product category] will [primary value proposition]. Unlike [next best alternative], this tool [primary difference].","title":"Solution Description"},{"location":"additional-resources/product-definition/#goal-objectives","text":"","title":"Goal &amp; Objectives"},{"location":"additional-resources/product-definition/#goal","text":"The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully.","title":"Goal"},{"location":"additional-resources/product-definition/#objectives","text":"Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives","title":"Objectives"},{"location":"additional-resources/product-definition/#users","text":"","title":"Users"},{"location":"additional-resources/product-definition/#primary","text":"Primary users are the subset of your boundary partners who will directly interact with the tool or solution. Think of these like your customers. You might want to group users with personas . For each primary user identified, complete the table below. If you've already completed the scoping exercise , you will have a starting point. Provide specific examples of each user group and describe their perspective to help put yourself in their shoes. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Add a row to the table for each problem scenario.","title":"Primary"},{"location":"additional-resources/product-definition/#secondary","text":"Secondary users don't directly use the tool or solution, but are affected by it. They may provide data inputs or make decisions from the outputs. Repeat the process you went through for primary users. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better?","title":"Secondary"},{"location":"additional-resources/product-definition/#uses","text":"Uses describe the conditions under which the tool is used and the purpose for using the tool in that case. They should be fairly broad and together encompass the list of uses you envision for the tool or solution. If you're familiar with user stories, you can think of a use case as an 'epic' user story. During product specification you will enumerate more specific user stories to motivate development of features. However, if it's helpful, you can frame use cases like user stories as \"As user X, I want to Y so I can Z.\" This helps you think through each use case (Y) as specific to a user (X) who is motivated to achieve some outcome (Z).","title":"Uses"},{"location":"additional-resources/product-definition/#workflow-or-product-sketch","text":"Describe in general terms how the tool will integrate with existing workflows or create new workflows (be careful in creating new workflows, without sufficient motivation, it's unlikely your users will fully adopt the tool). A journey map can be particularly effective in visualizing the users' workflow without the tool (before) and with the tool (after). Visualizing can really help you avoid pitfalls and identify necessary features. Be sure to jot down any insights you had. You can move the sketch to the back if breaks up the flow of the document too much.","title":"Workflow or Product Sketch"},{"location":"additional-resources/product-definition/#related-products","text":"List any products that will integrate with, inform, or be informed by this product, especially products that produce data used by the tool, products that receive data from the tool, or products to be replaced by the tool. You'll probably have identified a few of these products in the workflow section above, here is your chance to be thorough. Add links to existing products if possible for easy reference.","title":"Related Products"},{"location":"additional-resources/product-definition/#considerations","text":"List design principles and constraints. Consider integration, hosting, support, adaptive management and deployment but don't get into details yet. Your client may have constraints related to software licensing, budget, etc. Listing them here will make sure you don't come back with a solution that isn't feasible.","title":"Considerations"},{"location":"additional-resources/product-definition/#conditions-of-satisfaction","text":"Set expectations for the tool by clarifying how you will know this tool is successful.","title":"Conditions of Satisfaction"},{"location":"additional-resources/product-definition/#approach","text":"Provide a general description of the proposed work plan. Discuss roles, timelines, and resources required.","title":"Approach"},{"location":"additional-resources/scoping/","text":"Scoping Canvas \u00b6 This scoping exercise will help you better understand the problem context and kick-start the process of identifying solutions. This is best done together in person, if possible, and typically requires at least an hour but can take half a day or more, depending on how invested you are in building out the Situation Model (feel free to sub in a systems map or journey map if preferred). If you've already developed some of these elements, great! Bring along a copy and start with a quick review. Find a large whiteboard and divide it up into sections as illustrated below (don't label each box as large as shown below, you'll need room to write). You can also find an editable version of this scoping canvas here . Instructions for filling in each section are provided below. Instructions \u00b6 Work through the sections in approximately this order, but don't hesitate to jump around if that's where the conversation flows. Scope \u00b6 A project\u2019s scope defines what the project intends to affect. \u201cPlace-based\u201d projects have a geographic scope and include efforts to conserve or effectively manage ecoregions, priority areas, or protected areas. \u201cThematic-based\u201d projects include efforts to address specific conservation targets, threats, opportunities, or enabling conditions and generally have a corresponding thematic scope. Thematic-based projects may also define a geographic scope that spatially describes a project area and might reference specific elements of biodiversity or a specific threat. (CMP 2013). I recommend erring on the inclusive side here. You will continue to get more specific throughout this process, for now try to represent the entire problem space. The scope should include the scope for the program, not just the specific tactic, tool, or feature you're considering. Vision \u00b6 In addition to defining the scope, it is also necessary to decide on a clear and common vision \u2013 a description of the desired state or ultimate condition that you are working to achieve. Your vision can be summarized in a vision statement, which meets the criteria of being relatively general, visionary, and brief. (CMP 2013) It's important to be agnostic about the solution within the vision statement. The vision is not for everyone to be using the tool you're thinking about developing. Just describe the long-term outcomes that you're trying to achieve in sufficient detail that if you time-traveled to a time in which your strategy was successful, you'd know you were there. See Step 1: Start at the End of our metrics design philosophy for help if you get stuck. Map It! \u00b6 Create a visual representation of the problem space. A Situation Model is a great option, which consists of one or more conservation targets, threats, and drivers. Interventions should be mapped where appropriate. However, you can do a systems map, journey map, or whatever approach you choose. See Step 2: Map It of our metrics design philosophy for inspiration. The components of a Situation Model are defined below: Conservation Targets \u00b6 Conservation targets are specific species or ecological systems/habitats that are chosen to represent and encompass the full suite of biodiversity in the project area for place-based conservation or the focus of a thematic program (CMP 2013). If you've already been through the process of developing a situation model, you should have a list of conservation targets to draw from. You may also already know which target is the focus of today's effort. For expediency, you can focus on the conservation target you are, well, targeting. If you're not sure, include them all. Place the conservation targets on the far right side of the Situation Model diagram. Threats \u00b6 Direct threats are primarily human activities that immediately degrade a conservation target (e.g., unsustainable fishing, unsustainable hunting, oil drilling, construction of roads, industrial wastewater, or introduction of exotic invasive species), but they can be natural phenomena altered by human activities (e.g., increase in extreme storm events or increased evaporation due to global climate change) or in rare cases, natural phenomena whose impact is increased by other human activities (e.g., a potential tsunami that threatens the last remaining population of an Asian rhino). (CMP 2013) Feel free to list all threats, but what we really need is the threat that is mapped to the key intervention point that strategy will attack. If you haven't chosen a key intervention point yet, go ahead and build out the entire thing. If you have, just build out the relevant parts. List threats vertically to the left of the conservation target . Drivers \u00b6 From the Conservation Measures Partnership, drivers are: key factors that drive the direct threats and ultimately influence your conservation targets. These include indirect threats (also known as root causes and drivers), opportunities, and enabling conditions (CMP 2013). If you already have a situation model, just copy over the relevant bits. Otherwise, list the key drivers and make connections where there are relationships. Place drivers in boxes, mapped to their relevant threats, and group like drivers. Draw lines to connect related drivers. Leave a bit of room for interventions. Interventions \u00b6 Finally, map the primary existing interventions (e.g., strategies or tactics employed by boundary partners or strategic partners in your problem space). These are typically captured in hexagonal polygons. Connect them to the relevant threats or drivers. Take a step back and determine which driver you would like to intervene on. This is your key intervention point. Circle the key intervention point(s) (or leverage point or pain point, etc.). This is where your group has chosen to focus, either with the entire program or with this specific tactic, tool, or feature. Stakeholders \u00b6 Stakeholders include everyone that is involved in the problem space or might be affected by your selected intervention. That may be a lot of people, which is why we'll focus in on Boundary Partners here. List stakeholders and underline boundary partners. Boundary partners are people or organizations you can directly influence. Stakeholders you can't directly influence, but can work with directly, are called Strategic Partners . You might put a star next to their name to distinguish them. Feel free to group stakeholders to maintain a manageable list, but at the same time identify an actual human being who would fit in that group. If a boundary partner is described as 'permitting staff at state agencies', find a real human being in a permitting department at a state in your geographic scope as an example. If you can't do that, your boundary partner probably doesn't actually exist. See Step 3: Develop Empathy of our metrics design philosophy for a lot more detail on this process. Problems \u00b6 If you're coming from the Open Standards for the Practice of Conservation, everything so far has been very familiar. This is where we take a bit of a departure. Instead of jumping straight to listing strategies, we're going to think about how we can solve problems or create opportunities for the people whose behavior we need to change to achieve our conservation goal. Our strategy will be shaped by our understanding of these problems (but we actually won't define a specific strategy until the Product Definition step). Identify the most important boundary partner in your list. Ask yourself, what do we need them to do (or to stop doing) to achieve our conservation goal? In other words, what behaviors do we need them to exhibit in order that our vision is achieved? You might frame these as behaviors you'd 'expect to see', 'like to see', and 'love to see'. Now, write down the problems (or jobs to be done) that your boundary partners face in doing what you need them to do . Maybe they don't know where to get technical information, they don't have time to do it, they can't identify the priority areas to work, etc. If you - when you - talk to them about your proposed solution, they should relate to these problem statements - if not, these are not real problems (or jobs to be done) and solving them won't change their behavior. See Step 4: Define the Problem for helpful guidance on this prompt. Alternatives \u00b6 From the list of problems you've identified, have any of these problems been solved already? You may seek to solve them more expediently, which is great, but knowing what the alternatives are will allow you to evaluate your proposed solution against a real baseline. List the currently existing alternative solutions. How Might We? \u00b6 Now you can start to brainstorm solutions. \"How Might We?\" is a way of framing design questions that help you convert your problem statements into opportunities. If, for example, your boundary partners don't know where to find key information, how might we get that information to them? List the 'How Might We?' questions that address the problems you think will be most impactful for your boundary partners. Again, you'll take the time to craft your solutions during the Product Definition step. This will just give you a head start. Considerations/Constraints \u00b6 As your thinking about problems and potential solutions, you'll probably identify a few constraints on what you're able to do to solve the problem, or just some things to keep in mind going forward. Jot down any considerations and/or constraints so you don't forget. Research Questions \u00b6 This space is for recording questions that you don't have the answer to...yet. It might help to have an expert available during the scoping meeting so you don't get hung up on a question, but for those you can't address during the scoping meeting, write down the research questions and plan to get it answered before developing the Product Definition. References \u00b6 CMP 2013","title":"Scoping Canvas"},{"location":"additional-resources/scoping/#scoping-canvas","text":"This scoping exercise will help you better understand the problem context and kick-start the process of identifying solutions. This is best done together in person, if possible, and typically requires at least an hour but can take half a day or more, depending on how invested you are in building out the Situation Model (feel free to sub in a systems map or journey map if preferred). If you've already developed some of these elements, great! Bring along a copy and start with a quick review. Find a large whiteboard and divide it up into sections as illustrated below (don't label each box as large as shown below, you'll need room to write). You can also find an editable version of this scoping canvas here . Instructions for filling in each section are provided below.","title":"Scoping Canvas"},{"location":"additional-resources/scoping/#instructions","text":"Work through the sections in approximately this order, but don't hesitate to jump around if that's where the conversation flows.","title":"Instructions"},{"location":"additional-resources/scoping/#scope","text":"A project\u2019s scope defines what the project intends to affect. \u201cPlace-based\u201d projects have a geographic scope and include efforts to conserve or effectively manage ecoregions, priority areas, or protected areas. \u201cThematic-based\u201d projects include efforts to address specific conservation targets, threats, opportunities, or enabling conditions and generally have a corresponding thematic scope. Thematic-based projects may also define a geographic scope that spatially describes a project area and might reference specific elements of biodiversity or a specific threat. (CMP 2013). I recommend erring on the inclusive side here. You will continue to get more specific throughout this process, for now try to represent the entire problem space. The scope should include the scope for the program, not just the specific tactic, tool, or feature you're considering.","title":"Scope"},{"location":"additional-resources/scoping/#vision","text":"In addition to defining the scope, it is also necessary to decide on a clear and common vision \u2013 a description of the desired state or ultimate condition that you are working to achieve. Your vision can be summarized in a vision statement, which meets the criteria of being relatively general, visionary, and brief. (CMP 2013) It's important to be agnostic about the solution within the vision statement. The vision is not for everyone to be using the tool you're thinking about developing. Just describe the long-term outcomes that you're trying to achieve in sufficient detail that if you time-traveled to a time in which your strategy was successful, you'd know you were there. See Step 1: Start at the End of our metrics design philosophy for help if you get stuck.","title":"Vision"},{"location":"additional-resources/scoping/#map-it","text":"Create a visual representation of the problem space. A Situation Model is a great option, which consists of one or more conservation targets, threats, and drivers. Interventions should be mapped where appropriate. However, you can do a systems map, journey map, or whatever approach you choose. See Step 2: Map It of our metrics design philosophy for inspiration. The components of a Situation Model are defined below:","title":"Map It!"},{"location":"additional-resources/scoping/#conservation-targets","text":"Conservation targets are specific species or ecological systems/habitats that are chosen to represent and encompass the full suite of biodiversity in the project area for place-based conservation or the focus of a thematic program (CMP 2013). If you've already been through the process of developing a situation model, you should have a list of conservation targets to draw from. You may also already know which target is the focus of today's effort. For expediency, you can focus on the conservation target you are, well, targeting. If you're not sure, include them all. Place the conservation targets on the far right side of the Situation Model diagram.","title":"Conservation Targets"},{"location":"additional-resources/scoping/#threats","text":"Direct threats are primarily human activities that immediately degrade a conservation target (e.g., unsustainable fishing, unsustainable hunting, oil drilling, construction of roads, industrial wastewater, or introduction of exotic invasive species), but they can be natural phenomena altered by human activities (e.g., increase in extreme storm events or increased evaporation due to global climate change) or in rare cases, natural phenomena whose impact is increased by other human activities (e.g., a potential tsunami that threatens the last remaining population of an Asian rhino). (CMP 2013) Feel free to list all threats, but what we really need is the threat that is mapped to the key intervention point that strategy will attack. If you haven't chosen a key intervention point yet, go ahead and build out the entire thing. If you have, just build out the relevant parts. List threats vertically to the left of the conservation target .","title":"Threats"},{"location":"additional-resources/scoping/#drivers","text":"From the Conservation Measures Partnership, drivers are: key factors that drive the direct threats and ultimately influence your conservation targets. These include indirect threats (also known as root causes and drivers), opportunities, and enabling conditions (CMP 2013). If you already have a situation model, just copy over the relevant bits. Otherwise, list the key drivers and make connections where there are relationships. Place drivers in boxes, mapped to their relevant threats, and group like drivers. Draw lines to connect related drivers. Leave a bit of room for interventions.","title":"Drivers"},{"location":"additional-resources/scoping/#interventions","text":"Finally, map the primary existing interventions (e.g., strategies or tactics employed by boundary partners or strategic partners in your problem space). These are typically captured in hexagonal polygons. Connect them to the relevant threats or drivers. Take a step back and determine which driver you would like to intervene on. This is your key intervention point. Circle the key intervention point(s) (or leverage point or pain point, etc.). This is where your group has chosen to focus, either with the entire program or with this specific tactic, tool, or feature.","title":"Interventions"},{"location":"additional-resources/scoping/#stakeholders","text":"Stakeholders include everyone that is involved in the problem space or might be affected by your selected intervention. That may be a lot of people, which is why we'll focus in on Boundary Partners here. List stakeholders and underline boundary partners. Boundary partners are people or organizations you can directly influence. Stakeholders you can't directly influence, but can work with directly, are called Strategic Partners . You might put a star next to their name to distinguish them. Feel free to group stakeholders to maintain a manageable list, but at the same time identify an actual human being who would fit in that group. If a boundary partner is described as 'permitting staff at state agencies', find a real human being in a permitting department at a state in your geographic scope as an example. If you can't do that, your boundary partner probably doesn't actually exist. See Step 3: Develop Empathy of our metrics design philosophy for a lot more detail on this process.","title":"Stakeholders"},{"location":"additional-resources/scoping/#problems","text":"If you're coming from the Open Standards for the Practice of Conservation, everything so far has been very familiar. This is where we take a bit of a departure. Instead of jumping straight to listing strategies, we're going to think about how we can solve problems or create opportunities for the people whose behavior we need to change to achieve our conservation goal. Our strategy will be shaped by our understanding of these problems (but we actually won't define a specific strategy until the Product Definition step). Identify the most important boundary partner in your list. Ask yourself, what do we need them to do (or to stop doing) to achieve our conservation goal? In other words, what behaviors do we need them to exhibit in order that our vision is achieved? You might frame these as behaviors you'd 'expect to see', 'like to see', and 'love to see'. Now, write down the problems (or jobs to be done) that your boundary partners face in doing what you need them to do . Maybe they don't know where to get technical information, they don't have time to do it, they can't identify the priority areas to work, etc. If you - when you - talk to them about your proposed solution, they should relate to these problem statements - if not, these are not real problems (or jobs to be done) and solving them won't change their behavior. See Step 4: Define the Problem for helpful guidance on this prompt.","title":"Problems"},{"location":"additional-resources/scoping/#alternatives","text":"From the list of problems you've identified, have any of these problems been solved already? You may seek to solve them more expediently, which is great, but knowing what the alternatives are will allow you to evaluate your proposed solution against a real baseline. List the currently existing alternative solutions.","title":"Alternatives"},{"location":"additional-resources/scoping/#how-might-we","text":"Now you can start to brainstorm solutions. \"How Might We?\" is a way of framing design questions that help you convert your problem statements into opportunities. If, for example, your boundary partners don't know where to find key information, how might we get that information to them? List the 'How Might We?' questions that address the problems you think will be most impactful for your boundary partners. Again, you'll take the time to craft your solutions during the Product Definition step. This will just give you a head start.","title":"How Might We?"},{"location":"additional-resources/scoping/#considerationsconstraints","text":"As your thinking about problems and potential solutions, you'll probably identify a few constraints on what you're able to do to solve the problem, or just some things to keep in mind going forward. Jot down any considerations and/or constraints so you don't forget.","title":"Considerations/Constraints"},{"location":"additional-resources/scoping/#research-questions","text":"This space is for recording questions that you don't have the answer to...yet. It might help to have an expert available during the scoping meeting so you don't get hung up on a question, but for those you can't address during the scoping meeting, write down the research questions and plan to get it answered before developing the Product Definition.","title":"Research Questions"},{"location":"additional-resources/scoping/#references","text":"CMP 2013","title":"References"},{"location":"additional-resources/specification-doc/","text":"Specification Document \u00b6 The Specification document is the primary supporting documentation for most tools. Use this document whenever the tool will require greater than 60 hours of development time or is expected to be long-lived and widely-used. The Specification document replaces and expands both the Scoping exercise and the Product Definition. You may be able to copy/paste some content from each, but consider taking the time to improve this content. A template Specification Document is available here (page 3). Download a copy before continuing. Solution Description \u00b6 Describe the solution in broad terms. This section introduces the tool and allows the reader to develop an understanding of what the tool is and how it will be used. Context \u00b6 Provide useful background information, including why the solution is required and in what setting the solution will be used. This section is useful in understanding the motivation for building the tool at the time it was built. Scope \u00b6 Describe which types of problems the tool is intended to solve, and which it is not. Is this tool applicable in one or many business units? Is it internal or client facing? Clarity on the scope will help maintain focus during development. Goals & Objectives \u00b6 Revise the goals and objectives from the Product Definition. Goal \u00b6 The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully. Objectives \u00b6 Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives Intended Users & User Research \u00b6 Use this section to provide persona descriptions and user hypotheses. Describe how the user hypotheses were or will be tested. Evidence of Need/Opportunity \u00b6 Use this section to describe the problem scenarios, current alternatives, and the value hypotheses. Describe how the value hypotheses were or will be tested. Product Description \u00b6 This section should provide succinct and clear descriptions of the various elements of the product, from the user interface to the technology it is build upon. Intended Uses \u00b6 List and describe the user stories that will serve as the basis for tool development. Be exhaustive in scope but don't worry about being too specific for each story. Requirements \u00b6 Describe both the user requirements (e.g., skills, training, ability to access sensitive information) and the software requirements for using the tool. Workflow \u00b6 Describe how the tool will fit into existing or create new workflows. A before/after Journey Map is recommended. User Interface \u00b6 Provide mockups or wireframes of the user interface. Describe any important style guidance. Architecture \u00b6 Describe the suite of tools that will combine to create the desired functionality of this tool. Include any related products. Quality Assurance/Quality Control \u00b6 Describe both the testing approach used during development and the requirements for ongoing quality assurance once the tool is in use. Limitations \u00b6 Be upfront about any limitations of the tool. Governance \u00b6 Who is in charge of the tool? How can changes be made? Governance is important both for naming the party responsible for the tool and preventing unwanted changes or derivations of the tool. Product Support \u00b6 This section outlines the plan for onboarding and supporting users of the tool as well as maintenance and end-of-life plans. Stakeholder Engagement Plan \u00b6 Developing a solution does not guarantee that intended users will use the solution. Both primary users and stakeholders (secondary users) must be engaged to promote awareness of the product, learn how to use the product, and tailor the product to meet their needs. Provide a high level description of the plan to keep users engaged. Communication Plan \u00b6 Describe how you will make users aware of the tool. Consider using multiple communication channels and audience-specific tactics. The table below provides a framework for quickly building a communication strategy. Communication Plan Table \u00b6 Objective Audience Tactic Lead Timeframe Example: Promote awareness of the product Example: internal USAID Example: Email blast to key audience Example: Erik Anderson Example: Week of final production Dissemination Plan \u00b6 Describe where the tool will be made available to users and how users will receive the tool directly. If relevant, describe how users will access updates to the tool. Adoption Plan \u00b6 Adoption occurs after users are onboarded, increase engagement, and realize sufficient benefits from using the tool to fully adopt it. Onboarding is the first step towards adoption. Engagement entails more substantial or more frequent use of the tool. Adoption describes when the user is using the tool for most if not all of the intended use cases (i.e., not using other tools or workarounds). The table below will help you walk through this process and test where users might drop off from the adoption curve. Guidance for each section is provided below. Desired behavior: Describe what you want your user to do at each phase of adoption. Make sure it is an observable behavior. Timeframe: Describe how long the user will be in each adoption phase. For example, behaviors associated with engagement might happen after one month of using the tool. Test: Describe how you can test whether users are or are not exhibiting the desired behavior. Metrics: Describe the indicators that will be measured to provide evidence of whether, and to what extent, users are exhibiting the desired behavior. For example, a web app may measure number of site visits. Red flags: Describe what to watch out for that might indicate users are falling off of the adoption curve at each phase. Incentives: Describe any strategies for increasing adoption or mitigating red flags. Adoption Table \u00b6 Onboarding Engagement Adoption Desired behavior Timeframe Tests Metrics Red flags Incentives User Support \u00b6 Describe the plan for providing guidance and technical support. What documentation or guidance will be provided to support users? Who will the user reach out to if there is a problem? How will they know who to reach out to? Primary technical support means support provided directly to the user. This often includes coaching and help with less common use cases. Secondary technical support is needed when the problem cannot be addressed by the primary technical support provider. This typically includes bug fixes and new features. Maintenance & End-of-Life \u00b6 Describe when, how and why the tool will be updated or new features will be built. Consider what will be required if the technology underlying the tool is updated or sunsetted. Describe the end-of-life plan. How long will the tool be maintained? Under what set of conditions will the tool be retired? What will the tool be replaced by? Future Directions \u00b6 This section provides an opportunity to contemplate how the tool might adapt over time and capture any stretch goals that were not achieved during development. This section will be useful when returning to the tool for updates later on.","title":"Product Specification"},{"location":"additional-resources/specification-doc/#specification-document","text":"The Specification document is the primary supporting documentation for most tools. Use this document whenever the tool will require greater than 60 hours of development time or is expected to be long-lived and widely-used. The Specification document replaces and expands both the Scoping exercise and the Product Definition. You may be able to copy/paste some content from each, but consider taking the time to improve this content. A template Specification Document is available here (page 3). Download a copy before continuing.","title":"Specification Document"},{"location":"additional-resources/specification-doc/#solution-description","text":"Describe the solution in broad terms. This section introduces the tool and allows the reader to develop an understanding of what the tool is and how it will be used.","title":"Solution Description"},{"location":"additional-resources/specification-doc/#context","text":"Provide useful background information, including why the solution is required and in what setting the solution will be used. This section is useful in understanding the motivation for building the tool at the time it was built.","title":"Context"},{"location":"additional-resources/specification-doc/#scope","text":"Describe which types of problems the tool is intended to solve, and which it is not. Is this tool applicable in one or many business units? Is it internal or client facing? Clarity on the scope will help maintain focus during development.","title":"Scope"},{"location":"additional-resources/specification-doc/#goals-objectives","text":"Revise the goals and objectives from the Product Definition.","title":"Goals &amp; Objectives"},{"location":"additional-resources/specification-doc/#goal","text":"The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully.","title":"Goal"},{"location":"additional-resources/specification-doc/#objectives","text":"Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives","title":"Objectives"},{"location":"additional-resources/specification-doc/#intended-users-user-research","text":"Use this section to provide persona descriptions and user hypotheses. Describe how the user hypotheses were or will be tested.","title":"Intended Users &amp; User Research"},{"location":"additional-resources/specification-doc/#evidence-of-needopportunity","text":"Use this section to describe the problem scenarios, current alternatives, and the value hypotheses. Describe how the value hypotheses were or will be tested.","title":"Evidence of Need/Opportunity"},{"location":"additional-resources/specification-doc/#product-description","text":"This section should provide succinct and clear descriptions of the various elements of the product, from the user interface to the technology it is build upon.","title":"Product Description"},{"location":"additional-resources/specification-doc/#intended-uses","text":"List and describe the user stories that will serve as the basis for tool development. Be exhaustive in scope but don't worry about being too specific for each story.","title":"Intended Uses"},{"location":"additional-resources/specification-doc/#requirements","text":"Describe both the user requirements (e.g., skills, training, ability to access sensitive information) and the software requirements for using the tool.","title":"Requirements"},{"location":"additional-resources/specification-doc/#workflow","text":"Describe how the tool will fit into existing or create new workflows. A before/after Journey Map is recommended.","title":"Workflow"},{"location":"additional-resources/specification-doc/#user-interface","text":"Provide mockups or wireframes of the user interface. Describe any important style guidance.","title":"User Interface"},{"location":"additional-resources/specification-doc/#architecture","text":"Describe the suite of tools that will combine to create the desired functionality of this tool. Include any related products.","title":"Architecture"},{"location":"additional-resources/specification-doc/#quality-assurancequality-control","text":"Describe both the testing approach used during development and the requirements for ongoing quality assurance once the tool is in use.","title":"Quality Assurance/Quality Control"},{"location":"additional-resources/specification-doc/#limitations","text":"Be upfront about any limitations of the tool.","title":"Limitations"},{"location":"additional-resources/specification-doc/#governance","text":"Who is in charge of the tool? How can changes be made? Governance is important both for naming the party responsible for the tool and preventing unwanted changes or derivations of the tool.","title":"Governance"},{"location":"additional-resources/specification-doc/#product-support","text":"This section outlines the plan for onboarding and supporting users of the tool as well as maintenance and end-of-life plans.","title":"Product Support"},{"location":"additional-resources/specification-doc/#stakeholder-engagement-plan","text":"Developing a solution does not guarantee that intended users will use the solution. Both primary users and stakeholders (secondary users) must be engaged to promote awareness of the product, learn how to use the product, and tailor the product to meet their needs. Provide a high level description of the plan to keep users engaged.","title":"Stakeholder Engagement Plan"},{"location":"additional-resources/specification-doc/#communication-plan","text":"Describe how you will make users aware of the tool. Consider using multiple communication channels and audience-specific tactics. The table below provides a framework for quickly building a communication strategy.","title":"Communication Plan"},{"location":"additional-resources/specification-doc/#communication-plan-table","text":"Objective Audience Tactic Lead Timeframe Example: Promote awareness of the product Example: internal USAID Example: Email blast to key audience Example: Erik Anderson Example: Week of final production","title":"Communication Plan Table"},{"location":"additional-resources/specification-doc/#dissemination-plan","text":"Describe where the tool will be made available to users and how users will receive the tool directly. If relevant, describe how users will access updates to the tool.","title":"Dissemination Plan"},{"location":"additional-resources/specification-doc/#adoption-plan","text":"Adoption occurs after users are onboarded, increase engagement, and realize sufficient benefits from using the tool to fully adopt it. Onboarding is the first step towards adoption. Engagement entails more substantial or more frequent use of the tool. Adoption describes when the user is using the tool for most if not all of the intended use cases (i.e., not using other tools or workarounds). The table below will help you walk through this process and test where users might drop off from the adoption curve. Guidance for each section is provided below. Desired behavior: Describe what you want your user to do at each phase of adoption. Make sure it is an observable behavior. Timeframe: Describe how long the user will be in each adoption phase. For example, behaviors associated with engagement might happen after one month of using the tool. Test: Describe how you can test whether users are or are not exhibiting the desired behavior. Metrics: Describe the indicators that will be measured to provide evidence of whether, and to what extent, users are exhibiting the desired behavior. For example, a web app may measure number of site visits. Red flags: Describe what to watch out for that might indicate users are falling off of the adoption curve at each phase. Incentives: Describe any strategies for increasing adoption or mitigating red flags.","title":"Adoption Plan"},{"location":"additional-resources/specification-doc/#adoption-table","text":"Onboarding Engagement Adoption Desired behavior Timeframe Tests Metrics Red flags Incentives","title":"Adoption Table"},{"location":"additional-resources/specification-doc/#user-support","text":"Describe the plan for providing guidance and technical support. What documentation or guidance will be provided to support users? Who will the user reach out to if there is a problem? How will they know who to reach out to? Primary technical support means support provided directly to the user. This often includes coaching and help with less common use cases. Secondary technical support is needed when the problem cannot be addressed by the primary technical support provider. This typically includes bug fixes and new features.","title":"User Support"},{"location":"additional-resources/specification-doc/#maintenance-end-of-life","text":"Describe when, how and why the tool will be updated or new features will be built. Consider what will be required if the technology underlying the tool is updated or sunsetted. Describe the end-of-life plan. How long will the tool be maintained? Under what set of conditions will the tool be retired? What will the tool be replaced by?","title":"Maintenance &amp; End-of-Life"},{"location":"additional-resources/specification-doc/#future-directions","text":"This section provides an opportunity to contemplate how the tool might adapt over time and capture any stretch goals that were not achieved during development. This section will be useful when returning to the tool for updates later on.","title":"Future Directions"},{"location":"additional-resources/user-testing-guide/","text":"User Testing Guide \u00b6 User testing is essential to designing and building a solution that solves a real problem for a real person. This testing guide draws primarily from Alexander Cowan's Customer Discovery Handbook . Four types of user testing \u00b6 We define four types of user testing Persona hypothesis testing Problem hypothesis testing Value hypothesis testing Usability hypothesis testing What is hypothesis testing? \u00b6 We craft hypotheses to test so that we can better define useful tests. A hypothesis is just your educated guess about the world. Specifically, a guess that is testable. A hypothesis can be crafted as an \"if...then\" statement, but we'll allow any statement that can be proven false. Persona hypothesis testing \u00b6 Does our user exist and do we understand them? Make an educated guess about how they'll behave related to your problem/solution and test if it is true. If yes, you have evidence you understand them. If no, figure out what you're missing. You should also do discovery that is not necessarily \"hypotheses testing\" (e.g., talk to people, observe people in the wild). Problem hypothesis testing \u00b6 Does our user have the problem we think they do? Is it a priority problem? How do they currently solve it? You can't just ask your subject if they have the problem you think they do, because people are generally agreeable and will likely say 'yes'. Be more creative about confirming that they do have this problem and that it is a sufficient pain point to warrant a change in behavior (i.e., adoption of your solution). Make sure these problem hypotheses are falsifiable. Value hypothesis testing \u00b6 Can we create sufficient value to drive adoption of our solution? Value hypotheses should be stated as 'if...then' statements, generally \"If we do something for a specific user segment they will behave in a certain way.\" At this point, you won't have built your solution, so you'll need a creative way to test your value hypotheses. You can use technology (e.g., build a quick Google Site as example) or magic (e.g., physically do the thing you plan to automate) to see if you're on the right track. This is also a good point to enumerate all of the assumptions in your Product Hypothesis (combination of persona, problem and value hypotheses). Unpack these assumptions, define which ones still need testing, and test them. Usability testing \u00b6 Can our user actually use elements of our solution to solve specific objectives? Usability testing will be tied to user stories (i.e., specific descriptions of problems to be solved or jobs to be done that are the focus of your solution). Usability testing often involves observing a user attempt to accomplish an objective using your solution. Bonus points if you can validate that they would use your solution instead of the next best alternative. We divide usability testing into three phases: exploratory, assessment, and validation. Exploratory \u00b6 Use low-fidelity prototypes to A/B test alternative solution implementations. Which one delivers a better user experience? Use the results to decide on a design direction. Assessment \u00b6 Determine if users can meet topline objectives with your proposed solution. Use the results to continue refining your design or pivot to a new solution. Validation \u00b6 Generally validation occurs after release of your solution. Define and track metrics that help you continue to refine the solution or detect issues. Usability Testing Process \u00b6 Before usability assessment testing, you should have good evidence to support your persona, problem and value hypotheses. Now you want to know if the solution you are building will deliver value against a real problem for a real user. Before testing, answer these four questions What are we testing? (Usability hypothesis) How will we test it? (The test) How will we know we passed? (Grading; often includes metrics and acceptance criteria) What will we do with the test results, good or bad? (Adaptation) Testing Outline \u00b6 Usability testing (especially exploratory and assessment testing) will often take place with one or more users. Use the outline below to develop a testing script that all testers can use to maintain consistency. You want to make sure you have the right user, take a moment to confirm (or better understand) their characteristics and problems, conduct the test, gather qualitative feedback, and finally capture takeaways. Validation question: are you who we think you are? Discovery Do you have these characteristics (confirm persona hypotheses)? Do you have this problem (confirm problem hypotheses)? How do you solve it now (what is the next best alternative)? Testing Overview and instructions Test Example: Give the subject a goal (related to a problem hypothesis) and watch them attempt to solve it Measure: track the metrics that were identified in your grading plan User feedback: an opportunity for the user to share their qualitative experience (optional but helpful) Capture takeaways Persona and problem hypotheses UI and user stores Other notes","title":"User Testing Guide"},{"location":"additional-resources/user-testing-guide/#user-testing-guide","text":"User testing is essential to designing and building a solution that solves a real problem for a real person. This testing guide draws primarily from Alexander Cowan's Customer Discovery Handbook .","title":"User Testing Guide"},{"location":"additional-resources/user-testing-guide/#four-types-of-user-testing","text":"We define four types of user testing Persona hypothesis testing Problem hypothesis testing Value hypothesis testing Usability hypothesis testing","title":"Four types of user testing"},{"location":"additional-resources/user-testing-guide/#what-is-hypothesis-testing","text":"We craft hypotheses to test so that we can better define useful tests. A hypothesis is just your educated guess about the world. Specifically, a guess that is testable. A hypothesis can be crafted as an \"if...then\" statement, but we'll allow any statement that can be proven false.","title":"What is hypothesis testing?"},{"location":"additional-resources/user-testing-guide/#persona-hypothesis-testing","text":"Does our user exist and do we understand them? Make an educated guess about how they'll behave related to your problem/solution and test if it is true. If yes, you have evidence you understand them. If no, figure out what you're missing. You should also do discovery that is not necessarily \"hypotheses testing\" (e.g., talk to people, observe people in the wild).","title":"Persona hypothesis testing"},{"location":"additional-resources/user-testing-guide/#problem-hypothesis-testing","text":"Does our user have the problem we think they do? Is it a priority problem? How do they currently solve it? You can't just ask your subject if they have the problem you think they do, because people are generally agreeable and will likely say 'yes'. Be more creative about confirming that they do have this problem and that it is a sufficient pain point to warrant a change in behavior (i.e., adoption of your solution). Make sure these problem hypotheses are falsifiable.","title":"Problem hypothesis testing"},{"location":"additional-resources/user-testing-guide/#value-hypothesis-testing","text":"Can we create sufficient value to drive adoption of our solution? Value hypotheses should be stated as 'if...then' statements, generally \"If we do something for a specific user segment they will behave in a certain way.\" At this point, you won't have built your solution, so you'll need a creative way to test your value hypotheses. You can use technology (e.g., build a quick Google Site as example) or magic (e.g., physically do the thing you plan to automate) to see if you're on the right track. This is also a good point to enumerate all of the assumptions in your Product Hypothesis (combination of persona, problem and value hypotheses). Unpack these assumptions, define which ones still need testing, and test them.","title":"Value hypothesis testing"},{"location":"additional-resources/user-testing-guide/#usability-testing","text":"Can our user actually use elements of our solution to solve specific objectives? Usability testing will be tied to user stories (i.e., specific descriptions of problems to be solved or jobs to be done that are the focus of your solution). Usability testing often involves observing a user attempt to accomplish an objective using your solution. Bonus points if you can validate that they would use your solution instead of the next best alternative. We divide usability testing into three phases: exploratory, assessment, and validation.","title":"Usability testing"},{"location":"additional-resources/user-testing-guide/#exploratory","text":"Use low-fidelity prototypes to A/B test alternative solution implementations. Which one delivers a better user experience? Use the results to decide on a design direction.","title":"Exploratory"},{"location":"additional-resources/user-testing-guide/#assessment","text":"Determine if users can meet topline objectives with your proposed solution. Use the results to continue refining your design or pivot to a new solution.","title":"Assessment"},{"location":"additional-resources/user-testing-guide/#validation","text":"Generally validation occurs after release of your solution. Define and track metrics that help you continue to refine the solution or detect issues.","title":"Validation"},{"location":"additional-resources/user-testing-guide/#usability-testing-process","text":"Before usability assessment testing, you should have good evidence to support your persona, problem and value hypotheses. Now you want to know if the solution you are building will deliver value against a real problem for a real user. Before testing, answer these four questions What are we testing? (Usability hypothesis) How will we test it? (The test) How will we know we passed? (Grading; often includes metrics and acceptance criteria) What will we do with the test results, good or bad? (Adaptation)","title":"Usability Testing Process"},{"location":"additional-resources/user-testing-guide/#testing-outline","text":"Usability testing (especially exploratory and assessment testing) will often take place with one or more users. Use the outline below to develop a testing script that all testers can use to maintain consistency. You want to make sure you have the right user, take a moment to confirm (or better understand) their characteristics and problems, conduct the test, gather qualitative feedback, and finally capture takeaways. Validation question: are you who we think you are? Discovery Do you have these characteristics (confirm persona hypotheses)? Do you have this problem (confirm problem hypotheses)? How do you solve it now (what is the next best alternative)? Testing Overview and instructions Test Example: Give the subject a goal (related to a problem hypothesis) and watch them attempt to solve it Measure: track the metrics that were identified in your grading plan User feedback: an opportunity for the user to share their qualitative experience (optional but helpful) Capture takeaways Persona and problem hypotheses UI and user stores Other notes","title":"Testing Outline"},{"location":"data-management/database-overview/","text":"Database Overview \u00b6 Do you (or your client) have data stored across so many files you can't remember where they are? Are you scrambling to pull together information when requested, constantly months behind in reporting, or finding yourself dumping data from multiple sources into a single spreadsheet for analysis? If so, it might be time for a database. However, there are complexities to consider when setting up and managing a database. This tutorial will introduce you to databases , describe available database solutions , and help you select the right database for your needs . Introduction to Databases \u00b6 A database is simply an organized store for information. In a relational database, data are stored as rows and columns in tables; relationships are defined between tables. You might have tables for customers, products, and orders. Customers place orders which include products. The database helps you get the right products to the right customers. In addition to relational databases, graph databases and NOSQL (Not Only Structured Query Language) databases are available. How the data are stored is often less important than how the data are accessed. The primary reason you're developing a database is probably to make data easier to access, analyze and report. When choosing and designing a database, you'll need to pay as much attention to how the user will interface with the database as the database solution itself. If you're simply looking for data analysis and visualization (think: Tableau), check out our Introduction to Data Visualization . Interacting with a database \u00b6 Most users will interact with a database through a navigation form. The navigation form surfaces the important functionality of the database, including entering data, querying data, and generating reports. Here's an example navigation form for a bakery's order database created in Microsoft Access. The navigation form is set up after the database is configured by the database administrator (DBA; this may be you, someone on the Metrics Service Line, or an outside contractor). The navigation form--as well as other forms, queries, and reports that are made available to the user by the DBA--make data accessible to users while protecting the integrity of the database Forms \u00b6 Data is entered into the database primarily through forms (it can be imported as well). Forms can be made available through the Navigation Form or through individual forms designed and made available by the DBA. Queries \u00b6 Queries allow you to search for and update records in the database. Want to know how many orders were placed yesterday? How many blueberry muffins you've sold this year? You'll need a query for that. Reports \u00b6 Reports are queries or collections of queries that can be batched together and run periodically. You might have a standard report that is produced each quarter or a ready-made report to provide real time information to your boss when requested. Reports must also be set up and made available by the DBA. A database solution involves multiple components \u00b6 A database solution (aka database management system; DBMS) will include these key components Database Software and Hosting Platform : The database software runs the database. The database and the database software will be hosted either on a local server or on the cloud. Database Management Client : The database management client is the interface that the DBA will use to configure the database and manage access for users. The database client typically uses SQL (Structured Query Language) to interact with the database. User Interface and Analysis Platform : The user interface allows users to access the data while the analysis platform allows the user to gain insights from the data. Often, these are combined. To illustrate, below are three options for a database solution that you might consider. Example 1: Microsoft Access \u00b6 Microsoft Access is an all-in-one database solution from the '90s. It lacks many features that some would deem critical, but for lightweight and desktop-based database solutions, it's hard to beat the out-of-the-box functionality for low-tech users. Similar, more modern options are available as cloud-based, subscription services. Example 2: Microsoft Azure SQL Database + SQL Server Management Studio + Microsoft PowerApps \u00b6 A cloud-based solution that offers great customization, if you're tech-savvy enough to configure and maintain it. You don't have to be a software engineer, but don't expect it to be easy. Amazon, IBM and Oracle offer similar solutions. Example 3: MySQL + MySQL Workbench + Custom Web Application \u00b6 This open source solution may be your go-to if you're willing to invest in a web-application. Many open source solutions are available. Project Spotlight: The Registry for the Monarch Butterfly Habitat Exchange was built on this stack. Database Options \u00b6 There are many choices when it comes to databases, each with its own advantages and disadvantages. All-in-one, low tech solutions \u00b6 These are your options if you don't have a tech guru on the team and don't have enough budget to hire one. Free(ish) and Mostly Desktop-Based \u00b6 Microsoft Access and OpenOffice Base are good entry-level options. OpenOffice Base is free but has fewer features and less support than Microsoft Access. Microsoft Excel , especially if paired with custom VBA Forms and Modules, can even serve as a database but has limitations. If you'd prefer a cloud-based solution, Google Sheets paired with Google Forms might even work. Even though these are not 'databases' per se, you may be surprised by how much easier data management is when you separate the data storage functionality from the data entry and data viewing functionality. Cheap(ish) Cloud-based Subscription Services \u00b6 The need for data storage and analytics solutions to serve low-tech users has led to a boom in cloud-based solutions like AirTable , QuickBase , TeamDesk , Knack , and Sonadier . The price scales with increasing storage capacity and number of users, from cheap ($5/month) to not-so-cheap ($500/month or more). The data visualization powerhouse Tableau offers a data management add-on for $5/mo that might meet your needs if you're mostly focused on visualization and are already a Tableau subscriber. Salesforce and other CRMs (Customer Relationship Management software) also combine data management and analytics in a way that could serve as a solution for your needs. Finally, one notable desktop application is Notion , which is a do-everything app with a cult following that provides some database-like functionality. Solutions for the more technical crowd \u00b6 Venture into this territory and you'll want to make sure you have the technical chops on board to configure and maintain these options. Open Source Database Software \u00b6 PostgreSQL , MySQL , and MariaDB are examples of open source (free) database solutions. Neo4j is a long-time standard for graph databases. While these are robust, scalable database solutions used by many large corporations, they are not designed for your everyday desk jockey. These open source solutions are often paired with a web-based interface. If you are working with a web developer and have the budget for them to design the forms, queries and reports you need you'll likely choose one of these open source options. Don't forget you'll probably need to hire them again when you want to make changes or when the software reaches its End Of Life (EOL). Cloud-based Scalable Services \u00b6 Cloud-based options are offered by Amazon RCD , Microsoft Azure , IBM Db2 and Oracle Cloud Services . Your data now lives on the cloud. Price scales with use, and entry-level tiers can be very cheap, but you will have to provide a credit card and make sure the charges don't get out of hand. To get to your data, you'll need an application--either a custom web app or a desktop app you build using a service like Microsoft PowerApps , FileMaker , Oracle Forms , or Appian . These \"low code\" application platforms allow you to develop custom solutions without a team of software engineers, but do require some serious training. Enterprise Database Solutions \u00b6 Microsoft, IBM, and Oracle all sell enterprise database solutions. Expect to pay over $1,000 per month. You can get Microsoft SQL Server Express for free but make sure its limited set of features will work for you. Database Clients \u00b6 For these more technical solutions, you'll need a database management client. Some database solutions come with a preferred client, such as Microsoft's SQL Server Management Studio and Azure Data Studio , Oracle's SQL Developer , PostreSQL's pgAdmin , and MySQL's MySQL Workbench . However, you're not stuck with these options, some of these can be used for other databases, while a number of popular free alternative options exist including SQuirrel SLQ , DataGrip , or even VSCode with mssql extension (for MySQL databases; however see Azure Data Studio as a better solution built from the VSCode platform). For Python \u00b6 If you're working in Python, know that there are APIs for most of the open source and cloud-based database solutions. For example, psycopg2 is the API for Postrgres and Ibm-db is the API for IBM's DB2 cloud-based database. I'd recommend playing around with SQLite (sqlite3 package), which comes with the base Python distribution, to familiarize yourself with using databases in your applications. You can access and manipulate your database in a Jupyter Notebook using the appropriate API and pandas . Use the pandas.read_sql command to read the results of a SQL query into a pandas dataframe for further analysis. For a graph database solution, Neo4j has multiple APIs for Python, try py2neo . How to Choose? \u00b6 With so many options, how do you choose? This section will help you pick the right one. Budget \u00b6 Consider both the cost of the software itself and the cost of configuring and maintaining the database. Scalability \u00b6 Unless you're dealing with *Big Data*, you won't need to worry about the database growing too large to fit on a single server. The only likely exception would be if attempting to store geospatial data within a relational database outside of ArcGIS. If you go with a cloud-based solution you can scale as much as you want, provided you can pay for it. Graph databases and NOSQL solutions are more robust to changes in their schema, if you're worried about changes to the data that will be stored in your database overtime. Hosting & Integration \u00b6 If the database will be hosted and managed by the client or their website manager, your options will likely be limited to those that integrate with their existing systems. Work with them when selecting a database and during database design. In most other cases, the database will live on the client's local server (unless you picked a cloud-based alternative). Maintenance & Support \u00b6 Who will maintain the database over time? If it's the client, make sure they are comfortable with the solution. Who will provide support if something crashes or the client needs a new type of report developed? If it's you, make sure you are comfortable with the solution. Keep in mind that paying a contractor to set up a cool web-based database and interface will likely require paying a contractor to maintain that database and interface periodically over time. Graph Databases \u00b6 A graph database stores data in a graph, which is not the 'graph' with a x- and y-axis, but rather a collection of nodes (sometimes called vertices) and relationships (sometimes called edges). Instead of a table of products, orders, and customers; each would be represented as a node and relationships would be defined to connect products to orders and orders to customers. Graph databases are more flexible than relational databases, and because relationships are stored directly in the database, they can be more performant for evaluating relationships specifically. NOSQL \u00b6 NOSQL (Not Only SQL) databases store disconnected and unstructured data through a variety of strategies. Some definitions of NOSQL include graph databases because graph databases do not use SQL. NOSQL encompasses a large variety of database technologies that might suit your needs if a relational or graph database do not suffice. Here is how the same order data might be represented in a NOSQL database. Final Thoughts \u00b6 Setting up a database is a big step in the maturation of any data-driven program. The right database solution will keep your data safe and secure while also providing access to the data for analysis and reporting. When choosing a database, the best strategy is to keep it as simple as possible. Important! If you've decided that you're not the right person to decide which database to use, that's ok. While someone with more technical experience will be better able to select the right database solution, they may not know enough about your program to design the database and the forms, queries and reports you need. You're the expert on your program, stay involved in the process to make sure the final solution meets your needs!","title":"Database Overview"},{"location":"data-management/database-overview/#database-overview","text":"Do you (or your client) have data stored across so many files you can't remember where they are? Are you scrambling to pull together information when requested, constantly months behind in reporting, or finding yourself dumping data from multiple sources into a single spreadsheet for analysis? If so, it might be time for a database. However, there are complexities to consider when setting up and managing a database. This tutorial will introduce you to databases , describe available database solutions , and help you select the right database for your needs .","title":"Database Overview"},{"location":"data-management/database-overview/#introduction-to-databases","text":"A database is simply an organized store for information. In a relational database, data are stored as rows and columns in tables; relationships are defined between tables. You might have tables for customers, products, and orders. Customers place orders which include products. The database helps you get the right products to the right customers. In addition to relational databases, graph databases and NOSQL (Not Only Structured Query Language) databases are available. How the data are stored is often less important than how the data are accessed. The primary reason you're developing a database is probably to make data easier to access, analyze and report. When choosing and designing a database, you'll need to pay as much attention to how the user will interface with the database as the database solution itself. If you're simply looking for data analysis and visualization (think: Tableau), check out our Introduction to Data Visualization .","title":"Introduction to Databases"},{"location":"data-management/database-overview/#interacting-with-a-database","text":"Most users will interact with a database through a navigation form. The navigation form surfaces the important functionality of the database, including entering data, querying data, and generating reports. Here's an example navigation form for a bakery's order database created in Microsoft Access. The navigation form is set up after the database is configured by the database administrator (DBA; this may be you, someone on the Metrics Service Line, or an outside contractor). The navigation form--as well as other forms, queries, and reports that are made available to the user by the DBA--make data accessible to users while protecting the integrity of the database","title":"Interacting with a database"},{"location":"data-management/database-overview/#forms","text":"Data is entered into the database primarily through forms (it can be imported as well). Forms can be made available through the Navigation Form or through individual forms designed and made available by the DBA.","title":"Forms"},{"location":"data-management/database-overview/#queries","text":"Queries allow you to search for and update records in the database. Want to know how many orders were placed yesterday? How many blueberry muffins you've sold this year? You'll need a query for that.","title":"Queries"},{"location":"data-management/database-overview/#reports","text":"Reports are queries or collections of queries that can be batched together and run periodically. You might have a standard report that is produced each quarter or a ready-made report to provide real time information to your boss when requested. Reports must also be set up and made available by the DBA.","title":"Reports"},{"location":"data-management/database-overview/#a-database-solution-involves-multiple-components","text":"A database solution (aka database management system; DBMS) will include these key components Database Software and Hosting Platform : The database software runs the database. The database and the database software will be hosted either on a local server or on the cloud. Database Management Client : The database management client is the interface that the DBA will use to configure the database and manage access for users. The database client typically uses SQL (Structured Query Language) to interact with the database. User Interface and Analysis Platform : The user interface allows users to access the data while the analysis platform allows the user to gain insights from the data. Often, these are combined. To illustrate, below are three options for a database solution that you might consider.","title":"A database solution involves multiple components"},{"location":"data-management/database-overview/#example-1-microsoft-access","text":"Microsoft Access is an all-in-one database solution from the '90s. It lacks many features that some would deem critical, but for lightweight and desktop-based database solutions, it's hard to beat the out-of-the-box functionality for low-tech users. Similar, more modern options are available as cloud-based, subscription services.","title":"Example 1: Microsoft Access"},{"location":"data-management/database-overview/#example-2-microsoft-azure-sql-database-sql-server-management-studio-microsoft-powerapps","text":"A cloud-based solution that offers great customization, if you're tech-savvy enough to configure and maintain it. You don't have to be a software engineer, but don't expect it to be easy. Amazon, IBM and Oracle offer similar solutions.","title":"Example 2: Microsoft Azure SQL Database + SQL Server Management Studio + Microsoft PowerApps"},{"location":"data-management/database-overview/#example-3-mysql-mysql-workbench-custom-web-application","text":"This open source solution may be your go-to if you're willing to invest in a web-application. Many open source solutions are available. Project Spotlight: The Registry for the Monarch Butterfly Habitat Exchange was built on this stack.","title":"Example 3: MySQL + MySQL Workbench + Custom Web Application"},{"location":"data-management/database-overview/#database-options","text":"There are many choices when it comes to databases, each with its own advantages and disadvantages.","title":"Database Options"},{"location":"data-management/database-overview/#all-in-one-low-tech-solutions","text":"These are your options if you don't have a tech guru on the team and don't have enough budget to hire one.","title":"All-in-one, low tech solutions"},{"location":"data-management/database-overview/#freeish-and-mostly-desktop-based","text":"Microsoft Access and OpenOffice Base are good entry-level options. OpenOffice Base is free but has fewer features and less support than Microsoft Access. Microsoft Excel , especially if paired with custom VBA Forms and Modules, can even serve as a database but has limitations. If you'd prefer a cloud-based solution, Google Sheets paired with Google Forms might even work. Even though these are not 'databases' per se, you may be surprised by how much easier data management is when you separate the data storage functionality from the data entry and data viewing functionality.","title":"Free(ish) and Mostly Desktop-Based"},{"location":"data-management/database-overview/#cheapish-cloud-based-subscription-services","text":"The need for data storage and analytics solutions to serve low-tech users has led to a boom in cloud-based solutions like AirTable , QuickBase , TeamDesk , Knack , and Sonadier . The price scales with increasing storage capacity and number of users, from cheap ($5/month) to not-so-cheap ($500/month or more). The data visualization powerhouse Tableau offers a data management add-on for $5/mo that might meet your needs if you're mostly focused on visualization and are already a Tableau subscriber. Salesforce and other CRMs (Customer Relationship Management software) also combine data management and analytics in a way that could serve as a solution for your needs. Finally, one notable desktop application is Notion , which is a do-everything app with a cult following that provides some database-like functionality.","title":"Cheap(ish) Cloud-based Subscription Services"},{"location":"data-management/database-overview/#solutions-for-the-more-technical-crowd","text":"Venture into this territory and you'll want to make sure you have the technical chops on board to configure and maintain these options.","title":"Solutions for the more technical crowd"},{"location":"data-management/database-overview/#open-source-database-software","text":"PostgreSQL , MySQL , and MariaDB are examples of open source (free) database solutions. Neo4j is a long-time standard for graph databases. While these are robust, scalable database solutions used by many large corporations, they are not designed for your everyday desk jockey. These open source solutions are often paired with a web-based interface. If you are working with a web developer and have the budget for them to design the forms, queries and reports you need you'll likely choose one of these open source options. Don't forget you'll probably need to hire them again when you want to make changes or when the software reaches its End Of Life (EOL).","title":"Open Source Database Software"},{"location":"data-management/database-overview/#cloud-based-scalable-services","text":"Cloud-based options are offered by Amazon RCD , Microsoft Azure , IBM Db2 and Oracle Cloud Services . Your data now lives on the cloud. Price scales with use, and entry-level tiers can be very cheap, but you will have to provide a credit card and make sure the charges don't get out of hand. To get to your data, you'll need an application--either a custom web app or a desktop app you build using a service like Microsoft PowerApps , FileMaker , Oracle Forms , or Appian . These \"low code\" application platforms allow you to develop custom solutions without a team of software engineers, but do require some serious training.","title":"Cloud-based Scalable Services"},{"location":"data-management/database-overview/#enterprise-database-solutions","text":"Microsoft, IBM, and Oracle all sell enterprise database solutions. Expect to pay over $1,000 per month. You can get Microsoft SQL Server Express for free but make sure its limited set of features will work for you.","title":"Enterprise Database Solutions"},{"location":"data-management/database-overview/#database-clients","text":"For these more technical solutions, you'll need a database management client. Some database solutions come with a preferred client, such as Microsoft's SQL Server Management Studio and Azure Data Studio , Oracle's SQL Developer , PostreSQL's pgAdmin , and MySQL's MySQL Workbench . However, you're not stuck with these options, some of these can be used for other databases, while a number of popular free alternative options exist including SQuirrel SLQ , DataGrip , or even VSCode with mssql extension (for MySQL databases; however see Azure Data Studio as a better solution built from the VSCode platform).","title":"Database Clients"},{"location":"data-management/database-overview/#for-python","text":"If you're working in Python, know that there are APIs for most of the open source and cloud-based database solutions. For example, psycopg2 is the API for Postrgres and Ibm-db is the API for IBM's DB2 cloud-based database. I'd recommend playing around with SQLite (sqlite3 package), which comes with the base Python distribution, to familiarize yourself with using databases in your applications. You can access and manipulate your database in a Jupyter Notebook using the appropriate API and pandas . Use the pandas.read_sql command to read the results of a SQL query into a pandas dataframe for further analysis. For a graph database solution, Neo4j has multiple APIs for Python, try py2neo .","title":"For Python"},{"location":"data-management/database-overview/#how-to-choose","text":"With so many options, how do you choose? This section will help you pick the right one.","title":"How to Choose?"},{"location":"data-management/database-overview/#budget","text":"Consider both the cost of the software itself and the cost of configuring and maintaining the database.","title":"Budget"},{"location":"data-management/database-overview/#scalability","text":"Unless you're dealing with *Big Data*, you won't need to worry about the database growing too large to fit on a single server. The only likely exception would be if attempting to store geospatial data within a relational database outside of ArcGIS. If you go with a cloud-based solution you can scale as much as you want, provided you can pay for it. Graph databases and NOSQL solutions are more robust to changes in their schema, if you're worried about changes to the data that will be stored in your database overtime.","title":"Scalability"},{"location":"data-management/database-overview/#hosting-integration","text":"If the database will be hosted and managed by the client or their website manager, your options will likely be limited to those that integrate with their existing systems. Work with them when selecting a database and during database design. In most other cases, the database will live on the client's local server (unless you picked a cloud-based alternative).","title":"Hosting &amp; Integration"},{"location":"data-management/database-overview/#maintenance-support","text":"Who will maintain the database over time? If it's the client, make sure they are comfortable with the solution. Who will provide support if something crashes or the client needs a new type of report developed? If it's you, make sure you are comfortable with the solution. Keep in mind that paying a contractor to set up a cool web-based database and interface will likely require paying a contractor to maintain that database and interface periodically over time.","title":"Maintenance &amp; Support"},{"location":"data-management/database-overview/#graph-databases","text":"A graph database stores data in a graph, which is not the 'graph' with a x- and y-axis, but rather a collection of nodes (sometimes called vertices) and relationships (sometimes called edges). Instead of a table of products, orders, and customers; each would be represented as a node and relationships would be defined to connect products to orders and orders to customers. Graph databases are more flexible than relational databases, and because relationships are stored directly in the database, they can be more performant for evaluating relationships specifically.","title":"Graph Databases"},{"location":"data-management/database-overview/#nosql","text":"NOSQL (Not Only SQL) databases store disconnected and unstructured data through a variety of strategies. Some definitions of NOSQL include graph databases because graph databases do not use SQL. NOSQL encompasses a large variety of database technologies that might suit your needs if a relational or graph database do not suffice. Here is how the same order data might be represented in a NOSQL database.","title":"NOSQL"},{"location":"data-management/database-overview/#final-thoughts","text":"Setting up a database is a big step in the maturation of any data-driven program. The right database solution will keep your data safe and secure while also providing access to the data for analysis and reporting. When choosing a database, the best strategy is to keep it as simple as possible. Important! If you've decided that you're not the right person to decide which database to use, that's ok. While someone with more technical experience will be better able to select the right database solution, they may not know enough about your program to design the database and the forms, queries and reports you need. You're the expert on your program, stay involved in the process to make sure the final solution meets your needs!","title":"Final Thoughts"},{"location":"data-management/neo4j-advanced/","text":"Neo4j Advanced \u00b6 This topic contains advanced procedures for Neo4j graph databases. Neo4j admin \u00b6 Neo4j-admin is the CLI for Neo4j. To access it in Neo4j Desktop, select 'Manage' from the three-circle menu in the top right of a database card. Then select the 'Open Terminal' button. You must prepend bin\\ to any calls to neo4j-admin for it to work. For example: bin\\neo4j-admin ... Dump and load database \u00b6 If you need to make a copy of an existing graph database to share, you can use the dump and load commands with neo4j-admin. First, open the project you want to share. Make sure the database is stopped. Click the three-circles button in the top right of the database card and select 'Manage'. Click 'Open Terminal'. In the Terminal, type: bin\\neo4j-admin dump --database=neo4j --to=<path\\to\\destination.dump> Note that the default database for this project will be saved (I believe this is the database in the first position). If you use the name you gave a database in the --database flag, you'll get a database does not exist error. I have no idea why they set it up this way, but it is the way it is. To load the database, first create an empty database in a new project. Then, following the process above, access the terminal and type: bin \\ neo4j - admin load -- from =< path \\ to \\ destination . dump > -- database = neo4j -- force The --force flag will overwrite the existing database, which is why I recommend starting a new project so you don't overwrite something you want to keep. See here for more. Web hosting \u00b6 Neo4j graph databases can be hosted in Aura (Neo4j's own hosting service), on AWS or deployed in Docker (among other options). While Neo4j advertises pricing by the minute, in fact they bill by the month: you cannot stop an instance of an Aura graph database. One month of hosting will run about $64. Previously, GrapheneDB offered an option through Heroku for $9, however that is no longer supported. Create the database \u00b6 Setting up a database on Aura is fairly easy. You'll need to set up an account and provide a credit card. Then, just hit 'Create Database'. Save the username and password into a JSON file in your secrets/ directory. The URI is less sensitive, but might as well just keep it all together. The code below will establish a connection using neo4j . from neo4j import GraphDatabase import json with open ( 'secrets/aura_creds.json' ) as f : creds = json . load ( f ) URI = creds . get ( 'URI' ) USERNAME = creds . get ( 'USERNAME' ) PASSWORD = creds . get ( 'PASSWORD' ) graph = GraphDatabase . driver ( URI , auth = ( USERNAME , PASSWORD )) Note that it appears that py2neo v2020.1.0 does not support the protocol used by Aura. If your app simply queries data from the database with cypher, neo4j will be sufficient. If you need more advanced functionality, and don't want to learn neo4j to that extent, check this stack overflow topic to see if anyone can help. Al Populate with existing database \u00b6 To push an existing database to this cloud instance, follow the guidance under 'Import'. See the guidance above for help using the terminal. Note that the database name is not the one you gave it, it is likely 'neo4j'. Unit Testing \u00b6 As you populate your graph with data and your schema expands, you may want to set up unit tests to ensure that the results you're getting are accurate. You can set up a small graph with your initial schema but less data. Then write queries that you know the answers to. When you want to update the graph schema, first update the test graph schema and test that the right answers are returned. Write new tests to cover the extension of the graph schema you are planning. Importing Data \u00b6 You'll often start a graph database by importing data from CSV files. You can also import JSON files from you API. CSV files can be loaded from a specific 'Import' file on your local directory, or they can loaded from a URL (Google Sheets, GitHub, Dropbox). To find the local folder, open the 'Manage' menu of your graph (three dots on the top right in the Desktop client, not the Browser). Click the split button 'Open Folder' and select 'Import'. A folder will open; drag and drop files into this folder. Alternatively, commit your project to GitHub and get the URL for the raw version of your data. Or save it in Google Sheets and get the public link. For step by step instructions, see here . Once you have your data available, use the LOAD CSV command to read the data into Neo4j from the local file and parse the data into the graph. Here's a simple example, but see [our tutorial] or the documentation for more. Here's the first two rows of the CSV (avoid spaces in header names): Name Role Rate Jim Senior Associate $150 Jan Associate $100 Here's how to load entities into a graph database. Employees and Roles are represented as nodes. The role for each employee will be expressed as a relationship after nodes are initially loaded. You'll copy this Cypher code into the terminal of the Neo4j Browser application: LOAD CSV WITH HEADERS FROM 'file:///data.csv' AS row MERGE ( person : Person { name : row . Name }) MERGE ( role : Role { role : row . Role , rate : row . Rate }) In this statement, the CSV is first made available to the application. Each row will be iterated through, accessed with the row variable. Using the MERGE command, we assign the contents of the 'Name' column for that row ( row.Name ) to a node labelled 'Person' ( person:Person ), specifically to the property 'name' ( {name: row.Name} ). The MERGE command ensures no duplicates are created, CREATE could also be used but might result in duplication. Next, assign roles to employees through relationships: LOAD CSV WITH HEADERS FROM 'file:///data.csv' AS row MATCH ( person : Person { name : row . Name }) MATCH ( role : Role { role : row . Role }) MERGE ( person ) -[ : HAS_ROLE ]-> ( role ) Again, the CSV is made available to the application. We'll need it to see which person has which role. The two MATCH statements identify nodes within our graph that have the same name and role as the first row in the CSV. With the correct Person and Role nodes identified, a new relationship HAS_ROLE is created with the MERGE command, again to avoid duplication. Continue in this way until all entities are imported as nodes and all relationships are defined between nodes. Working with files \u00b6 The Neo4j tutorial provides in depth guidance for translating a relational database to a graph database using csvs. Below we provide some basic guidance on working with files, especially csvs. Neo4j does not allow you to access files on your local system. Instead, you must upload files into the project's file directory before they can be available to Neo4j. Add a file using the 'Add File' card in the Desktop client. Of course, changes made to the file in your local directory will not be reflected in this copy, you must upload any changes directly. To use the file in Neo4j, click on the three-circle icon in the top right of the uploaded file card and select 'copy file URL'. Use the copied file url like this: LOAD CSV WITH HEADERS FROM \"<paste url here>\" as row ... For example, you can load entities from two columns 'Project_Number' and 'Region' with: LOAD CSV WITH HEADERS FROM \"http://localhost:11001/project-5810fc37-0742-4c0b-b0d7-238646cc50ea/reports.csv\" as row MERGE ( p : Project { name : row . Project_Number }) MERGE ( r : Region { name : row . Region }) RETURN p , r To create relationships between these nodes (where any nodes in the same row are related): LOAD CSV WITH HEADERS FROM \"http://localhost:11001/project-5810fc37-0742-4c0b-b0d7-238646cc50ea/reports.csv\" as row MATCH ( p : Project { name : row . Project_Number }) MATCH ( r : Region { name : row . Region }) MERGE ( p ) -[ : LOCATED_IN ]-> ( r )","title":"Neo4j Advanced"},{"location":"data-management/neo4j-advanced/#neo4j-advanced","text":"This topic contains advanced procedures for Neo4j graph databases.","title":"Neo4j Advanced"},{"location":"data-management/neo4j-advanced/#neo4j-admin","text":"Neo4j-admin is the CLI for Neo4j. To access it in Neo4j Desktop, select 'Manage' from the three-circle menu in the top right of a database card. Then select the 'Open Terminal' button. You must prepend bin\\ to any calls to neo4j-admin for it to work. For example: bin\\neo4j-admin ...","title":"Neo4j admin"},{"location":"data-management/neo4j-advanced/#dump-and-load-database","text":"If you need to make a copy of an existing graph database to share, you can use the dump and load commands with neo4j-admin. First, open the project you want to share. Make sure the database is stopped. Click the three-circles button in the top right of the database card and select 'Manage'. Click 'Open Terminal'. In the Terminal, type: bin\\neo4j-admin dump --database=neo4j --to=<path\\to\\destination.dump> Note that the default database for this project will be saved (I believe this is the database in the first position). If you use the name you gave a database in the --database flag, you'll get a database does not exist error. I have no idea why they set it up this way, but it is the way it is. To load the database, first create an empty database in a new project. Then, following the process above, access the terminal and type: bin \\ neo4j - admin load -- from =< path \\ to \\ destination . dump > -- database = neo4j -- force The --force flag will overwrite the existing database, which is why I recommend starting a new project so you don't overwrite something you want to keep. See here for more.","title":"Dump and load database"},{"location":"data-management/neo4j-advanced/#web-hosting","text":"Neo4j graph databases can be hosted in Aura (Neo4j's own hosting service), on AWS or deployed in Docker (among other options). While Neo4j advertises pricing by the minute, in fact they bill by the month: you cannot stop an instance of an Aura graph database. One month of hosting will run about $64. Previously, GrapheneDB offered an option through Heroku for $9, however that is no longer supported.","title":"Web hosting"},{"location":"data-management/neo4j-advanced/#create-the-database","text":"Setting up a database on Aura is fairly easy. You'll need to set up an account and provide a credit card. Then, just hit 'Create Database'. Save the username and password into a JSON file in your secrets/ directory. The URI is less sensitive, but might as well just keep it all together. The code below will establish a connection using neo4j . from neo4j import GraphDatabase import json with open ( 'secrets/aura_creds.json' ) as f : creds = json . load ( f ) URI = creds . get ( 'URI' ) USERNAME = creds . get ( 'USERNAME' ) PASSWORD = creds . get ( 'PASSWORD' ) graph = GraphDatabase . driver ( URI , auth = ( USERNAME , PASSWORD )) Note that it appears that py2neo v2020.1.0 does not support the protocol used by Aura. If your app simply queries data from the database with cypher, neo4j will be sufficient. If you need more advanced functionality, and don't want to learn neo4j to that extent, check this stack overflow topic to see if anyone can help. Al","title":"Create the database"},{"location":"data-management/neo4j-advanced/#populate-with-existing-database","text":"To push an existing database to this cloud instance, follow the guidance under 'Import'. See the guidance above for help using the terminal. Note that the database name is not the one you gave it, it is likely 'neo4j'.","title":"Populate with existing database"},{"location":"data-management/neo4j-advanced/#unit-testing","text":"As you populate your graph with data and your schema expands, you may want to set up unit tests to ensure that the results you're getting are accurate. You can set up a small graph with your initial schema but less data. Then write queries that you know the answers to. When you want to update the graph schema, first update the test graph schema and test that the right answers are returned. Write new tests to cover the extension of the graph schema you are planning.","title":"Unit Testing"},{"location":"data-management/neo4j-advanced/#importing-data","text":"You'll often start a graph database by importing data from CSV files. You can also import JSON files from you API. CSV files can be loaded from a specific 'Import' file on your local directory, or they can loaded from a URL (Google Sheets, GitHub, Dropbox). To find the local folder, open the 'Manage' menu of your graph (three dots on the top right in the Desktop client, not the Browser). Click the split button 'Open Folder' and select 'Import'. A folder will open; drag and drop files into this folder. Alternatively, commit your project to GitHub and get the URL for the raw version of your data. Or save it in Google Sheets and get the public link. For step by step instructions, see here . Once you have your data available, use the LOAD CSV command to read the data into Neo4j from the local file and parse the data into the graph. Here's a simple example, but see [our tutorial] or the documentation for more. Here's the first two rows of the CSV (avoid spaces in header names): Name Role Rate Jim Senior Associate $150 Jan Associate $100 Here's how to load entities into a graph database. Employees and Roles are represented as nodes. The role for each employee will be expressed as a relationship after nodes are initially loaded. You'll copy this Cypher code into the terminal of the Neo4j Browser application: LOAD CSV WITH HEADERS FROM 'file:///data.csv' AS row MERGE ( person : Person { name : row . Name }) MERGE ( role : Role { role : row . Role , rate : row . Rate }) In this statement, the CSV is first made available to the application. Each row will be iterated through, accessed with the row variable. Using the MERGE command, we assign the contents of the 'Name' column for that row ( row.Name ) to a node labelled 'Person' ( person:Person ), specifically to the property 'name' ( {name: row.Name} ). The MERGE command ensures no duplicates are created, CREATE could also be used but might result in duplication. Next, assign roles to employees through relationships: LOAD CSV WITH HEADERS FROM 'file:///data.csv' AS row MATCH ( person : Person { name : row . Name }) MATCH ( role : Role { role : row . Role }) MERGE ( person ) -[ : HAS_ROLE ]-> ( role ) Again, the CSV is made available to the application. We'll need it to see which person has which role. The two MATCH statements identify nodes within our graph that have the same name and role as the first row in the CSV. With the correct Person and Role nodes identified, a new relationship HAS_ROLE is created with the MERGE command, again to avoid duplication. Continue in this way until all entities are imported as nodes and all relationships are defined between nodes.","title":"Importing Data"},{"location":"data-management/neo4j-advanced/#working-with-files","text":"The Neo4j tutorial provides in depth guidance for translating a relational database to a graph database using csvs. Below we provide some basic guidance on working with files, especially csvs. Neo4j does not allow you to access files on your local system. Instead, you must upload files into the project's file directory before they can be available to Neo4j. Add a file using the 'Add File' card in the Desktop client. Of course, changes made to the file in your local directory will not be reflected in this copy, you must upload any changes directly. To use the file in Neo4j, click on the three-circle icon in the top right of the uploaded file card and select 'copy file URL'. Use the copied file url like this: LOAD CSV WITH HEADERS FROM \"<paste url here>\" as row ... For example, you can load entities from two columns 'Project_Number' and 'Region' with: LOAD CSV WITH HEADERS FROM \"http://localhost:11001/project-5810fc37-0742-4c0b-b0d7-238646cc50ea/reports.csv\" as row MERGE ( p : Project { name : row . Project_Number }) MERGE ( r : Region { name : row . Region }) RETURN p , r To create relationships between these nodes (where any nodes in the same row are related): LOAD CSV WITH HEADERS FROM \"http://localhost:11001/project-5810fc37-0742-4c0b-b0d7-238646cc50ea/reports.csv\" as row MATCH ( p : Project { name : row . Project_Number }) MATCH ( r : Region { name : row . Region }) MERGE ( p ) -[ : LOCATED_IN ]-> ( r )","title":"Working with files"},{"location":"data-management/neo4j-overview/","text":"Neo4j Overview \u00b6 This section is a high-level summary of content in the Neo4j Developer Guides ; links are provided to relevant guides in each section. Also see these resources (the O'Reilly Graph Databases book (free) is a must-read). Installation & Set Up \u00b6 Download the latest Neo4j Desktop release for your operating system. Install as normal. Make sure you meet the minimum system requirements (esp. check that PowerShell is v5 or above). The following sections explain some of the key features of Neo4j and link to set up instructions. Solutions \u00b6 Neo4j Desktop \u00b6 Open the Neo4j Desktop client on your computer. Follow these instructions to provide your activation key and orient yourself to the client. One important note: you'll want to open the ' Manage ' menu on the database to locate the import folder (Neo4j can only read files within this folder, it can't access files in your project's directory) and install plugins. Neo4j Browser \u00b6 You'll interact with graph databases in the Browser tool. 'Start' a graph database (or 'Add New Database') and, once it's ready, select 'Open'. This will open the Brower tool. Read through these instructions for an orientation. Neo4j Bloom \u00b6 If you are going to be sharing your graph with a non-technical audience, Bloom is a great tool for making your graph available and defining helpful searches. Your audience will need to download Neo4j Desktop and have access to your graph, and have some training on the interface. However, once set up it's a great way to explore the graph manually or execute pre-defined searches. APOC \u00b6 APOC stands for Awesome Procedures on Cypher. (Cypher is the SQL-like language used by Neo4j). You'll need to install APOC and provision it for your database (see instructions ). APOC contains many of the Natural Language Processing (NLP) algorithms you'll want (see more ). Graph Algorithms \u00b6 The Graph Algorithms library provides access to many graph algorithms for exploring and quantifying the graph. Check out the free book O'Reilly Graph Algorithms . This library is downloaded and provisioned similar to APOC ( instructions ). Graph Data Science Playground \u00b6 The Graph Data Science Playground provides a streamlined experience for executing graph data science algorithms against your graph. First install it by selecting the split button 'Open' and selecting 'Graph Apps Gallery'. Find the card for Graph Data Science Playground and click install (or paste the URL in the install bar as described here ). The Playground will now be available under the 'Open' split button. Aura \u00b6 Neo4j provides a graph hosting service 'Aura'. Fees are advertised by the second, but in actuality graph instances cannot be stopped, so the cheapest option comes to $64.80 per month. It's fairly easy to set up and upload an existing graph database . Instances can only be connected to with neo4j (not py2neo), so keep that in mind before selecting this option for hosting. Graphs can also be hosted on AWS or deployed in Docker . Cypher \u00b6 Cypher is the SQL-like language that Neo4j uses to add data and query graph databases. It's helpful but not necessary to have experience with SQL before learning Cypher. The free book O'Reilly Graph Databases is the best way to start learning Cypher, then check out the tutorials here . Cypher is more like a drawing using ASCII characters than a language per se. Entities are represented in parentheses (jim:Person) , properties are listed in braces (jim:Person {name:'Jim'}) , and relationships are represented by square brackets between an arrow ( -[:OWES]-> ). Relationships can also have properties -[:CONTAINS {amount:50}]-> . So, the following statement would state that Jim owes Jan $50: (jim:Person {name:'Jim'}) -[:OWES {amount: 50}]-> (jan:Person {name: 'Jan'}) Note that the jim and jan before the :Person are optional in this statement, but would be required if returning one or both of the entities in a query. Use CREATE or MERGE to create that relationship in the graph (best practice is to use MERGE to avoid duplication). Use MATCH to find that relationship in the graph. Cypher also has commands WHILE , WITH , SET , UNION , FOREACH , CREATE UNIQUE . Use RETURN to get the results of a query. Data Modelling \u00b6 The great thing about graph databases is that the schema is almost identical to the way we originally whiteboard out the information design. Unlike relational databases, you don't need to go through the normalization process and insert additional complexity, as when creating many-to-many lookup tables. First, describe in words the system you want to model. The 'nouns' in your system (Jim, Denver, desk) will be represented as nodes . Nodes are labelled as the type of entity they represent (people, places, things). Think about how your entities are connected (or identify verbs in your system); these are relationships . People 'know' other people. People 'work on' projects. Projects 'belong to' business units. Assign properties to each node or relationship to capture attributes. Attributes often answer questions you have of each entity or relationship ('how old is Jim?', 'when did Jim move to Denver'?; add an attribute 'age' to the 'Person' node and an attribute 'move date' to the 'LIVES_IN' relationship). A few additional considerations (see the Data Modeling chapter of Graph Databases): Use relationship attributes to express strength, weight, or quality of a relationship, plus any necessary metadata like timestamps, version numbers, etc. Use multiple relationships between nodes to express fine-grained and generic relationships. For example, one user may have a different billing and delivery address. Create two relationships to each, one generic ADDRESS and one more specific ( BILLING_ADDRESS ) or ( DELIVERY_ADDRESS ). You can then search for either address or both very efficiently. This is better than using relationship attributes for the same purpose, because it is faster to search. Use reciprocal relationships for bi-directional relationships. In other words, create a relationship going from node a to node b , and the same relationship from b to a . When searching, it's best to omit the arrow in the relationship if the direction is not known or is not important; however, relationships cannot be defined without a direction. Add relationships when use cases dictate. For example, while you could find everyone that worked together in a company by traversing from people through projects to other people, you can also make this relationship explicit by including a -[:WORKS_WITH]-> relationship. This will make queries faster and easier to write. Use nodes to model facts. For example, a person may work in one role for some time, and then be promoted to another role. Use a Job node to model the time that each person spends in each role. A job will have a start and end date as attributes, and be related to a role. (If job history is not important, you can simply delete the current relationship to a role and update with a new one.) Once you have a general idea of the entities and relationships, think through the use cases for the graph. Write user stories if helpful ( As User X, I want Y so I can Z ). Make sure that your schema will allow you to answer the questions that you'll need to answer to satisfy your use cases. You don't need to have a fully built out schema to begin, in fact one of the benefits of graph databases is that the schema can grow (i.e., you can add entities and relationships) over time. However, you will want to be thoughtful about your design so that it is efficient and captures the relationships you need to capture. For more detail, see Neo4j's modeling designs guide . Naming Guide \u00b6 Node labels are CapWords Relationships are ALL_CAPS Properties, variables, parameters, aliases and functions are camelCase Visualizing & Sharing \u00b6 Graphs are so intuitive that it's tempting to share the graph itself with users so they can explore it. Sometimes you just need to make the graph available without exposing the graph itself. Neo4j offers a few options for sharing the power of graphs. Schema \u00b6 To get the schema, use the command call db . schema . visualization () You can export this as an image (see next header). Image \u00b6 You can export a PNG or SVG of the graph from the Browser client by querying the entire graph (or a subset) and using the drop-down menu above the result. To get the entire graph, use the cypher command: MATCH ( n ) -[ r ]- () RETURN n , r Bloom \u00b6 Bloom is installed alongside the Neo4j Desktop client and provides great functionality for exploring the graph in visual format. You can also define searches for users to help them get the information they need. Once you have a graph populated, use the split button 'Open' to open with Bloom instead of the Browser. GraphGists \u00b6 GraphGists are similar to Jupyter Notebooks, but for graphs. They load the graph into the browser itself, so no back-end is required. However, they can only support small graphs (150 nodes and edges). Use these if you want to provide somewhat interactive graphs in a report or presentation. Kumu.io \u00b6 Kumu.io provides a platform for sharing graphs and system maps with some interactivity. You can display node information in a sidebar and 'focus' on specific nodes and relationships by clicking. It's a great way to explore systems maps, situation models, and other complex maps. Check out this system analysis from The Nature Conservancy for an example. Heroku and GrapheneDB \u00b6 Warning GrapheneDB as a low-cost solution has been deprecated. You can deploy your graph database powered application to the web with Heroku and the GrapheneDB plugin. To get more than 1,000 nodes and 10,000 relationships, you'll need to pay the monthly fee for GrapheneDB ($9/month and up). To get some experience with deploying on Heroku, check out the Movies Database tutorial (DO NOT USE THE py2neo repo they have listed, a new repo was created for the more recent version of py2neo--I have also found issues with that, contact me for a working copy).","title":"Neo4j Graph Databases"},{"location":"data-management/neo4j-overview/#neo4j-overview","text":"This section is a high-level summary of content in the Neo4j Developer Guides ; links are provided to relevant guides in each section. Also see these resources (the O'Reilly Graph Databases book (free) is a must-read).","title":"Neo4j Overview"},{"location":"data-management/neo4j-overview/#installation-set-up","text":"Download the latest Neo4j Desktop release for your operating system. Install as normal. Make sure you meet the minimum system requirements (esp. check that PowerShell is v5 or above). The following sections explain some of the key features of Neo4j and link to set up instructions.","title":"Installation &amp; Set Up"},{"location":"data-management/neo4j-overview/#solutions","text":"","title":"Solutions"},{"location":"data-management/neo4j-overview/#neo4j-desktop","text":"Open the Neo4j Desktop client on your computer. Follow these instructions to provide your activation key and orient yourself to the client. One important note: you'll want to open the ' Manage ' menu on the database to locate the import folder (Neo4j can only read files within this folder, it can't access files in your project's directory) and install plugins.","title":"Neo4j Desktop"},{"location":"data-management/neo4j-overview/#neo4j-browser","text":"You'll interact with graph databases in the Browser tool. 'Start' a graph database (or 'Add New Database') and, once it's ready, select 'Open'. This will open the Brower tool. Read through these instructions for an orientation.","title":"Neo4j Browser"},{"location":"data-management/neo4j-overview/#neo4j-bloom","text":"If you are going to be sharing your graph with a non-technical audience, Bloom is a great tool for making your graph available and defining helpful searches. Your audience will need to download Neo4j Desktop and have access to your graph, and have some training on the interface. However, once set up it's a great way to explore the graph manually or execute pre-defined searches.","title":"Neo4j Bloom"},{"location":"data-management/neo4j-overview/#apoc","text":"APOC stands for Awesome Procedures on Cypher. (Cypher is the SQL-like language used by Neo4j). You'll need to install APOC and provision it for your database (see instructions ). APOC contains many of the Natural Language Processing (NLP) algorithms you'll want (see more ).","title":"APOC"},{"location":"data-management/neo4j-overview/#graph-algorithms","text":"The Graph Algorithms library provides access to many graph algorithms for exploring and quantifying the graph. Check out the free book O'Reilly Graph Algorithms . This library is downloaded and provisioned similar to APOC ( instructions ).","title":"Graph Algorithms"},{"location":"data-management/neo4j-overview/#graph-data-science-playground","text":"The Graph Data Science Playground provides a streamlined experience for executing graph data science algorithms against your graph. First install it by selecting the split button 'Open' and selecting 'Graph Apps Gallery'. Find the card for Graph Data Science Playground and click install (or paste the URL in the install bar as described here ). The Playground will now be available under the 'Open' split button.","title":"Graph Data Science Playground"},{"location":"data-management/neo4j-overview/#aura","text":"Neo4j provides a graph hosting service 'Aura'. Fees are advertised by the second, but in actuality graph instances cannot be stopped, so the cheapest option comes to $64.80 per month. It's fairly easy to set up and upload an existing graph database . Instances can only be connected to with neo4j (not py2neo), so keep that in mind before selecting this option for hosting. Graphs can also be hosted on AWS or deployed in Docker .","title":"Aura"},{"location":"data-management/neo4j-overview/#cypher","text":"Cypher is the SQL-like language that Neo4j uses to add data and query graph databases. It's helpful but not necessary to have experience with SQL before learning Cypher. The free book O'Reilly Graph Databases is the best way to start learning Cypher, then check out the tutorials here . Cypher is more like a drawing using ASCII characters than a language per se. Entities are represented in parentheses (jim:Person) , properties are listed in braces (jim:Person {name:'Jim'}) , and relationships are represented by square brackets between an arrow ( -[:OWES]-> ). Relationships can also have properties -[:CONTAINS {amount:50}]-> . So, the following statement would state that Jim owes Jan $50: (jim:Person {name:'Jim'}) -[:OWES {amount: 50}]-> (jan:Person {name: 'Jan'}) Note that the jim and jan before the :Person are optional in this statement, but would be required if returning one or both of the entities in a query. Use CREATE or MERGE to create that relationship in the graph (best practice is to use MERGE to avoid duplication). Use MATCH to find that relationship in the graph. Cypher also has commands WHILE , WITH , SET , UNION , FOREACH , CREATE UNIQUE . Use RETURN to get the results of a query.","title":"Cypher"},{"location":"data-management/neo4j-overview/#data-modelling","text":"The great thing about graph databases is that the schema is almost identical to the way we originally whiteboard out the information design. Unlike relational databases, you don't need to go through the normalization process and insert additional complexity, as when creating many-to-many lookup tables. First, describe in words the system you want to model. The 'nouns' in your system (Jim, Denver, desk) will be represented as nodes . Nodes are labelled as the type of entity they represent (people, places, things). Think about how your entities are connected (or identify verbs in your system); these are relationships . People 'know' other people. People 'work on' projects. Projects 'belong to' business units. Assign properties to each node or relationship to capture attributes. Attributes often answer questions you have of each entity or relationship ('how old is Jim?', 'when did Jim move to Denver'?; add an attribute 'age' to the 'Person' node and an attribute 'move date' to the 'LIVES_IN' relationship). A few additional considerations (see the Data Modeling chapter of Graph Databases): Use relationship attributes to express strength, weight, or quality of a relationship, plus any necessary metadata like timestamps, version numbers, etc. Use multiple relationships between nodes to express fine-grained and generic relationships. For example, one user may have a different billing and delivery address. Create two relationships to each, one generic ADDRESS and one more specific ( BILLING_ADDRESS ) or ( DELIVERY_ADDRESS ). You can then search for either address or both very efficiently. This is better than using relationship attributes for the same purpose, because it is faster to search. Use reciprocal relationships for bi-directional relationships. In other words, create a relationship going from node a to node b , and the same relationship from b to a . When searching, it's best to omit the arrow in the relationship if the direction is not known or is not important; however, relationships cannot be defined without a direction. Add relationships when use cases dictate. For example, while you could find everyone that worked together in a company by traversing from people through projects to other people, you can also make this relationship explicit by including a -[:WORKS_WITH]-> relationship. This will make queries faster and easier to write. Use nodes to model facts. For example, a person may work in one role for some time, and then be promoted to another role. Use a Job node to model the time that each person spends in each role. A job will have a start and end date as attributes, and be related to a role. (If job history is not important, you can simply delete the current relationship to a role and update with a new one.) Once you have a general idea of the entities and relationships, think through the use cases for the graph. Write user stories if helpful ( As User X, I want Y so I can Z ). Make sure that your schema will allow you to answer the questions that you'll need to answer to satisfy your use cases. You don't need to have a fully built out schema to begin, in fact one of the benefits of graph databases is that the schema can grow (i.e., you can add entities and relationships) over time. However, you will want to be thoughtful about your design so that it is efficient and captures the relationships you need to capture. For more detail, see Neo4j's modeling designs guide .","title":"Data Modelling"},{"location":"data-management/neo4j-overview/#naming-guide","text":"Node labels are CapWords Relationships are ALL_CAPS Properties, variables, parameters, aliases and functions are camelCase","title":"Naming Guide"},{"location":"data-management/neo4j-overview/#visualizing-sharing","text":"Graphs are so intuitive that it's tempting to share the graph itself with users so they can explore it. Sometimes you just need to make the graph available without exposing the graph itself. Neo4j offers a few options for sharing the power of graphs.","title":"Visualizing &amp; Sharing"},{"location":"data-management/neo4j-overview/#schema","text":"To get the schema, use the command call db . schema . visualization () You can export this as an image (see next header).","title":"Schema"},{"location":"data-management/neo4j-overview/#image","text":"You can export a PNG or SVG of the graph from the Browser client by querying the entire graph (or a subset) and using the drop-down menu above the result. To get the entire graph, use the cypher command: MATCH ( n ) -[ r ]- () RETURN n , r","title":"Image"},{"location":"data-management/neo4j-overview/#bloom","text":"Bloom is installed alongside the Neo4j Desktop client and provides great functionality for exploring the graph in visual format. You can also define searches for users to help them get the information they need. Once you have a graph populated, use the split button 'Open' to open with Bloom instead of the Browser.","title":"Bloom"},{"location":"data-management/neo4j-overview/#graphgists","text":"GraphGists are similar to Jupyter Notebooks, but for graphs. They load the graph into the browser itself, so no back-end is required. However, they can only support small graphs (150 nodes and edges). Use these if you want to provide somewhat interactive graphs in a report or presentation.","title":"GraphGists"},{"location":"data-management/neo4j-overview/#kumuio","text":"Kumu.io provides a platform for sharing graphs and system maps with some interactivity. You can display node information in a sidebar and 'focus' on specific nodes and relationships by clicking. It's a great way to explore systems maps, situation models, and other complex maps. Check out this system analysis from The Nature Conservancy for an example.","title":"Kumu.io"},{"location":"data-management/neo4j-overview/#heroku-and-graphenedb","text":"Warning GrapheneDB as a low-cost solution has been deprecated. You can deploy your graph database powered application to the web with Heroku and the GrapheneDB plugin. To get more than 1,000 nodes and 10,000 relationships, you'll need to pay the monthly fee for GrapheneDB ($9/month and up). To get some experience with deploying on Heroku, check out the Movies Database tutorial (DO NOT USE THE py2neo repo they have listed, a new repo was created for the more recent version of py2neo--I have also found issues with that, contact me for a working copy).","title":"Heroku and GrapheneDB"},{"location":"data-management/neo4j-tutorial/","text":"Neo4j Tutorial \u00b6 This tutorial demonstrates how to convert from a simple relational database schema to a graph database. We'll be building a basic graph of Environmental Incentive's current project teaming structure. The graph will include staff, the projects they work on, and the organization of projects under Practice Areas. The purpose of the graph will be to explore the connectivity of EI's professional network across teams. (Staff will be said to work on a project if they are authorized to bill on a project). See also the Neo4j-provided tutorial describing the Windward database. 1. Project set up \u00b6 1.1 Create a project directory as you would otherwise 1.2 Open Neo4j Desktop 1.3 Create a New Project 2. Get the data \u00b6 Let's assume our data is stored in a relational database with the following tables: Staff: Name Abragan, Maria Celes L Alexandrovich, Andrew Projects : Code Project Practice_Area 1001 MI2 International 1002 LAC ESSC International 2.1 First we'd join the staff to the projects table to get a flat file. Each staff x project combo would be represented as a row, looking like: Code Project Practice_Area Name 1001 MI2 International Alexandrovich, Andrew 1001 MI2 International Ajroud, Brittany N 2.3 Now export that table as a CSV and save to your data/ folder for your project (i.e., save this file in the main project's directory--create one if necessary. This step has nothing to do with Neo4j, you should have a project directory set up already). 2.4 In Neo4j Desktop, click 'Add Files' and add the saved CSV. 3. Create a graph database \u00b6 3.1 Create a new graph database 3.2 Select 'Create a Local Graph' 3.3 Provide a graph name and password (remember the password). Click 'Create'. 3.4 Start the graph and then open with the Neo4j browser. The Browser will open in a new application window. 4. Populate the graph \u00b6 We'll use the LOAD CSV command to read in the data from the CSV we created in step 2. It may be helpful to have the CSV open to reference the header names. 4.1 In the Neo4j Desktop client, open the menu for the file you previously uploaded and select 'Copy url to clipboard'. The url is the most direct path (the file path doesn't include the file name, so is less useful here). 4.2 With the URL copied, return to the Neo4j Browser and type the following, then click the play button: LOAD CSV WITH HEADERS FROM < paste URL here > as row RETURN row This will check that the file is accessible and return a JSON representation of the data row by row, helpful if you need to see the structure of the data. In the Browser, it would look like this: 4.3 Import entities as nodes. We'll want to import each entity as a node before we create relationships. Use 'Shift+Enter' to input multi-line commands into the terminal. LOAD CSV WITH HEADERS FROM < paste URL here > as row MERGE ( e : Employee { name : row . Name }) RETURN e You can also access columns with their cell names (row.\"D2\") or by index (row[1] (note indexing starts at 1)). You should see a graph with all of the nodes for each employee in the Browser. Now that we have the hang of it, we'll load the remainder of the entities all at once: LOAD CSV WITH HEADERS FROM < paste URL here > as row MERGE (: Project { name : row . Project , code : toInteger ( row . Code )}) MERGE (: PracticeArea { name : row . Practice_Area }) Notice here that we transformed the Code column to integer before reading in. All columns are read as text unless specified using toInteger() , . Also note that we read the Code as a property of the Project node, rather than as it's own node--not every column needs to be a node. Finally, we did not assign a variable name to each node since we did not want to return anything. Some other tips: You can create a key for each imported row on the fly using the linenumber() command ( key: {linenumber()} ) When naming variables, don't use reserved keywords . Review the Style Guide to make your code and the names of your nodes, relationships, and properties easier to read and more consistent. 4.4 Add relationships. Now that nodes are created, we can add relationships between them. We'll use the CSV again for this, by matching the Project to the Practice Area row by row, and then establishing the relationship. LOAD CSV WITH HEADERS FROM < paste URL here > as row MATCH ( a : PracticeArea { name : row . Practice_Area }) MATCH ( p : Project { code : row . Code }) MERGE ( a ) -[ r : OWNS ]-> ( p ) RETURN ( a ) -[ r ]-> ( p ) Here we identify (match) the nodes PracticeArea and Project of each row in the CSV, find those in the graph, and add (merge) a relationship of 'OWNS'. We return the graph illustrating these relationships. If you need to remove relationships you can use: MATCH () -[ r : OWNS ]- () DELETE r Let's add the relationship between employees and the projects the work on from the CSV: LOAD CSV WITH HEADERS FROM < paste URL here > as row MATCH ( e : Employee { name : row . Name }) MATCH ( p : Project { code : row . Code }) MERGE ( e ) -[ : WORKS_ON ]-> ( p ) 4.5 Add any missing relationships. This example illustrates the power of graph databases. I happen to know that one project is owned by two practice areas. This is a rare exception, and had this been a relational database it would have been a pain--you'd need to migrate from a one-to-many relationship to a many-to-many relationship, which would break the schema. Here, we just add one more relationship. Let's first check to see which practice area the project (Walton) is currently assigned to: MATCH ( a : PracticeArea ) -[ : OWNS ]- ( p : Project { name : \"Walton\" }) RETURN p , a This will show us that Walton is owned by Habitat. So, let's add a relationship with International: MATCH ( a : PracticeArea { name : ' International ' }) MATCH ( p : Project { name : ' Walton ' }) MERGE ( a ) -[ : OWNS ]-> ( p ) And then we'll check that both relationships still exist (you can click the log entry for when we used this previously to quickly load it again): MATCH ( a : PracticeArea ) -[ : OWNS ]- ( p : Project { name : \"Walton\" }) RETURN p , a 4.6 Review schema You can now review the schema, which will look like this: The command to return a schema is CALL db.schema.visualization , however note that this was created in Bloom (see instructions ). 5. Evaluate 'closeness' \u00b6 Closeness is a graph data science algorithm that can detect nodes that are able to spread information efficiently through a graph. This is exactly what we'd like to quantify to see who is well-connected, and who is not, in our professional network. You have two options for executing graph data science algorithms. For a no-code solution (recommended to begin), install and open the Graph Data Science Playground. If you'd prefer using the terminal in Desktop, install the Graph Data Science Library. This tutorial will focus on the Graph Data Science Playground. For an intro video, see here . 6.1 Install the Graph Data Science playground. In the Neo4j Desktop client, select the split button 'Open' for your graph and select 'Graph Apps Gallery'. Find the card for Graph Data Science Playground and click install (or paste the URL in the install bar as described here ). The Playground will now be available under the 'Open' split button. You will also need to install the APOC plugin (see below). OR Install the Graph Data Science library. In the Neo4j Browser client, add the Graph Database Plugin by clicking Add Plugin and selecting the Graph Database plugin. 6.2 Open the Graph Data Science Playground using the 'Open' split button in the Neo4j Desktop. 6.3 Run the closeness algorithm. Select the Centralities category of algorithms, and then locate the Closeness algorithm in the menu bar. Set the 'Label' and 'Relationship Type' as 'Any'. This will allow us to find which node, whether an employee, a project, or a practice area, has the best ability to transmit information*. Change the 'Relationship Orientation' to 'Undirected' since information can flow either direction. Check the box to Store results, which will write the results into a new property in our nodes so we can access it later. Increase the 'Rows to Show' to 100 so you can see all nodes. Hit Run. You can explore the results with the Chart or Visualization in the Playground. You can also see the code so you could write it with the Graph Data Science Library in the Browser if needed. *Note that because employees are not connected to each other, we cannot simply run the closeness algorithm on employees only. We would need to add a relationship between employees whenever they are connected by a project to evaluate only employees, rather than all nodes in the graph. 6. Explore the graph \u00b6 The best way to explore the graph is in Neo4j Bloom, which comes standard with Neo4j Desktop. 6.1 In Neo4j Desktop, use the 'Open' split button to open with Neo4j Bloom. 6.2 Use the suggested perspective. Perspectives are views on the data that can include or exclude certain parts of the graph. For our purposes, we want to see everything. The suggested perspective will get us there quickest. If you need to configure your own perspective, check out this video . 6.3 Show Employees and Projects by searching. In the search bar, type Employee WORKS_ON and hit enter. This will return all employees and the projects they work on. 6.4 Re-color the Employee nodes by closeness. In the right panel, click on the colored employee legend item and select 'Rule-based' to apply a color based on the closeness score. Review the results in the Graph Data Science Library to see the minimum and maximum values. Make sure 'Apply Color' is switched on. That's it! You can now explore in Bloom by zooming and panning to see who is connected and how. Here's the final graph: Creating a schema in Bloom \u00b6 To get a pretty representation of your schema, first color and provide icons for each node, and optionally color each relationship in Bloom. Next, create a search phrase with the following code as the Cypher query: CALL db . schema . visualization () YIELD nodes , relationships UNWIND nodes as n UNWIND relationships as r RETURN n , r Enter the search phrase in the search bar to get the schema.","title":"Neo4j Tutorial"},{"location":"data-management/neo4j-tutorial/#neo4j-tutorial","text":"This tutorial demonstrates how to convert from a simple relational database schema to a graph database. We'll be building a basic graph of Environmental Incentive's current project teaming structure. The graph will include staff, the projects they work on, and the organization of projects under Practice Areas. The purpose of the graph will be to explore the connectivity of EI's professional network across teams. (Staff will be said to work on a project if they are authorized to bill on a project). See also the Neo4j-provided tutorial describing the Windward database.","title":"Neo4j Tutorial"},{"location":"data-management/neo4j-tutorial/#1-project-set-up","text":"1.1 Create a project directory as you would otherwise 1.2 Open Neo4j Desktop 1.3 Create a New Project","title":"1. Project set up"},{"location":"data-management/neo4j-tutorial/#2-get-the-data","text":"Let's assume our data is stored in a relational database with the following tables: Staff: Name Abragan, Maria Celes L Alexandrovich, Andrew Projects : Code Project Practice_Area 1001 MI2 International 1002 LAC ESSC International 2.1 First we'd join the staff to the projects table to get a flat file. Each staff x project combo would be represented as a row, looking like: Code Project Practice_Area Name 1001 MI2 International Alexandrovich, Andrew 1001 MI2 International Ajroud, Brittany N 2.3 Now export that table as a CSV and save to your data/ folder for your project (i.e., save this file in the main project's directory--create one if necessary. This step has nothing to do with Neo4j, you should have a project directory set up already). 2.4 In Neo4j Desktop, click 'Add Files' and add the saved CSV.","title":"2. Get the data"},{"location":"data-management/neo4j-tutorial/#3-create-a-graph-database","text":"3.1 Create a new graph database 3.2 Select 'Create a Local Graph' 3.3 Provide a graph name and password (remember the password). Click 'Create'. 3.4 Start the graph and then open with the Neo4j browser. The Browser will open in a new application window.","title":"3. Create a graph database"},{"location":"data-management/neo4j-tutorial/#4-populate-the-graph","text":"We'll use the LOAD CSV command to read in the data from the CSV we created in step 2. It may be helpful to have the CSV open to reference the header names. 4.1 In the Neo4j Desktop client, open the menu for the file you previously uploaded and select 'Copy url to clipboard'. The url is the most direct path (the file path doesn't include the file name, so is less useful here). 4.2 With the URL copied, return to the Neo4j Browser and type the following, then click the play button: LOAD CSV WITH HEADERS FROM < paste URL here > as row RETURN row This will check that the file is accessible and return a JSON representation of the data row by row, helpful if you need to see the structure of the data. In the Browser, it would look like this: 4.3 Import entities as nodes. We'll want to import each entity as a node before we create relationships. Use 'Shift+Enter' to input multi-line commands into the terminal. LOAD CSV WITH HEADERS FROM < paste URL here > as row MERGE ( e : Employee { name : row . Name }) RETURN e You can also access columns with their cell names (row.\"D2\") or by index (row[1] (note indexing starts at 1)). You should see a graph with all of the nodes for each employee in the Browser. Now that we have the hang of it, we'll load the remainder of the entities all at once: LOAD CSV WITH HEADERS FROM < paste URL here > as row MERGE (: Project { name : row . Project , code : toInteger ( row . Code )}) MERGE (: PracticeArea { name : row . Practice_Area }) Notice here that we transformed the Code column to integer before reading in. All columns are read as text unless specified using toInteger() , . Also note that we read the Code as a property of the Project node, rather than as it's own node--not every column needs to be a node. Finally, we did not assign a variable name to each node since we did not want to return anything. Some other tips: You can create a key for each imported row on the fly using the linenumber() command ( key: {linenumber()} ) When naming variables, don't use reserved keywords . Review the Style Guide to make your code and the names of your nodes, relationships, and properties easier to read and more consistent. 4.4 Add relationships. Now that nodes are created, we can add relationships between them. We'll use the CSV again for this, by matching the Project to the Practice Area row by row, and then establishing the relationship. LOAD CSV WITH HEADERS FROM < paste URL here > as row MATCH ( a : PracticeArea { name : row . Practice_Area }) MATCH ( p : Project { code : row . Code }) MERGE ( a ) -[ r : OWNS ]-> ( p ) RETURN ( a ) -[ r ]-> ( p ) Here we identify (match) the nodes PracticeArea and Project of each row in the CSV, find those in the graph, and add (merge) a relationship of 'OWNS'. We return the graph illustrating these relationships. If you need to remove relationships you can use: MATCH () -[ r : OWNS ]- () DELETE r Let's add the relationship between employees and the projects the work on from the CSV: LOAD CSV WITH HEADERS FROM < paste URL here > as row MATCH ( e : Employee { name : row . Name }) MATCH ( p : Project { code : row . Code }) MERGE ( e ) -[ : WORKS_ON ]-> ( p ) 4.5 Add any missing relationships. This example illustrates the power of graph databases. I happen to know that one project is owned by two practice areas. This is a rare exception, and had this been a relational database it would have been a pain--you'd need to migrate from a one-to-many relationship to a many-to-many relationship, which would break the schema. Here, we just add one more relationship. Let's first check to see which practice area the project (Walton) is currently assigned to: MATCH ( a : PracticeArea ) -[ : OWNS ]- ( p : Project { name : \"Walton\" }) RETURN p , a This will show us that Walton is owned by Habitat. So, let's add a relationship with International: MATCH ( a : PracticeArea { name : ' International ' }) MATCH ( p : Project { name : ' Walton ' }) MERGE ( a ) -[ : OWNS ]-> ( p ) And then we'll check that both relationships still exist (you can click the log entry for when we used this previously to quickly load it again): MATCH ( a : PracticeArea ) -[ : OWNS ]- ( p : Project { name : \"Walton\" }) RETURN p , a 4.6 Review schema You can now review the schema, which will look like this: The command to return a schema is CALL db.schema.visualization , however note that this was created in Bloom (see instructions ).","title":"4. Populate the graph"},{"location":"data-management/neo4j-tutorial/#5-evaluate-closeness","text":"Closeness is a graph data science algorithm that can detect nodes that are able to spread information efficiently through a graph. This is exactly what we'd like to quantify to see who is well-connected, and who is not, in our professional network. You have two options for executing graph data science algorithms. For a no-code solution (recommended to begin), install and open the Graph Data Science Playground. If you'd prefer using the terminal in Desktop, install the Graph Data Science Library. This tutorial will focus on the Graph Data Science Playground. For an intro video, see here . 6.1 Install the Graph Data Science playground. In the Neo4j Desktop client, select the split button 'Open' for your graph and select 'Graph Apps Gallery'. Find the card for Graph Data Science Playground and click install (or paste the URL in the install bar as described here ). The Playground will now be available under the 'Open' split button. You will also need to install the APOC plugin (see below). OR Install the Graph Data Science library. In the Neo4j Browser client, add the Graph Database Plugin by clicking Add Plugin and selecting the Graph Database plugin. 6.2 Open the Graph Data Science Playground using the 'Open' split button in the Neo4j Desktop. 6.3 Run the closeness algorithm. Select the Centralities category of algorithms, and then locate the Closeness algorithm in the menu bar. Set the 'Label' and 'Relationship Type' as 'Any'. This will allow us to find which node, whether an employee, a project, or a practice area, has the best ability to transmit information*. Change the 'Relationship Orientation' to 'Undirected' since information can flow either direction. Check the box to Store results, which will write the results into a new property in our nodes so we can access it later. Increase the 'Rows to Show' to 100 so you can see all nodes. Hit Run. You can explore the results with the Chart or Visualization in the Playground. You can also see the code so you could write it with the Graph Data Science Library in the Browser if needed. *Note that because employees are not connected to each other, we cannot simply run the closeness algorithm on employees only. We would need to add a relationship between employees whenever they are connected by a project to evaluate only employees, rather than all nodes in the graph.","title":"5. Evaluate 'closeness'"},{"location":"data-management/neo4j-tutorial/#6-explore-the-graph","text":"The best way to explore the graph is in Neo4j Bloom, which comes standard with Neo4j Desktop. 6.1 In Neo4j Desktop, use the 'Open' split button to open with Neo4j Bloom. 6.2 Use the suggested perspective. Perspectives are views on the data that can include or exclude certain parts of the graph. For our purposes, we want to see everything. The suggested perspective will get us there quickest. If you need to configure your own perspective, check out this video . 6.3 Show Employees and Projects by searching. In the search bar, type Employee WORKS_ON and hit enter. This will return all employees and the projects they work on. 6.4 Re-color the Employee nodes by closeness. In the right panel, click on the colored employee legend item and select 'Rule-based' to apply a color based on the closeness score. Review the results in the Graph Data Science Library to see the minimum and maximum values. Make sure 'Apply Color' is switched on. That's it! You can now explore in Bloom by zooming and panning to see who is connected and how. Here's the final graph:","title":"6. Explore the graph"},{"location":"data-management/neo4j-tutorial/#creating-a-schema-in-bloom","text":"To get a pretty representation of your schema, first color and provide icons for each node, and optionally color each relationship in Bloom. Next, create a search phrase with the following code as the Cypher query: CALL db . schema . visualization () YIELD nodes , relationships UNWIND nodes as n UNWIND relationships as r RETURN n , r Enter the search phrase in the search bar to get the schema.","title":"Creating a schema in Bloom"},{"location":"data-management/notes/","text":"Database Design Conceptual Data Model: must reflect the actual and possible states of the outside world. For example, if people can have two phone numbers, this must be possible. ER-Model Unified Modelling Language Schema/Logical Data Model: actual implementation of the conceptual data model in the chosen database technology. Normalization: According to wikipeda, \"each elementary 'fact' is recorded in one place so that insertions, updates, and deletions automatically maintain consistency\". Physical database design: affects performance, scalability, recovery, security, etc. Security Views","title":"Notes"},{"location":"data-management/postgres-tutorial/","text":"PostgreSQL Tutorial \u00b6 This tutorial demonstrates how to integrate a Postgres database with a Dash app. First, we'll set up a local Postgres database for testing and then provision a Postgres database through Heroku. Credit to Charming Data for his tutorial and inspiration for this tutorial. 1. Install Postgres \u00b6 Visit the Postgres website and download the right version of Postgres for your operating system (note that you may need to use a different browser if the download does not start automatically). Once the install begins, follow these instructions to set up Postgres on your local machine. Uncheck 'Stack Builder' when prompted, you won't need it. Make sure you write down the superuser password you set up! This installation will automatically install pgAdmin, the default database management client for Postgres, and psql, a command line interface for Postgres. (If you're following the instructions linked above to verify your Postgres install, note that the password will not display on screen when you type it for security reasons). 2. Configure a local server \u00b6 A local database will be used during development to test the app before we deploy to Heroku. Open pgAdmin (note pgAdmin opens in your browser). When you first open pgAdmin, you'll need to set a master password (for simplicity, I'd recommend using the same password you used to set up Postgres). Select 'Add New Server' (under Dashboard). Configure the new server following the images below. You may change the name of the server if you wish. Provide the password used to install Postgres as the password here. 3. Set up a local database \u00b6 In the left sidebar, right-click on Databases under my_local_server and select Create> Database... . Call the database test and save. 4. Create a table \u00b6 In the left sidebar, navigate to my_local_server> Databases> test> Schemas> Tables to access the tables associated with the database. Since this is a new database, we'll need to create a table. Note, if you called your database something other than test , you should navigate to your database in the tree. Right-click Tables and select Create> Table... . Configure the new table following the images below. You may change the name of the table if you wish, but the name of the table will be used in our code so don't forget it. Use the 'plus' sign on the right to add columns. If you have an existing CSV that you would like to import, make sure that the column names you create match exactly the column names in the CSV. If you need help figuring out which data type to use, review the guidance here . The most common types are 'text', 'int', 'real', and 'date'. 5. Populate table \u00b6 If you have an existing CSV, you can import it by right-clicking on the table in the left sidebar and selecting 'Import/Export'. Configure as shown below. Otherwise, select 'View/Edit Data' to input data manually. I'll be using a CSV like the one below to import data. Code Project Practice_Area 1001 MI2 International 1002 LAC ESSC International After importing, select 'View/Edit Data' from the my_table context menu (right-click) to confirm successful import. 6. Connect to the local database in the app \u00b6 The code below illustrates how to connect to the database on your local server in a simple Dash app. We'll use SQLAlchemy through Flask (Dash is built on Flask) to connect to the database. # See Charming Data's tutorials at https://www.youtube.com/channel/UCqBFsuAz41sqWcFjZkqmJqQ/featured import dash from dash.dependencies import Input , Output , State import dash_table import dash_core_components as dcc import dash_html_components as html import pandas as pd import os from flask_sqlalchemy import SQLAlchemy from flask import Flask # app requires \"pip install psycopg2\" as well server = Flask ( __name__ ) app = dash . Dash ( __name__ , server = server , suppress_callback_exceptions = True ) app . server . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = False # for your home PostgreSQL test table, replace <password> # Format: \"postgresql://<username>:<password>@<host name/address>/<database name>\" app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = \"postgresql://postgres:<password>@localhost/test\" db = SQLAlchemy ( app . server ) # ------------------------------------------------------------------------------------------------ app . layout = html . Div ([ html . Div ( id = 'postgres_datatable' ), dcc . Interval ( id = 'interval_pg' , interval = 86400000 * 7 , n_intervals = 0 ) ] ) # ------------------------------------------------------------------------------------------------ @app . callback ( Output ( 'postgres_datatable' , 'children' ), [ Input ( 'interval_pg' , 'n_intervals' )]) def populate_datatable ( n_intervals ): df = pd . read_sql_table ( 'my_table' , con = db . engine ) return [ dash_table . DataTable ( id = 'our-table' , columns = [{ 'name' : str ( x ), 'id' : str ( x ), 'deletable' : False , } for x in df . columns ], data = df . to_dict ( 'records' ), editable = True , row_deletable = True , filter_action = \"native\" , sort_action = \"native\" , # give user capability to sort columns sort_mode = \"single\" , # sort across 'multi' or 'single' columns page_action = 'none' , # render all of the data at once. No paging. style_table = { 'height' : '300px' , 'overflowY' : 'auto' }, style_cell = { 'textAlign' : 'left' , 'minWidth' : '100px' , 'width' : '100px' , 'maxWidth' : '100px' }, style_cell_conditional = [ { 'if' : { 'column_id' : c }, 'textAlign' : 'right' } for c in [ 'Price' , 'Sales' ] ] ), ] if __name__ == '__main__' : app . run_server ( debug = True ) Note how we set up a server using Flask, configured the SQLAlchemy Database URI connection string, and instantiated a database connection object db = SQLAlchemy(app.server) , highlighted below. # code specific to SQLAlchemy from flask_sqlalchemy import SQLAlchemy from flask import Flask server = Flask ( __name__ ) app = dash . Dash ( __name__ , server = server , suppress_callback_exceptions = True ) app . server . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = False app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = \"postgresql://postgres:<password>@localhost/test\" db = SQLAlchemy ( app . server ) To read in the table, we use pandas' pd.read_sql_table , passing db.engine as the connection object. # code specific to reading sql tables import pandas as pd df = pd . read_sql_table ( 'my_table' , con = db . engine ) 7. Deploy app to Heroku and provision a database \u00b6 Next, we'll deploy the app to Heroku and provision a database using the Heroku Postgres add-on. If you need guidance on deploying to Heroku, see our tutorial . Open the app at Heroku.com and click on 'Resources' to provision a Postgres database. Search for 'postgres', select 'Heroku Postgres', and click 'Provision' (you can use the Free version for this demo). 7. Connect to the remote database in pgAdmin \u00b6 In 'Resources' you should see the add-on you provisioned. Click on it, click 'Settings', and 'View Credentials'. You'll need this info to connect to the server through pgAdmin on your computer, and to use this database in your app. Open pgAdmin and in the left sidebar, right-click on 'Servers' and select 'Create Server'. By copying and pasting the credentials from Heroku, provision the server. In the images below, I've typed the corresponding field from Heroku to use in pgAdmin. The password should also be copied over. You should now be able to find your database in the list of databases within the server you just set up. There will be hundreds of databases in that list, use Ctrl+F and search for the database name to find it quickly (it will be colored gold). Note that the credentials you used may be 'rotated' (changed) by Heroku. This won't affect your app, but it will affect your connection through pgAdmin. If you can't find your database in pgAdmin, just repeat this step. Repeat steps 4 and 5 to create and populate tables in your new database. 8. Connect your app to your remote database \u00b6 Now you're ready to connect your app to your remote database! You'll want to keep your database credentials out of git and GitHub, so we'll use an environment variable . Helpfully, Heroku creates a DATABASE_URL environment variable for you when provisioning the database. You can check this at your app page in Heroku: select 'Settings' and click 'Reveal Config Vars'. Simply swap out your local connection string with the database URI provided for you Heroku database using the environment variable created by Heroku (you can find the URI listed in the database credentials under Settings). import os app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = os . environ [ 'DATABASE_URL' ] Note you will need to install psycopg2 in your environment if you haven't already. Update your requirements.txt and re-deploy the app with the new connection details. If you want to test your app locally, you can set up a database_uri.json file in your secrets/ directory to store the URI. Here's what that file would look like: { \"DATABASE_URI\" : \"<DATABASE URI>\" } You can use a try/except block to handle connections: import json try : app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = os . environ [ 'DATABASE_URL' ] except : with open ( 'secrets/database_uri.json' ) as f : app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = json . load ( f ) . get ( \"DATABASE_URI\" ) For more info on Heroku Postgres, read the docs .","title":"PostgreSQL Tutorial"},{"location":"data-management/postgres-tutorial/#postgresql-tutorial","text":"This tutorial demonstrates how to integrate a Postgres database with a Dash app. First, we'll set up a local Postgres database for testing and then provision a Postgres database through Heroku. Credit to Charming Data for his tutorial and inspiration for this tutorial.","title":"PostgreSQL Tutorial"},{"location":"data-management/postgres-tutorial/#1-install-postgres","text":"Visit the Postgres website and download the right version of Postgres for your operating system (note that you may need to use a different browser if the download does not start automatically). Once the install begins, follow these instructions to set up Postgres on your local machine. Uncheck 'Stack Builder' when prompted, you won't need it. Make sure you write down the superuser password you set up! This installation will automatically install pgAdmin, the default database management client for Postgres, and psql, a command line interface for Postgres. (If you're following the instructions linked above to verify your Postgres install, note that the password will not display on screen when you type it for security reasons).","title":"1. Install Postgres"},{"location":"data-management/postgres-tutorial/#2-configure-a-local-server","text":"A local database will be used during development to test the app before we deploy to Heroku. Open pgAdmin (note pgAdmin opens in your browser). When you first open pgAdmin, you'll need to set a master password (for simplicity, I'd recommend using the same password you used to set up Postgres). Select 'Add New Server' (under Dashboard). Configure the new server following the images below. You may change the name of the server if you wish. Provide the password used to install Postgres as the password here.","title":"2. Configure a local server"},{"location":"data-management/postgres-tutorial/#3-set-up-a-local-database","text":"In the left sidebar, right-click on Databases under my_local_server and select Create> Database... . Call the database test and save.","title":"3. Set up a local database"},{"location":"data-management/postgres-tutorial/#4-create-a-table","text":"In the left sidebar, navigate to my_local_server> Databases> test> Schemas> Tables to access the tables associated with the database. Since this is a new database, we'll need to create a table. Note, if you called your database something other than test , you should navigate to your database in the tree. Right-click Tables and select Create> Table... . Configure the new table following the images below. You may change the name of the table if you wish, but the name of the table will be used in our code so don't forget it. Use the 'plus' sign on the right to add columns. If you have an existing CSV that you would like to import, make sure that the column names you create match exactly the column names in the CSV. If you need help figuring out which data type to use, review the guidance here . The most common types are 'text', 'int', 'real', and 'date'.","title":"4. Create a table"},{"location":"data-management/postgres-tutorial/#5-populate-table","text":"If you have an existing CSV, you can import it by right-clicking on the table in the left sidebar and selecting 'Import/Export'. Configure as shown below. Otherwise, select 'View/Edit Data' to input data manually. I'll be using a CSV like the one below to import data. Code Project Practice_Area 1001 MI2 International 1002 LAC ESSC International After importing, select 'View/Edit Data' from the my_table context menu (right-click) to confirm successful import.","title":"5. Populate table"},{"location":"data-management/postgres-tutorial/#6-connect-to-the-local-database-in-the-app","text":"The code below illustrates how to connect to the database on your local server in a simple Dash app. We'll use SQLAlchemy through Flask (Dash is built on Flask) to connect to the database. # See Charming Data's tutorials at https://www.youtube.com/channel/UCqBFsuAz41sqWcFjZkqmJqQ/featured import dash from dash.dependencies import Input , Output , State import dash_table import dash_core_components as dcc import dash_html_components as html import pandas as pd import os from flask_sqlalchemy import SQLAlchemy from flask import Flask # app requires \"pip install psycopg2\" as well server = Flask ( __name__ ) app = dash . Dash ( __name__ , server = server , suppress_callback_exceptions = True ) app . server . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = False # for your home PostgreSQL test table, replace <password> # Format: \"postgresql://<username>:<password>@<host name/address>/<database name>\" app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = \"postgresql://postgres:<password>@localhost/test\" db = SQLAlchemy ( app . server ) # ------------------------------------------------------------------------------------------------ app . layout = html . Div ([ html . Div ( id = 'postgres_datatable' ), dcc . Interval ( id = 'interval_pg' , interval = 86400000 * 7 , n_intervals = 0 ) ] ) # ------------------------------------------------------------------------------------------------ @app . callback ( Output ( 'postgres_datatable' , 'children' ), [ Input ( 'interval_pg' , 'n_intervals' )]) def populate_datatable ( n_intervals ): df = pd . read_sql_table ( 'my_table' , con = db . engine ) return [ dash_table . DataTable ( id = 'our-table' , columns = [{ 'name' : str ( x ), 'id' : str ( x ), 'deletable' : False , } for x in df . columns ], data = df . to_dict ( 'records' ), editable = True , row_deletable = True , filter_action = \"native\" , sort_action = \"native\" , # give user capability to sort columns sort_mode = \"single\" , # sort across 'multi' or 'single' columns page_action = 'none' , # render all of the data at once. No paging. style_table = { 'height' : '300px' , 'overflowY' : 'auto' }, style_cell = { 'textAlign' : 'left' , 'minWidth' : '100px' , 'width' : '100px' , 'maxWidth' : '100px' }, style_cell_conditional = [ { 'if' : { 'column_id' : c }, 'textAlign' : 'right' } for c in [ 'Price' , 'Sales' ] ] ), ] if __name__ == '__main__' : app . run_server ( debug = True ) Note how we set up a server using Flask, configured the SQLAlchemy Database URI connection string, and instantiated a database connection object db = SQLAlchemy(app.server) , highlighted below. # code specific to SQLAlchemy from flask_sqlalchemy import SQLAlchemy from flask import Flask server = Flask ( __name__ ) app = dash . Dash ( __name__ , server = server , suppress_callback_exceptions = True ) app . server . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = False app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = \"postgresql://postgres:<password>@localhost/test\" db = SQLAlchemy ( app . server ) To read in the table, we use pandas' pd.read_sql_table , passing db.engine as the connection object. # code specific to reading sql tables import pandas as pd df = pd . read_sql_table ( 'my_table' , con = db . engine )","title":"6. Connect to the local database in the app"},{"location":"data-management/postgres-tutorial/#7-deploy-app-to-heroku-and-provision-a-database","text":"Next, we'll deploy the app to Heroku and provision a database using the Heroku Postgres add-on. If you need guidance on deploying to Heroku, see our tutorial . Open the app at Heroku.com and click on 'Resources' to provision a Postgres database. Search for 'postgres', select 'Heroku Postgres', and click 'Provision' (you can use the Free version for this demo).","title":"7. Deploy app to Heroku and provision a database"},{"location":"data-management/postgres-tutorial/#7-connect-to-the-remote-database-in-pgadmin","text":"In 'Resources' you should see the add-on you provisioned. Click on it, click 'Settings', and 'View Credentials'. You'll need this info to connect to the server through pgAdmin on your computer, and to use this database in your app. Open pgAdmin and in the left sidebar, right-click on 'Servers' and select 'Create Server'. By copying and pasting the credentials from Heroku, provision the server. In the images below, I've typed the corresponding field from Heroku to use in pgAdmin. The password should also be copied over. You should now be able to find your database in the list of databases within the server you just set up. There will be hundreds of databases in that list, use Ctrl+F and search for the database name to find it quickly (it will be colored gold). Note that the credentials you used may be 'rotated' (changed) by Heroku. This won't affect your app, but it will affect your connection through pgAdmin. If you can't find your database in pgAdmin, just repeat this step. Repeat steps 4 and 5 to create and populate tables in your new database.","title":"7. Connect to the remote database in pgAdmin"},{"location":"data-management/postgres-tutorial/#8-connect-your-app-to-your-remote-database","text":"Now you're ready to connect your app to your remote database! You'll want to keep your database credentials out of git and GitHub, so we'll use an environment variable . Helpfully, Heroku creates a DATABASE_URL environment variable for you when provisioning the database. You can check this at your app page in Heroku: select 'Settings' and click 'Reveal Config Vars'. Simply swap out your local connection string with the database URI provided for you Heroku database using the environment variable created by Heroku (you can find the URI listed in the database credentials under Settings). import os app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = os . environ [ 'DATABASE_URL' ] Note you will need to install psycopg2 in your environment if you haven't already. Update your requirements.txt and re-deploy the app with the new connection details. If you want to test your app locally, you can set up a database_uri.json file in your secrets/ directory to store the URI. Here's what that file would look like: { \"DATABASE_URI\" : \"<DATABASE URI>\" } You can use a try/except block to handle connections: import json try : app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = os . environ [ 'DATABASE_URL' ] except : with open ( 'secrets/database_uri.json' ) as f : app . server . config [ \"SQLALCHEMY_DATABASE_URI\" ] = json . load ( f ) . get ( \"DATABASE_URI\" ) For more info on Heroku Postgres, read the docs .","title":"8. Connect your app to your remote database"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/","text":"Machine Learning Tutorial \u00b6 This tutorial is intended to illustrate a typical workflow for machine learning to solve a land use and land cover classification problem. Land Use and Land Cover classifications are used to identify the dominant land cover or land use type in an area. We use the Naive Bayes and Random Forest classifiers, as implemented within scikit-learn library. This tutorial borrows heavily from the very helpful tutorial developed by Chris Holden and updated by Patrick Gray. Also used: rasterio , geopandas , numpy , pandas , shapely , and matplotlib . The Challenge \u00b6 Our client required a rapid approach for evaluating the benefits of conservation projects to Mule Deer. We proposed using Ecological State and Transition Models (STMs) as the basis for the evaluation. The NRCS has developed STMs for the dominant ecological sites within the region, however only a subset of the region was mapped. We use vegetation data (provided by the Rangelands App ) and other environmental variables to predict STM for the unmapped areas of the range. Import Statements \u00b6 import rasterio from rasterio.mask import mask from rasterio.plot import show from rasterio.plot import show_hist from rasterio.windows import Window from rasterio.plot import reshape_as_raster , reshape_as_image import geopandas as gpd import numpy as np from shapely.geometry import mapping from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn import preprocessing from sklearn.pipeline import make_pipeline from sklearn.utils import resample import matplotlib.pyplot as plt # ipywidgets is used to create a progress bar from ipywidgets import IntProgress from IPython.display import display % matplotlib inline Import Data \u00b6 We'll be using supervised classification techniques. We'll need both labels and predictor features to train the models. Our labels will come from the mapped STM derived from NRCS data. To begin, we'll use vegetation data as predictors, including cover estimates for trees, shrubs, perennial grasses and forbs, and bare ground. # CHANGE TO VEG_COVER_PATH # Read in features and training data train_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\train_clip_utm.tif' data = rasterio . open ( train_path ) data . crs # Check the projection, all features must share a projection CRS.from_dict(init='epsg:26913') labels_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\labels.shp' labels = gpd . read_file ( labels_path ) labels . crs {'init': 'epsg:26913'} len ( labels ) 6136 We have 6,136 labeled polygons, represented within a geopandas dataframe, while our predictor features are represented in a rasterio raster. Both share the same projection. Explore Data and Preparation Steps \u00b6 To train the classifiers, we'll need to associate our vector data (labels as polygons) with our raster pixels (predictor features). We'll accomplish this with the rasterio mask function. The mask function will essentially clip (or mask) our raster with each polygon. First, we'll want to extract the geometry of each feature in the labels shapefile to GeoJSON format. # this generates a list of shapely geometries geoms = labels . geometry . values # let's grab a single shapely geometry to check geometry = geoms [ 0 ] print ( \"This is a shapely polygon\" ) print ( type ( geometry )) print ( geometry ) # transform to GeoJSON format (note 'mapping' is in the shapely namespace) feature = [ mapping ( geometry )] # can also do this using polygon.__geo_interface__ print ( \"This is the same polygon in GeoJSON format\" ) print ( type ( feature )) print ( feature ) This is a shapely polygon <class 'shapely.geometry.polygon.Polygon'> POLYGON ((155090.8039999995 4317864.1029, 155095.9642000003 4317847.2246, 155096.9452999998 4317767.406400001, 155079.9972999999 4317764.4924, 155013.6201999998 4317772.8774, 154946.0262000002 4317762.761399999, 154906.4253000002 4317765.351500001, 154878.2922999999 4317781.742900001, 154874.5252 4317788.509099999, 154877.2857999997 4317829.973099999, 154890.6249000002 4317873.230799999, 154892.2852999996 4317898.202400001, 154873.1852000002 4318020.8462, 154866.6454999996 4318032.8059, 154861.3509 4318094.8554, 154880.1355999997 4318314.824999999, 154876.2439000001 4318350.6874, 154879.5504999999 4318400.631100001, 154884.4243000001 4318410.842, 154890.0175999999 4318527.3519, 154897.2582 4318573.0209, 154890.3213 4318594.527100001, 154894.7226999998 4318598.269300001, 154913.1912000002 4318593.0416, 154946.1224999996 4318553.260600001, 154952.6897 4318463.052999999, 154967.4232000001 4318401.3926, 154951.2766000004 4318220.3751, 154975.3370000003 4318141.542099999, 155019.5067999996 4318081.9695, 155041.7379000001 4318038.8718, 155041.0279000001 4317965.691199999, 155054.3926999997 4317914.6457, 155080.8075000001 4317871.2906, 155090.8039999995 4317864.1029)) This is the same polygon in GeoJSON format <class 'list'> [{'type': 'Polygon', 'coordinates': (((155090.80399999954, 4317864.1029), (155095.96420000028, 4317847.2246), (155096.9452999998, 4317767.406400001), (155079.99729999993, 4317764.4924), (155013.62019999977, 4317772.8774), (154946.0262000002, 4317762.761399999), (154906.42530000024, 4317765.351500001), (154878.29229999986, 4317781.742900001), (154874.52520000003, 4317788.509099999), (154877.28579999972, 4317829.973099999), (154890.62490000017, 4317873.230799999), (154892.28529999964, 4317898.202400001), (154873.18520000018, 4318020.8462000005), (154866.64549999963, 4318032.8059), (154861.35089999996, 4318094.8554), (154880.1355999997, 4318314.824999999), (154876.24390000012, 4318350.6874), (154879.5504999999, 4318400.631100001), (154884.42430000007, 4318410.842), (154890.0175999999, 4318527.3519), (154897.25820000004, 4318573.0209), (154890.32129999995, 4318594.5271000005), (154894.7226999998, 4318598.269300001), (154913.19120000023, 4318593.0416), (154946.1224999996, 4318553.260600001), (154952.6897, 4318463.052999999), (154967.42320000008, 4318401.3926), (154951.27660000045, 4318220.3751), (154975.3370000003, 4318141.542099999), (155019.50679999962, 4318081.9695), (155041.73790000007, 4318038.8718), (155041.0279000001, 4317965.691199999), (155054.39269999973, 4317914.6457), (155080.8075000001, 4317871.2906), (155090.80399999954, 4317864.1029)),)}] Now let's extract the raster values within each polygon using the rasterio mask() function . out_image , out_transform = mask ( data , feature , crop = True ) out_image . shape (6, 32, 10) show ( out_image [ 0 ]) <matplotlib.axes._subplots.AxesSubplot at 0xbea5c5b70> As you can see, the features raster was clipped to a single polygon. There are 6 bands and 32x10 pixels. We'll repeat this process for all 6,136 polygons to build our dataset. We'll also need to clean this raster up a bit before we use it in training. We'll explore all of this next. But first, we'll be doing a lot of memory intensive work so we'll close the dataset for now. data . close () Build the Training Data for sckit-learn \u00b6 We'll repeat the above process for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels. Note that the column 'MuleDeer_1' in the labels geodataframe has the label we're after. TODO: change to column with label instead of numeric. %% time # Lets create a progress bar as this step can take some time p_bar = IntProgress ( min = 0 , max = len ( geoms ), description = 'Processing' ) display ( p_bar ) # Set up arrays X = np . array ([], dtype = np . int8 ) . reshape ( 0 , 6 ) # pixels for training y = np . array ([], dtype = np . string_ ) # labels for training # extract the raster values within the polygon with rasterio . open ( train_path ) as src : band_count = src . count for index , geom in enumerate ( geoms ): # Note geoms was created above feature = [ mapping ( geom )] # the mask function returns an array of the raster pixels within this feature out_image , out_transform = mask ( src , feature , crop = True ) # eliminate all the pixels with 0 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image [:, ~ np . all ( out_image == 0 , axis = 0 )] # eliminate all the pixels with 255 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image_trimmed [:, ~ np . all ( out_image_trimmed == 255 , axis = 0 )] # reshape the array to [pixel count, bands] out_image_reshaped = out_image_trimmed . reshape ( - 1 , band_count ) # append the labels to the y array y = np . append ( y , [ labels [ \"MuleDeer_1\" ][ index ]] * out_image_reshaped . shape [ 0 ]) # ??? # stack the pixels onto the pixel array X = np . vstack (( X , out_image_reshaped )) # increment the progress bar p_bar . value += 1 IntProgress(value=0, description='Processing', max=6136) Wall time: 25min 33s Save the output \u00b6 We'll save the output as a numpy array to avoid the long process of rebuilding the features in the future. This will also allow us to share this analysis with others without them needing access to the input data. If this were not a tutorial, I might have ended the notebook here and started a new one for the remainder of the analysis. I've commented out the load statements below, uncomment to load in the saved features and labels. # Save features and labels np . save ( 'E:/lulc-features.npy' , X ) np . save ( 'E:/lulc-labels.npy' , y ) # Load in data X = np . load ( 'E:/lulc-features.npy' ) # fill in path y = np . load ( 'E:/lulc-labels.npy' ) Splitting the data for testing \u00b6 In order to evaluate the accuracy of our model, we'll reserve a subset of the data for testing. The train_test_split function allows us to quickly and randomly subset our data for this purpose. # split out 30% of data for testing. Random state set for reproducibility. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) Pairing y with X \u00b6 Now that we have the image we want to classify (X_train) and the land cover labels (y_train), let's check to make sure they match in size so we can feed them to our models. st_names = np . unique ( y_train ) print ( 'The training data include {n} classes: {classes} \\n ' . format ( n = st_names . size , classes = st_names )) # We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels print ( 'Our X matrix is sized: {sz} ' . format ( sz = X_train . shape )) print ( 'Our y array is sized: {sz} ' . format ( sz = y_train . shape )) The training data include 5 classes : [ ' Encroached Shrub ' ' Loamy Bottom ' ' P-J ' ' Perennial Shrub ' ' Wet/Salt Meadow ' ] Our X matrix is sized : ( 4755679 , 6 ) Our y array is sized : ( 4755679 , ) That looks good. We have 5 classes (i.e., our STM names); 6 predictor features (i.e., the 6 bands in our X matrix, now flattened; and both the X and y array are the same length. We'll treat these vegetation cover values as spectral signatures, and plot each to make sure they're actually separable since all we're going by in this classification is pixel values. fig , ax = plt . subplots ( 1 , 3 , figsize = [ 20 , 8 ]) # bands are numbered 1 through 6 following GDAL convention band_count = np . arange ( 1 , 7 ) classes = np . unique ( y_train ) for class_type in classes : band_intensity = np . mean ( X_train [ y_train == class_type , :], axis = 0 ) ax [ 0 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 1 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 2 ] . plot ( band_count , band_intensity , label = class_type ) #plot them as lines # Add some axis labels # Add some axis labels ax [ 0 ] . set_xlabel ( 'Band #' ) ax [ 0 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_xlabel ( 'Band #' ) ax [ 2 ] . set_ylabel ( 'Reflectance Value' ) ax [ 2 ] . set_xlabel ( 'Band #' ) ax [ 1 ] . legend ( loc = \"upper right\" ) # Add a title ax [ 0 ] . set_title ( 'Band Intensities Full Overview' ) ax [ 1 ] . set_title ( 'Band Intensities Lower Ref Subset' ) ax [ 2 ] . set_title ( 'Band Intensities Higher Ref Subset' ) plt . show () Looks like each class will be easily separable. This will be a helper function to convert class labels into indices so we're predicting to integers instead of strings. TODO: switch labels and numbers def str_class_to_int ( class_array ): class_array [ class_array == 'P-J' ] = 1 class_array [ class_array == 'Perennial Shrub' ] = 2 class_array [ class_array == 'Encroached Shrub' ] = 3 class_array [ class_array == 'Loamy Bottom' ] = 4 class_array [ class_array == 'Wet/Salt Meadow' ] = 5 return ( class_array . astype ( int )) Training the Classifier \u00b6 Now that we have our X matrix of feature inputs (the vegetation cover bands) and our y array (the labels), we can train our model. Visit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn . gnb = GaussianNB () gnb . fit ( X_train , y_train ) GaussianNB ( priors = None , var_smoothing = 1e-09 ) It's that simple to train a classifier in sckit-learn . The hard part is often validation and interpretation. Validation \u00b6 To see how well our classifier worked, we could use the test data we partioned earlier. However, we may want to adjust the model if our results are not as accurate as we'd like. This could lead to overfitting by 'leaking' information from the test set into our training of the model. Overfitting will hurt the performance of our model on predicting novel data, and will lead to inflated accuracy metrics. So how do we evaluate our model at this stage? Cross-validation . There are a few options for cross-validation, but for our purposes k-fold cross validation will work. # 5-fold cross validation scores = cross_val_score ( gnb , X_train , y_train , cv = 5 ) scores stores the results of computing the score 5 consecutive times (with different splits each time) scores array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The mean score and the 95% confidence interval of the score estimate are hence given by: print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) Accuracy : 0.59 (+/- 0.002 ) Improving Model Accuracy \u00b6 Standardizing Values \u00b6 The accuracy is not as good as we'd like. How can we improve the accuracy of the model? One option is to standardize the data so that each of the features are similar in magnitude. This avoids some higher values from overwhelming lower values. It is often necessary to complete this step, depending on the model used. sklearn provides a preprocessing module that facilitate this scaling. # Hide warnings for converting ints to floats (or save X, y as float64 type) import warnings warnings . filterwarnings ( \"ignore\" ) gnb = make_pipeline ( preprocessing . StandardScaler (), GaussianNB ()) cross_val_score ( gnb , X_train , y_train , cv = 5 ) array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The accuracy is exactly the same! This is because the Gaussian Naive Bayes is robust to scaling. In essence Gaussian Naive Bayes performs standardization internally . Balancing Classes \u00b6 Unbalanced classes can also impact the accuracy of a model. Imagine you have data on a rare disease that only 0.01% of people have. A simple model that predicts everyone does not have the disease would be right 99.99% of the time! To get a model that can actually predict when people do have the disease, you'll need to address the imbalanced classes. Again, this is an issue depending on the model used. We'll try a different type of model later in the tutorial that is more robust to imbalanced data (Random Forests). To balance the data, you can either down-sample the over-represented classes or up-sample the under-represented classes. How unbalanced are the classes now? \u00b6 Let's quickly plot the number of each label so we know how unbalanced the data are. import pandas as pd df_plot = pd . DataFrame ( y_train ) df_plot [ 0 ] . value_counts () . plot ( kind = 'bar' ) <matplotlib.axes._subplots.AxesSubplot at 0x22899b1eda0> We'll try up-sampling first so we don't reduce the number of data points too much. How much to upsample? We'll resample each of the less represented features with replacement to get the number of features contained in the most represented class. df_plot [ 0 ] . value_counts () P-J 2877442 Perennial Shrub 1388305 Encroached Shrub 375820 Loamy Bottom 85357 Wet/Salt Meadow 28755 Name: 0, dtype: int64 This will be easier with a pandas DataFrame, so let's convert our data to a DataFrame. df = pd . concat ( [ pd . DataFrame ( y_train , columns = [ 'label' ]), pd . DataFrame ( X_train )], axis = 1 ) . set_index ( 'label' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 label P-J 18 19 12 12 11 13 P-J 9 9 7 8 7 13 P-J 18 24 21 22 20 20 Perennial Shrub 14 18 16 12 14 13 P-J 4 5 9 9 7 7 df_shrub = resample ( df . loc [ 'Perennial Shrub' ], replace = True , n_samples = df . index . value_counts () . max ()) df_shrub . shape [ 0 ] 2877442 Now we have the same number of Perennial Shrub classes as P-J classes, our dominant class type. Let's repeat for the remaining features. We'll loop through our labels and concatenate the results to the most represented class so that this step is robust to changes in which features we explore. df . index . value_counts () . max () 2877442 # Start by subsetting the most represented class max_class = df . index . value_counts () . idxmax () df_upsampled = df . loc [ max_class ] labels = list ( df . index . unique ()) labels . remove ( max_class ) # For each label, resample the features to balance classes and append # to the empty dataframe for label in labels : df_temp = resample ( df . loc [ label ], replace = True , n_samples = df . index . value_counts () . max () ) df_upsampled = pd . concat ([ df_upsampled , df_temp ], axis = 0 ) df_upsampled . index . value_counts () P-J 2877442 Encroached Shrub 2877442 Perennial Shrub 2877442 Wet/Salt Meadow 2877442 Loamy Bottom 2877442 Name: label, dtype: int64 Now, we can split out our X and y data again and re-train the model on the more balanced classes. X_train_upsampled = df_upsampled . reset_index () . drop ( 'label' , axis = 1 ) y_train_upsampled = df_upsampled . index . values # 5-fold cross validation scores = cross_val_score ( gnb , X_train_upsampled , y_train_upsampled , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) Accuracy : 0.24 (+/- 0.001 ) Our accuracy took a bit of a nosedive, as now we're probably over-representing those less common classes on the landscape. For the purposes of our model, it may actually be better to simply ignore those more rare classes instead of over-representing them. We could try downsampling instead, but it will likely not help our accuracy very much. Instead, let's try a different model, Random Forests. Alternative Model: Random Forests \u00b6 Random Forests is robust to unscaled and unbalanced data, making it a good option for this classification problem out-of-the-box. It's not a bad idea to scale the data as we did earlier, but since our data is well scaled (percent cover data from 0 - 100%), we'll skip this step. Acknowledgements to this tutorial used in developing this section. %% time from sklearn.ensemble import RandomForestClassifier # Initialize our model with 10 estimators to limit processing time rfc = RandomForestClassifier ( n_estimators = 10 , random_state = 0 ) # 5-fold cross validation scores = cross_val_score ( rfc , X_train , y_train , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) Accuracy : 0.58 (+/- 0.001 ) Wall time : 3 h 13 min 13 s Our accuracy (57%) is slightly less than our accuracy with the unbalanced classes using Naive Bayes (59%) but comparable. Confusion Matrix \u00b6 We can visualize how well we're classifying each class (and where the model is getting confused) using a confusion matrix . The code for the confusion matrix is copied from the linked documentation. from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels def plot_confusion_matrix ( y_true , y_pred , classes , normalize = False , title = None , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if not title : if normalize : title = 'Normalized confusion matrix' else : title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix ( y_true , y_pred ) # Only use the labels that appear in the data classes = unique_labels ( y_true , y_pred ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) fig , ax = plt . subplots ( figsize = ( 15 , 10 )) im = ax . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) ax . figure . colorbar ( im , ax = ax ) # We want to show all ticks... ax . set ( xticks = np . arange ( cm . shape [ 1 ]), yticks = np . arange ( cm . shape [ 0 ]), # ... and label them with the respective list entries xticklabels = classes , yticklabels = classes , title = title , ylabel = 'True label' , xlabel = 'Predicted label' ) # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 45 , ha = \"right\" , rotation_mode = \"anchor\" ) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i in range ( cm . shape [ 0 ]): for j in range ( cm . shape [ 1 ]): ax . text ( j , i , format ( cm [ i , j ], fmt ), ha = \"center\" , va = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) fig . tight_layout () return ax # Use the model to predict on the X_train dataset y_pred = gnb . fit ( X_train , y_train ) . predict ( X_train ) class_names = unique_labels ( y_train , y_pred ) np . set_printoptions ( precision = 2 ) # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.90e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.91e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.17e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.71e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] The confusion matrix indicates that the model is catogorizing most pixels as P-J and some as Perennial Shrub, with just a few Wet Meadow predictions. It ignores Encroached Shrub and Loamy Bottom. Focusing on the final row of the matrix, you can see that the Wet/Salt Meadow is being categorized as P-J 75% of the time, Perennial Shrub 24% of the time, and its correct label only 1% of the time. y_pred_rfc = rfc . fit ( X_train , y_pred ) . predict ( X_train ) # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred_rfc , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.87e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.85e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.06e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.69e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] Interestingly, the confusion matrix for both the Naive Bayes and Random Forests model is the same. This may signal that we've done about as well as we could with these features. We could incorporate additional features through the same process as we went through before, considering topography, soils, precipitation, and spatial measures using distances or moving windows. We should also carefully consider feature selection to optimize the bias-variance tradeoffs. Feature engingeering and feature selection is beyond the scope of this tutorial. A classification report will provide reportable metrics of model accuracy to allow us document the performance of the model. It reports precision, recall and the F1 score. Precision is the ability of the model to avoid false positives. Recall is the ability to identify the feature correctly (notice it is equivalent to where the diagonal axis of the confusion matrix). The F1 score is a harmonic mean of precision and recall. from sklearn.metrics import classification_report print ( classification_report ( y_train , y_pred , target_names = class_names )) C:\\Users\\Erik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) precision recall f1-score support Encroached Shrub 0.00 0.00 0.00 375820 Loamy Bottom 0.00 0.00 0.00 85357 P-J 0.62 0.89 0.73 2877442 Perennial Shrub 0.40 0.19 0.26 1388305 Wet/Salt Meadow 0.02 0.01 0.02 28755 micro avg 0.59 0.59 0.59 4755679 macro avg 0.21 0.22 0.20 4755679 weighted avg 0.49 0.59 0.52 4755679 \u200b Predicting on the Image \u00b6 With our classifier fit, we can now proceed by trying to classify the entire image. range_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\range_clip_utm.tif' with rasterio . open ( range_path ) as src : profile = src . profile # the src profile will be used to save the output later img = src . read () # Take our full iamge and reshape into long 2d array (nrow * ncol, nband) for classification print ( img . shape ) reshaped_img = reshape_as_image ( img ) print ( reshaped_img . shape ) (6, 9412, 7341) (9412, 7341, 6) Now we can predict for each pixel in our image. class_prediction = gnb . predict ( reshaped_img . reshape ( - 1 , 6 )) # Reshape our classification map back into a 2d matrix so we can visualize it class_prediction = class_prediction . reshape ( reshaped_img [:, :, 0 ] . shape ) Because we used labels as strings we will want to convert them to numpy array with integers using the helper function we made earlier. # This function converts the class prediction to ints from strings because it was originally created as a string # See template notebook for more class_prediction = str_class_to_int ( class_prediction ) Visualize the Results \u00b6 First we'll make a colormap so we can adjust the colors of each class to more logical colors. def color_stretch ( image , index ): colors = image [:, :, index ] . astype ( np . float64 ) for b in range ( colors . shape [ 2 ]): colors [:, :, b ] = rasterio . plot . adjust_band ( colors [:, :, b ]) return colors # find the highest pixel value in the prediction image n = int ( np . max ( class_prediction )) # next setup a colormap for our map colors = dict (( ( 1 , ( 34 , 139 , 34 , 255 )), # Forest Green - PJ ( 2 , ( 139 , 69 , 19 , 255 )), # Brown - Perennial Shrub ( 3 , ( 48 , 156 , 214 , 255 )), # Blue - Encroached Shrub ( 4 , ( 244 , 164 , 96 , 255 )), # Tan - Loamy Bottom ( 5 , ( 206 , 224 , 196 , 255 )), # Lime - Grass Meadow )) # Put 0 - 255 as float 0 - 1 for k in colors : v = colors [ k ] _v = [ _v / 255.0 for _v in v ] colors [ k ] = _v index_colors = [ colors [ key ] if key in colors else ( 255 , 255 , 255 , 0 ) for key in range ( 0 , n + 1 )] cmap = plt . matplotlib . colors . ListedColormap ( index_colors , 'Classification' , n + 1 ) show ( class_prediction ) <matplotlib.axes._subplots.AxesSubplot at 0xbeb7d2278> with rasterio . Env (): profile . update ( dtype = rasterio . uint8 , count = 1 , compress = 'lzw' ) with rasterio . open ( r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\example.tif' , 'w' , ** profile ) as dst : dst . write ( class_prediction . astype ( rasterio . uint8 ), 1 ) Improving our Model Accuracy \u00b6 In the following sections (TBD), we'll improve upon our model, evaluate accuracy, and explore different classifiers * Normalize values * Split into training and testing data * Balance classes in training data * Introduce additional features (elevation, precip, soil temperature, aspect, moving window stats, imagery) * Explore different classifiers * Feature Importance * Parameter tuning * Correlations Evaluation Accuracy","title":"Machine Learning Tutorial"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#machine-learning-tutorial","text":"This tutorial is intended to illustrate a typical workflow for machine learning to solve a land use and land cover classification problem. Land Use and Land Cover classifications are used to identify the dominant land cover or land use type in an area. We use the Naive Bayes and Random Forest classifiers, as implemented within scikit-learn library. This tutorial borrows heavily from the very helpful tutorial developed by Chris Holden and updated by Patrick Gray. Also used: rasterio , geopandas , numpy , pandas , shapely , and matplotlib .","title":"Machine Learning Tutorial"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#the-challenge","text":"Our client required a rapid approach for evaluating the benefits of conservation projects to Mule Deer. We proposed using Ecological State and Transition Models (STMs) as the basis for the evaluation. The NRCS has developed STMs for the dominant ecological sites within the region, however only a subset of the region was mapped. We use vegetation data (provided by the Rangelands App ) and other environmental variables to predict STM for the unmapped areas of the range.","title":"The Challenge"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#import-statements","text":"import rasterio from rasterio.mask import mask from rasterio.plot import show from rasterio.plot import show_hist from rasterio.windows import Window from rasterio.plot import reshape_as_raster , reshape_as_image import geopandas as gpd import numpy as np from shapely.geometry import mapping from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn import preprocessing from sklearn.pipeline import make_pipeline from sklearn.utils import resample import matplotlib.pyplot as plt # ipywidgets is used to create a progress bar from ipywidgets import IntProgress from IPython.display import display % matplotlib inline","title":"Import Statements"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#import-data","text":"We'll be using supervised classification techniques. We'll need both labels and predictor features to train the models. Our labels will come from the mapped STM derived from NRCS data. To begin, we'll use vegetation data as predictors, including cover estimates for trees, shrubs, perennial grasses and forbs, and bare ground. # CHANGE TO VEG_COVER_PATH # Read in features and training data train_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\train_clip_utm.tif' data = rasterio . open ( train_path ) data . crs # Check the projection, all features must share a projection CRS.from_dict(init='epsg:26913') labels_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\labels.shp' labels = gpd . read_file ( labels_path ) labels . crs {'init': 'epsg:26913'} len ( labels ) 6136 We have 6,136 labeled polygons, represented within a geopandas dataframe, while our predictor features are represented in a rasterio raster. Both share the same projection.","title":"Import Data"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#explore-data-and-preparation-steps","text":"To train the classifiers, we'll need to associate our vector data (labels as polygons) with our raster pixels (predictor features). We'll accomplish this with the rasterio mask function. The mask function will essentially clip (or mask) our raster with each polygon. First, we'll want to extract the geometry of each feature in the labels shapefile to GeoJSON format. # this generates a list of shapely geometries geoms = labels . geometry . values # let's grab a single shapely geometry to check geometry = geoms [ 0 ] print ( \"This is a shapely polygon\" ) print ( type ( geometry )) print ( geometry ) # transform to GeoJSON format (note 'mapping' is in the shapely namespace) feature = [ mapping ( geometry )] # can also do this using polygon.__geo_interface__ print ( \"This is the same polygon in GeoJSON format\" ) print ( type ( feature )) print ( feature ) This is a shapely polygon <class 'shapely.geometry.polygon.Polygon'> POLYGON ((155090.8039999995 4317864.1029, 155095.9642000003 4317847.2246, 155096.9452999998 4317767.406400001, 155079.9972999999 4317764.4924, 155013.6201999998 4317772.8774, 154946.0262000002 4317762.761399999, 154906.4253000002 4317765.351500001, 154878.2922999999 4317781.742900001, 154874.5252 4317788.509099999, 154877.2857999997 4317829.973099999, 154890.6249000002 4317873.230799999, 154892.2852999996 4317898.202400001, 154873.1852000002 4318020.8462, 154866.6454999996 4318032.8059, 154861.3509 4318094.8554, 154880.1355999997 4318314.824999999, 154876.2439000001 4318350.6874, 154879.5504999999 4318400.631100001, 154884.4243000001 4318410.842, 154890.0175999999 4318527.3519, 154897.2582 4318573.0209, 154890.3213 4318594.527100001, 154894.7226999998 4318598.269300001, 154913.1912000002 4318593.0416, 154946.1224999996 4318553.260600001, 154952.6897 4318463.052999999, 154967.4232000001 4318401.3926, 154951.2766000004 4318220.3751, 154975.3370000003 4318141.542099999, 155019.5067999996 4318081.9695, 155041.7379000001 4318038.8718, 155041.0279000001 4317965.691199999, 155054.3926999997 4317914.6457, 155080.8075000001 4317871.2906, 155090.8039999995 4317864.1029)) This is the same polygon in GeoJSON format <class 'list'> [{'type': 'Polygon', 'coordinates': (((155090.80399999954, 4317864.1029), (155095.96420000028, 4317847.2246), (155096.9452999998, 4317767.406400001), (155079.99729999993, 4317764.4924), (155013.62019999977, 4317772.8774), (154946.0262000002, 4317762.761399999), (154906.42530000024, 4317765.351500001), (154878.29229999986, 4317781.742900001), (154874.52520000003, 4317788.509099999), (154877.28579999972, 4317829.973099999), (154890.62490000017, 4317873.230799999), (154892.28529999964, 4317898.202400001), (154873.18520000018, 4318020.8462000005), (154866.64549999963, 4318032.8059), (154861.35089999996, 4318094.8554), (154880.1355999997, 4318314.824999999), (154876.24390000012, 4318350.6874), (154879.5504999999, 4318400.631100001), (154884.42430000007, 4318410.842), (154890.0175999999, 4318527.3519), (154897.25820000004, 4318573.0209), (154890.32129999995, 4318594.5271000005), (154894.7226999998, 4318598.269300001), (154913.19120000023, 4318593.0416), (154946.1224999996, 4318553.260600001), (154952.6897, 4318463.052999999), (154967.42320000008, 4318401.3926), (154951.27660000045, 4318220.3751), (154975.3370000003, 4318141.542099999), (155019.50679999962, 4318081.9695), (155041.73790000007, 4318038.8718), (155041.0279000001, 4317965.691199999), (155054.39269999973, 4317914.6457), (155080.8075000001, 4317871.2906), (155090.80399999954, 4317864.1029)),)}] Now let's extract the raster values within each polygon using the rasterio mask() function . out_image , out_transform = mask ( data , feature , crop = True ) out_image . shape (6, 32, 10) show ( out_image [ 0 ]) <matplotlib.axes._subplots.AxesSubplot at 0xbea5c5b70> As you can see, the features raster was clipped to a single polygon. There are 6 bands and 32x10 pixels. We'll repeat this process for all 6,136 polygons to build our dataset. We'll also need to clean this raster up a bit before we use it in training. We'll explore all of this next. But first, we'll be doing a lot of memory intensive work so we'll close the dataset for now. data . close ()","title":"Explore Data and Preparation Steps"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#build-the-training-data-for-sckit-learn","text":"We'll repeat the above process for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels. Note that the column 'MuleDeer_1' in the labels geodataframe has the label we're after. TODO: change to column with label instead of numeric. %% time # Lets create a progress bar as this step can take some time p_bar = IntProgress ( min = 0 , max = len ( geoms ), description = 'Processing' ) display ( p_bar ) # Set up arrays X = np . array ([], dtype = np . int8 ) . reshape ( 0 , 6 ) # pixels for training y = np . array ([], dtype = np . string_ ) # labels for training # extract the raster values within the polygon with rasterio . open ( train_path ) as src : band_count = src . count for index , geom in enumerate ( geoms ): # Note geoms was created above feature = [ mapping ( geom )] # the mask function returns an array of the raster pixels within this feature out_image , out_transform = mask ( src , feature , crop = True ) # eliminate all the pixels with 0 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image [:, ~ np . all ( out_image == 0 , axis = 0 )] # eliminate all the pixels with 255 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image_trimmed [:, ~ np . all ( out_image_trimmed == 255 , axis = 0 )] # reshape the array to [pixel count, bands] out_image_reshaped = out_image_trimmed . reshape ( - 1 , band_count ) # append the labels to the y array y = np . append ( y , [ labels [ \"MuleDeer_1\" ][ index ]] * out_image_reshaped . shape [ 0 ]) # ??? # stack the pixels onto the pixel array X = np . vstack (( X , out_image_reshaped )) # increment the progress bar p_bar . value += 1 IntProgress(value=0, description='Processing', max=6136) Wall time: 25min 33s","title":"Build the Training Data for sckit-learn"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#save-the-output","text":"We'll save the output as a numpy array to avoid the long process of rebuilding the features in the future. This will also allow us to share this analysis with others without them needing access to the input data. If this were not a tutorial, I might have ended the notebook here and started a new one for the remainder of the analysis. I've commented out the load statements below, uncomment to load in the saved features and labels. # Save features and labels np . save ( 'E:/lulc-features.npy' , X ) np . save ( 'E:/lulc-labels.npy' , y ) # Load in data X = np . load ( 'E:/lulc-features.npy' ) # fill in path y = np . load ( 'E:/lulc-labels.npy' )","title":"Save the output"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#splitting-the-data-for-testing","text":"In order to evaluate the accuracy of our model, we'll reserve a subset of the data for testing. The train_test_split function allows us to quickly and randomly subset our data for this purpose. # split out 30% of data for testing. Random state set for reproducibility. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 )","title":"Splitting the data for testing"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#pairing-y-with-x","text":"Now that we have the image we want to classify (X_train) and the land cover labels (y_train), let's check to make sure they match in size so we can feed them to our models. st_names = np . unique ( y_train ) print ( 'The training data include {n} classes: {classes} \\n ' . format ( n = st_names . size , classes = st_names )) # We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels print ( 'Our X matrix is sized: {sz} ' . format ( sz = X_train . shape )) print ( 'Our y array is sized: {sz} ' . format ( sz = y_train . shape )) The training data include 5 classes : [ ' Encroached Shrub ' ' Loamy Bottom ' ' P-J ' ' Perennial Shrub ' ' Wet/Salt Meadow ' ] Our X matrix is sized : ( 4755679 , 6 ) Our y array is sized : ( 4755679 , ) That looks good. We have 5 classes (i.e., our STM names); 6 predictor features (i.e., the 6 bands in our X matrix, now flattened; and both the X and y array are the same length. We'll treat these vegetation cover values as spectral signatures, and plot each to make sure they're actually separable since all we're going by in this classification is pixel values. fig , ax = plt . subplots ( 1 , 3 , figsize = [ 20 , 8 ]) # bands are numbered 1 through 6 following GDAL convention band_count = np . arange ( 1 , 7 ) classes = np . unique ( y_train ) for class_type in classes : band_intensity = np . mean ( X_train [ y_train == class_type , :], axis = 0 ) ax [ 0 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 1 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 2 ] . plot ( band_count , band_intensity , label = class_type ) #plot them as lines # Add some axis labels # Add some axis labels ax [ 0 ] . set_xlabel ( 'Band #' ) ax [ 0 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_xlabel ( 'Band #' ) ax [ 2 ] . set_ylabel ( 'Reflectance Value' ) ax [ 2 ] . set_xlabel ( 'Band #' ) ax [ 1 ] . legend ( loc = \"upper right\" ) # Add a title ax [ 0 ] . set_title ( 'Band Intensities Full Overview' ) ax [ 1 ] . set_title ( 'Band Intensities Lower Ref Subset' ) ax [ 2 ] . set_title ( 'Band Intensities Higher Ref Subset' ) plt . show () Looks like each class will be easily separable. This will be a helper function to convert class labels into indices so we're predicting to integers instead of strings. TODO: switch labels and numbers def str_class_to_int ( class_array ): class_array [ class_array == 'P-J' ] = 1 class_array [ class_array == 'Perennial Shrub' ] = 2 class_array [ class_array == 'Encroached Shrub' ] = 3 class_array [ class_array == 'Loamy Bottom' ] = 4 class_array [ class_array == 'Wet/Salt Meadow' ] = 5 return ( class_array . astype ( int ))","title":"Pairing y with X"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#training-the-classifier","text":"Now that we have our X matrix of feature inputs (the vegetation cover bands) and our y array (the labels), we can train our model. Visit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn . gnb = GaussianNB () gnb . fit ( X_train , y_train ) GaussianNB ( priors = None , var_smoothing = 1e-09 ) It's that simple to train a classifier in sckit-learn . The hard part is often validation and interpretation.","title":"Training the Classifier"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#validation","text":"To see how well our classifier worked, we could use the test data we partioned earlier. However, we may want to adjust the model if our results are not as accurate as we'd like. This could lead to overfitting by 'leaking' information from the test set into our training of the model. Overfitting will hurt the performance of our model on predicting novel data, and will lead to inflated accuracy metrics. So how do we evaluate our model at this stage? Cross-validation . There are a few options for cross-validation, but for our purposes k-fold cross validation will work. # 5-fold cross validation scores = cross_val_score ( gnb , X_train , y_train , cv = 5 ) scores stores the results of computing the score 5 consecutive times (with different splits each time) scores array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The mean score and the 95% confidence interval of the score estimate are hence given by: print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) Accuracy : 0.59 (+/- 0.002 )","title":"Validation"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#improving-model-accuracy","text":"","title":"Improving Model Accuracy"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#standardizing-values","text":"The accuracy is not as good as we'd like. How can we improve the accuracy of the model? One option is to standardize the data so that each of the features are similar in magnitude. This avoids some higher values from overwhelming lower values. It is often necessary to complete this step, depending on the model used. sklearn provides a preprocessing module that facilitate this scaling. # Hide warnings for converting ints to floats (or save X, y as float64 type) import warnings warnings . filterwarnings ( \"ignore\" ) gnb = make_pipeline ( preprocessing . StandardScaler (), GaussianNB ()) cross_val_score ( gnb , X_train , y_train , cv = 5 ) array([0.59125447, 0.59230899, 0.59061017, 0.59067851, 0.59257414]) The accuracy is exactly the same! This is because the Gaussian Naive Bayes is robust to scaling. In essence Gaussian Naive Bayes performs standardization internally .","title":"Standardizing Values"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#balancing-classes","text":"Unbalanced classes can also impact the accuracy of a model. Imagine you have data on a rare disease that only 0.01% of people have. A simple model that predicts everyone does not have the disease would be right 99.99% of the time! To get a model that can actually predict when people do have the disease, you'll need to address the imbalanced classes. Again, this is an issue depending on the model used. We'll try a different type of model later in the tutorial that is more robust to imbalanced data (Random Forests). To balance the data, you can either down-sample the over-represented classes or up-sample the under-represented classes.","title":"Balancing Classes"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#how-unbalanced-are-the-classes-now","text":"Let's quickly plot the number of each label so we know how unbalanced the data are. import pandas as pd df_plot = pd . DataFrame ( y_train ) df_plot [ 0 ] . value_counts () . plot ( kind = 'bar' ) <matplotlib.axes._subplots.AxesSubplot at 0x22899b1eda0> We'll try up-sampling first so we don't reduce the number of data points too much. How much to upsample? We'll resample each of the less represented features with replacement to get the number of features contained in the most represented class. df_plot [ 0 ] . value_counts () P-J 2877442 Perennial Shrub 1388305 Encroached Shrub 375820 Loamy Bottom 85357 Wet/Salt Meadow 28755 Name: 0, dtype: int64 This will be easier with a pandas DataFrame, so let's convert our data to a DataFrame. df = pd . concat ( [ pd . DataFrame ( y_train , columns = [ 'label' ]), pd . DataFrame ( X_train )], axis = 1 ) . set_index ( 'label' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 label P-J 18 19 12 12 11 13 P-J 9 9 7 8 7 13 P-J 18 24 21 22 20 20 Perennial Shrub 14 18 16 12 14 13 P-J 4 5 9 9 7 7 df_shrub = resample ( df . loc [ 'Perennial Shrub' ], replace = True , n_samples = df . index . value_counts () . max ()) df_shrub . shape [ 0 ] 2877442 Now we have the same number of Perennial Shrub classes as P-J classes, our dominant class type. Let's repeat for the remaining features. We'll loop through our labels and concatenate the results to the most represented class so that this step is robust to changes in which features we explore. df . index . value_counts () . max () 2877442 # Start by subsetting the most represented class max_class = df . index . value_counts () . idxmax () df_upsampled = df . loc [ max_class ] labels = list ( df . index . unique ()) labels . remove ( max_class ) # For each label, resample the features to balance classes and append # to the empty dataframe for label in labels : df_temp = resample ( df . loc [ label ], replace = True , n_samples = df . index . value_counts () . max () ) df_upsampled = pd . concat ([ df_upsampled , df_temp ], axis = 0 ) df_upsampled . index . value_counts () P-J 2877442 Encroached Shrub 2877442 Perennial Shrub 2877442 Wet/Salt Meadow 2877442 Loamy Bottom 2877442 Name: label, dtype: int64 Now, we can split out our X and y data again and re-train the model on the more balanced classes. X_train_upsampled = df_upsampled . reset_index () . drop ( 'label' , axis = 1 ) y_train_upsampled = df_upsampled . index . values # 5-fold cross validation scores = cross_val_score ( gnb , X_train_upsampled , y_train_upsampled , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) Accuracy : 0.24 (+/- 0.001 ) Our accuracy took a bit of a nosedive, as now we're probably over-representing those less common classes on the landscape. For the purposes of our model, it may actually be better to simply ignore those more rare classes instead of over-representing them. We could try downsampling instead, but it will likely not help our accuracy very much. Instead, let's try a different model, Random Forests.","title":"How unbalanced are the classes now?"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#alternative-model-random-forests","text":"Random Forests is robust to unscaled and unbalanced data, making it a good option for this classification problem out-of-the-box. It's not a bad idea to scale the data as we did earlier, but since our data is well scaled (percent cover data from 0 - 100%), we'll skip this step. Acknowledgements to this tutorial used in developing this section. %% time from sklearn.ensemble import RandomForestClassifier # Initialize our model with 10 estimators to limit processing time rfc = RandomForestClassifier ( n_estimators = 10 , random_state = 0 ) # 5-fold cross validation scores = cross_val_score ( rfc , X_train , y_train , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) Accuracy : 0.58 (+/- 0.001 ) Wall time : 3 h 13 min 13 s Our accuracy (57%) is slightly less than our accuracy with the unbalanced classes using Naive Bayes (59%) but comparable.","title":"Alternative Model: Random Forests"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#confusion-matrix","text":"We can visualize how well we're classifying each class (and where the model is getting confused) using a confusion matrix . The code for the confusion matrix is copied from the linked documentation. from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels def plot_confusion_matrix ( y_true , y_pred , classes , normalize = False , title = None , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if not title : if normalize : title = 'Normalized confusion matrix' else : title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix ( y_true , y_pred ) # Only use the labels that appear in the data classes = unique_labels ( y_true , y_pred ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) fig , ax = plt . subplots ( figsize = ( 15 , 10 )) im = ax . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) ax . figure . colorbar ( im , ax = ax ) # We want to show all ticks... ax . set ( xticks = np . arange ( cm . shape [ 1 ]), yticks = np . arange ( cm . shape [ 0 ]), # ... and label them with the respective list entries xticklabels = classes , yticklabels = classes , title = title , ylabel = 'True label' , xlabel = 'Predicted label' ) # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 45 , ha = \"right\" , rotation_mode = \"anchor\" ) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i in range ( cm . shape [ 0 ]): for j in range ( cm . shape [ 1 ]): ax . text ( j , i , format ( cm [ i , j ], fmt ), ha = \"center\" , va = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) fig . tight_layout () return ax # Use the model to predict on the X_train dataset y_pred = gnb . fit ( X_train , y_train ) . predict ( X_train ) class_names = unique_labels ( y_train , y_pred ) np . set_printoptions ( precision = 2 ) # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.90e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.91e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.17e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.71e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] The confusion matrix indicates that the model is catogorizing most pixels as P-J and some as Perennial Shrub, with just a few Wet Meadow predictions. It ignores Encroached Shrub and Loamy Bottom. Focusing on the final row of the matrix, you can see that the Wet/Salt Meadow is being categorized as P-J 75% of the time, Perennial Shrub 24% of the time, and its correct label only 1% of the time. y_pred_rfc = rfc . fit ( X_train , y_pred ) . predict ( X_train ) # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred_rfc , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () Normalized confusion matrix [[0.00e+00 0.00e+00 8.77e-01 1.22e-01 2.87e-04] [0.00e+00 0.00e+00 8.13e-01 1.84e-01 2.85e-03] [0.00e+00 0.00e+00 8.86e-01 1.14e-01 8.06e-05] [0.00e+00 0.00e+00 8.01e-01 1.89e-01 9.69e-03] [0.00e+00 0.00e+00 7.46e-01 2.42e-01 1.17e-02]] Interestingly, the confusion matrix for both the Naive Bayes and Random Forests model is the same. This may signal that we've done about as well as we could with these features. We could incorporate additional features through the same process as we went through before, considering topography, soils, precipitation, and spatial measures using distances or moving windows. We should also carefully consider feature selection to optimize the bias-variance tradeoffs. Feature engingeering and feature selection is beyond the scope of this tutorial. A classification report will provide reportable metrics of model accuracy to allow us document the performance of the model. It reports precision, recall and the F1 score. Precision is the ability of the model to avoid false positives. Recall is the ability to identify the feature correctly (notice it is equivalent to where the diagonal axis of the confusion matrix). The F1 score is a harmonic mean of precision and recall. from sklearn.metrics import classification_report print ( classification_report ( y_train , y_pred , target_names = class_names )) C:\\Users\\Erik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) precision recall f1-score support Encroached Shrub 0.00 0.00 0.00 375820 Loamy Bottom 0.00 0.00 0.00 85357 P-J 0.62 0.89 0.73 2877442 Perennial Shrub 0.40 0.19 0.26 1388305 Wet/Salt Meadow 0.02 0.01 0.02 28755 micro avg 0.59 0.59 0.59 4755679 macro avg 0.21 0.22 0.20 4755679 weighted avg 0.49 0.59 0.52 4755679 \u200b","title":"Confusion Matrix"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#predicting-on-the-image","text":"With our classifier fit, we can now proceed by trying to classify the entire image. range_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\range_clip_utm.tif' with rasterio . open ( range_path ) as src : profile = src . profile # the src profile will be used to save the output later img = src . read () # Take our full iamge and reshape into long 2d array (nrow * ncol, nband) for classification print ( img . shape ) reshaped_img = reshape_as_image ( img ) print ( reshaped_img . shape ) (6, 9412, 7341) (9412, 7341, 6) Now we can predict for each pixel in our image. class_prediction = gnb . predict ( reshaped_img . reshape ( - 1 , 6 )) # Reshape our classification map back into a 2d matrix so we can visualize it class_prediction = class_prediction . reshape ( reshaped_img [:, :, 0 ] . shape ) Because we used labels as strings we will want to convert them to numpy array with integers using the helper function we made earlier. # This function converts the class prediction to ints from strings because it was originally created as a string # See template notebook for more class_prediction = str_class_to_int ( class_prediction )","title":"Predicting on the Image"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#visualize-the-results","text":"First we'll make a colormap so we can adjust the colors of each class to more logical colors. def color_stretch ( image , index ): colors = image [:, :, index ] . astype ( np . float64 ) for b in range ( colors . shape [ 2 ]): colors [:, :, b ] = rasterio . plot . adjust_band ( colors [:, :, b ]) return colors # find the highest pixel value in the prediction image n = int ( np . max ( class_prediction )) # next setup a colormap for our map colors = dict (( ( 1 , ( 34 , 139 , 34 , 255 )), # Forest Green - PJ ( 2 , ( 139 , 69 , 19 , 255 )), # Brown - Perennial Shrub ( 3 , ( 48 , 156 , 214 , 255 )), # Blue - Encroached Shrub ( 4 , ( 244 , 164 , 96 , 255 )), # Tan - Loamy Bottom ( 5 , ( 206 , 224 , 196 , 255 )), # Lime - Grass Meadow )) # Put 0 - 255 as float 0 - 1 for k in colors : v = colors [ k ] _v = [ _v / 255.0 for _v in v ] colors [ k ] = _v index_colors = [ colors [ key ] if key in colors else ( 255 , 255 , 255 , 0 ) for key in range ( 0 , n + 1 )] cmap = plt . matplotlib . colors . ListedColormap ( index_colors , 'Classification' , n + 1 ) show ( class_prediction ) <matplotlib.axes._subplots.AxesSubplot at 0xbeb7d2278> with rasterio . Env (): profile . update ( dtype = rasterio . uint8 , count = 1 , compress = 'lzw' ) with rasterio . open ( r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\example.tif' , 'w' , ** profile ) as dst : dst . write ( class_prediction . astype ( rasterio . uint8 ), 1 )","title":"Visualize the Results"},{"location":"data-science/machine-learning-tutorial/machine-learning-tutorial/#improving-our-model-accuracy","text":"In the following sections (TBD), we'll improve upon our model, evaluate accuracy, and explore different classifiers * Normalize values * Split into training and testing data * Balance classes in training data * Introduce additional features (elevation, precip, soil temperature, aspect, moving window stats, imagery) * Explore different classifiers * Feature Importance * Parameter tuning * Correlations Evaluation Accuracy","title":"Improving our Model Accuracy"},{"location":"data-visualization/dashboards/","text":"Dashboards \u00b6 Dashboards promise to distill meaning from complexity and facilitate at-a-glance performance monitoring. However, if poorly designed, they can result in an overwhelming array of incoherent data visualizations that confuse more than illuminate. There's a reason we don't all walk around wearing Tony Stark's helmet . This page will help you design a dashboard and select a platform to implement it. Designing a dashboard \u00b6 If you haven't already, review the introduction to data visualization ; it will help you through the process of designing the components of the dashboard. Here are some additional considerations specific to design of a dashboard. Consider audience and purpose \u00b6 As with any data visualization, you'll want to start by considering the purpose of the dashboard and who will be using it. You should list and prioritize the questions you'd like your dashboard to address before you start designing. Dashboards are commonly categorized as either operational, strategic, or analytical. Operational dashboards focus on monitoring the status of key components of the system. Strategic dashboards allow for quick review of performance indicators. Analytical dashboards are intended to facilitate understanding of the system. Because the purpose of these dashboards are different, the design will also likely differ. Be concise and purposeful \u00b6 There is a real temptation to throw everything you can on a dashboard. Resist. If you can eliminate something, do it. If you can combine two things without cluttering or confusing, do it. Don't add animations or interactions simply because you can. Remember that your purpose is to help a decision maker answer a question quickly. Always weigh the value of an additional feature against the value for the decision maker. Keep it to one view \u00b6 In general, aim to keep your dashboard to a single screen. At the very least, you want to have one view that provides all of the highest priority information. If you must include more content through scrolling or additional pages, group content thematically so that there is no need to flip back and forth or scroll up and down. This might even mean repeating key information across multiple views. If you want your dashboard to work on mobile devices, you'll need a responsive design. Keep in mind that this will almost certainly mean a different flow of information. It might not be possible to have all information on a single screen. Treat the responsive layout as its own layout. Use interactions strategically \u00b6 Interactions allow the user to drill down into data, manipulate parameters, or personalize the dashboard. These can be very helpful for progressively revealing information while maintaining the layout and size of your dashboard. However, the purpose of your dashboard is to surface information, not hide it behind a bunch of widgets. Your dashboard is not a puzzle. Avoid burying important information in interactions. You may also want to link to source data to allow the user to quickly access key resources. Consider layout and flow \u00b6 The user's eye typically follows an 'F' pattern across a screen, glancing from left to right while progressing down the left margin (depending on your region). Thus, place the most important elements in the top left of the screen. The bottom right will have the least important information. Group like content together. Finally, use white space liberally. An overly-dense data display can be disorienting and difficult to read. Dashboards are often designed with cards , which allow for easy alignment and responsive design. Each card represents one element. You might divide your dashboard into a few rows and a handful of columns before deciding which element should go where. One element may take up multiple cells, but no cell should be split between multiple elements. Be consistent with formatting \u00b6 Make sure numbers are formatted correctly. Use sans-serif fonts. Make labels descriptive. Use well-known abbreviations when possible to reduce clutter (but make sure your audience knows what they mean). Repeat colors across multiple elements to highlight their identities. Keep text horizontal as much as possible. Consistent formatting will help communicate information and make your dashboard more appealing to the eye. Provide context \u00b6 Metrics should be presented with context. Show change over time, indicate the change since yesterday, illustrate the difference between the number and its target, color low performance red and high performance green. Just make sure your audience can quickly gauge the significance of any number and identify trends when it is important. If two indicators should be evaluated in relation to one another, keep them together. Selecting a platform \u00b6 Dashboards are having a moment right now as startups and Fortune-500 companies alike are increasingly using data in their decision making. Platforms abound. Many of these are designed for marketing analytics and business intelligence use cases, but should have sufficient functionality for our purposes. Here are a few considerations when selecting a platform. Cost \u00b6 There are free options and there are subscription-based options. Subscriptions are not unreasonable, ranging from $30 to $200 a month and up. The budget options limit features like the amount of data you can use, the number of data sources, the number of dashboards, and the frequency of data updates. Carefully review the pricing tiers and their associated limitations before selecting a paid option. Sharing and hosting \u00b6 Consider how you will share the dashboard and where it will be hosted. I would suggest that to be really useful the dashboard should be available on the web. Many cloud-based platforms are available for hosting a web-based dashboard. However, make sure you don't share any personal or sensitive information publicly. Integrations \u00b6 Where do your data live now, and how are your data generated? You'll want whatever solution you select to integrate with your data source(s). Most solutions support an array of integrations, but if you are committed to using Google Sheets, and the platform doesn't support a Google Sheets integration, it won't work for you. Simplicity and customization \u00b6 There is a tradeoff between simplicity and customization. The truly easy-to-use platforms do not offer much in the way of customization (which is why they are easy to use). You won't always know what a platform can't do until you try to make it do that thing, which can be very frustrating. Stick to the reviewed platforms below and we can help you understand what can, and can't, be done. Blending and computation \u00b6 Data blending is the process of taking two data sources and combining them in a single dashboard. You may also want to do some computation on the data. Different platforms have different support for blending and computation. Many platforms require you use SQL (Structured Query Language) to do anything but the most basic data blending. Computations are typically performed in a language similar to what you would use in Excel or Google Sheets (e.g., SUM(A, B)). If you need to do much computation, and you don't want to build an intermediary step into your data pipeline to do the computation, you will need to select a platform that has robust computation support. Growing with your platform \u00b6 Finally, consider how your dashboard needs may grow over time. Can you easily extend your chosen platform as you grow? You might need to add dashboards for different teams, handle larger amounts of data, customize views for different users, etc. Most solutions will be capable of growing with your needs, but with subscription-based options that may come at a cost. Platforms reviewed \u00b6 The platforms reviewed below were selected because they include a free tier and have reasonably good functionality as dashboards. Keep scrolling to see examples of each. Excel/Google Sheets Dash Tableau Public Google Data Studio Power BI Fine Report Others include Plecto, Geckoboard, Chartio, Looker, Klipfolio, Holistics, Sisense, Domo, Qlik, Databox, DataHero - literally so many options. If you're able to pay for a platform, evaluate a number of options against the considerations listed above to find one that best meets your needs and budget. Excel/Google Sheets \u00b6 You're familiar with Excel's charts and data manipulation language, your data is already in there, why not just create a dashboard here? You can use pivot charts, filters and timeline slicers to provide some interactivity. There are even basic maps you can use to show chloropleths. Just keep in mind the limitations: you'll need to share the workbook to share your dashboard and your data will go along with it - which is fine for a couple users but could be problematic with many users or with sensitive data. If you're looking for more functionality that still integrates with your data, check out Power BI (for Excel) or Data Studio (for Google Sheets). Tableau Public \u00b6 Tableau is capable of creating beautiful visualizations, but at the free tier will require you to share your dashboard publicly - a non-starter for personal or sensitive data. Subscriptions are $70 per user per month for what they call 'Creator', while additional users to view the dashboard (securely) can be added for $15 per user per month. But, if you don't mind the world seeing your dashboard, this is a great option. Credit: Sam Drole, Originally published on Tableau Public. Dash \u00b6 Dash is a Python library for building analytic web apps. In terms of simplicity vs customizability, this solution skews heavily towards customizability. However, it's free, you can host it anywhere, it integrates with almost everything, computation is nearly unlimited, and it will scale without issue. (It's also a great option for interactive reports, which I'll hope to cover in this wiki at some point.) The only catch, you'll need to know Python to develop and maintain it. See more here . Google Data Studio \u00b6 Google's Data Studio is a great dashboard tool if you're already using Google Sheets and need to build something quickly. You'll probably be disappointed with it's ability to blend data and compute. The functions available are rather limited as compared with Google Sheets, and the values users pass to widgets aren't exposed for computation (e.g., the start and end dates of the filter below). However, it's a great option if you have data that is in one place and already computed (e.g., a budget sheet). There are a number of options for restricted sharing, similar to Google Sheets. Best of all, it's totally free. Power BI \u00b6 Power BI is desktop software that allows for publishing and sharing dashboards to the web. If your team is already using Microsoft this may be a good option. An individual license is free, but to share dashboards you'll need to pay $9.99 per user per month (sold as an annual subscription) to upgrade to Power BI Pro. You can only share with others who also have Power BI Pro; to share with those that do not have Power BI Pro, you'll need to have a Premium capacity which costs $5,000. Read up here before fully investing in this solution. Fine Report \u00b6 Fine Report has a lot of functionality, but I find it to be rather unappealing aesthetically. They seem to have spent most of their time developing widgets that would look good in the 2002 sci-fi thriller Minority Report but don't actually communicate any meaning. Don't build a dashboard like the one pictured below (which is from their website). However, as a truly free and no-code solution that still has a good deal of functionality, Fine Report may be your best option - just don't fall for the temptation of their 'Extended Charts'. Check out their full library of examples. ArcGIS Story Maps & Dashboard \u00b6 ArcGIS StoryMaps are often used when the dashboard will include a map component. However, the map doesn't need to be the focus of the dashboard. In fact, some developers who are skilled with ArcGIS Story Maps will use this as their go-to dashboard. In my experience, it's best as a 'scrolly-telling' application, where charts are stacked one top of one another and the user scrolls through to reveal additional data. These can be created for free, but you'll want to use our license to develop the input layers for any maps you create. This is a great example . ESRI recently released ArcGIS Dashboards as a solution for dashboard-specific applications. Using this solution requires a license for ArcGIS Online.","title":"Dashboards"},{"location":"data-visualization/dashboards/#dashboards","text":"Dashboards promise to distill meaning from complexity and facilitate at-a-glance performance monitoring. However, if poorly designed, they can result in an overwhelming array of incoherent data visualizations that confuse more than illuminate. There's a reason we don't all walk around wearing Tony Stark's helmet . This page will help you design a dashboard and select a platform to implement it.","title":"Dashboards"},{"location":"data-visualization/dashboards/#designing-a-dashboard","text":"If you haven't already, review the introduction to data visualization ; it will help you through the process of designing the components of the dashboard. Here are some additional considerations specific to design of a dashboard.","title":"Designing a dashboard"},{"location":"data-visualization/dashboards/#consider-audience-and-purpose","text":"As with any data visualization, you'll want to start by considering the purpose of the dashboard and who will be using it. You should list and prioritize the questions you'd like your dashboard to address before you start designing. Dashboards are commonly categorized as either operational, strategic, or analytical. Operational dashboards focus on monitoring the status of key components of the system. Strategic dashboards allow for quick review of performance indicators. Analytical dashboards are intended to facilitate understanding of the system. Because the purpose of these dashboards are different, the design will also likely differ.","title":"Consider audience and purpose"},{"location":"data-visualization/dashboards/#be-concise-and-purposeful","text":"There is a real temptation to throw everything you can on a dashboard. Resist. If you can eliminate something, do it. If you can combine two things without cluttering or confusing, do it. Don't add animations or interactions simply because you can. Remember that your purpose is to help a decision maker answer a question quickly. Always weigh the value of an additional feature against the value for the decision maker.","title":"Be concise and purposeful"},{"location":"data-visualization/dashboards/#keep-it-to-one-view","text":"In general, aim to keep your dashboard to a single screen. At the very least, you want to have one view that provides all of the highest priority information. If you must include more content through scrolling or additional pages, group content thematically so that there is no need to flip back and forth or scroll up and down. This might even mean repeating key information across multiple views. If you want your dashboard to work on mobile devices, you'll need a responsive design. Keep in mind that this will almost certainly mean a different flow of information. It might not be possible to have all information on a single screen. Treat the responsive layout as its own layout.","title":"Keep it to one view"},{"location":"data-visualization/dashboards/#use-interactions-strategically","text":"Interactions allow the user to drill down into data, manipulate parameters, or personalize the dashboard. These can be very helpful for progressively revealing information while maintaining the layout and size of your dashboard. However, the purpose of your dashboard is to surface information, not hide it behind a bunch of widgets. Your dashboard is not a puzzle. Avoid burying important information in interactions. You may also want to link to source data to allow the user to quickly access key resources.","title":"Use interactions strategically"},{"location":"data-visualization/dashboards/#consider-layout-and-flow","text":"The user's eye typically follows an 'F' pattern across a screen, glancing from left to right while progressing down the left margin (depending on your region). Thus, place the most important elements in the top left of the screen. The bottom right will have the least important information. Group like content together. Finally, use white space liberally. An overly-dense data display can be disorienting and difficult to read. Dashboards are often designed with cards , which allow for easy alignment and responsive design. Each card represents one element. You might divide your dashboard into a few rows and a handful of columns before deciding which element should go where. One element may take up multiple cells, but no cell should be split between multiple elements.","title":"Consider layout and flow"},{"location":"data-visualization/dashboards/#be-consistent-with-formatting","text":"Make sure numbers are formatted correctly. Use sans-serif fonts. Make labels descriptive. Use well-known abbreviations when possible to reduce clutter (but make sure your audience knows what they mean). Repeat colors across multiple elements to highlight their identities. Keep text horizontal as much as possible. Consistent formatting will help communicate information and make your dashboard more appealing to the eye.","title":"Be consistent with formatting"},{"location":"data-visualization/dashboards/#provide-context","text":"Metrics should be presented with context. Show change over time, indicate the change since yesterday, illustrate the difference between the number and its target, color low performance red and high performance green. Just make sure your audience can quickly gauge the significance of any number and identify trends when it is important. If two indicators should be evaluated in relation to one another, keep them together.","title":"Provide context"},{"location":"data-visualization/dashboards/#selecting-a-platform","text":"Dashboards are having a moment right now as startups and Fortune-500 companies alike are increasingly using data in their decision making. Platforms abound. Many of these are designed for marketing analytics and business intelligence use cases, but should have sufficient functionality for our purposes. Here are a few considerations when selecting a platform.","title":"Selecting a platform"},{"location":"data-visualization/dashboards/#cost","text":"There are free options and there are subscription-based options. Subscriptions are not unreasonable, ranging from $30 to $200 a month and up. The budget options limit features like the amount of data you can use, the number of data sources, the number of dashboards, and the frequency of data updates. Carefully review the pricing tiers and their associated limitations before selecting a paid option.","title":"Cost"},{"location":"data-visualization/dashboards/#sharing-and-hosting","text":"Consider how you will share the dashboard and where it will be hosted. I would suggest that to be really useful the dashboard should be available on the web. Many cloud-based platforms are available for hosting a web-based dashboard. However, make sure you don't share any personal or sensitive information publicly.","title":"Sharing and hosting"},{"location":"data-visualization/dashboards/#integrations","text":"Where do your data live now, and how are your data generated? You'll want whatever solution you select to integrate with your data source(s). Most solutions support an array of integrations, but if you are committed to using Google Sheets, and the platform doesn't support a Google Sheets integration, it won't work for you.","title":"Integrations"},{"location":"data-visualization/dashboards/#simplicity-and-customization","text":"There is a tradeoff between simplicity and customization. The truly easy-to-use platforms do not offer much in the way of customization (which is why they are easy to use). You won't always know what a platform can't do until you try to make it do that thing, which can be very frustrating. Stick to the reviewed platforms below and we can help you understand what can, and can't, be done.","title":"Simplicity and customization"},{"location":"data-visualization/dashboards/#blending-and-computation","text":"Data blending is the process of taking two data sources and combining them in a single dashboard. You may also want to do some computation on the data. Different platforms have different support for blending and computation. Many platforms require you use SQL (Structured Query Language) to do anything but the most basic data blending. Computations are typically performed in a language similar to what you would use in Excel or Google Sheets (e.g., SUM(A, B)). If you need to do much computation, and you don't want to build an intermediary step into your data pipeline to do the computation, you will need to select a platform that has robust computation support.","title":"Blending and computation"},{"location":"data-visualization/dashboards/#growing-with-your-platform","text":"Finally, consider how your dashboard needs may grow over time. Can you easily extend your chosen platform as you grow? You might need to add dashboards for different teams, handle larger amounts of data, customize views for different users, etc. Most solutions will be capable of growing with your needs, but with subscription-based options that may come at a cost.","title":"Growing with your platform"},{"location":"data-visualization/dashboards/#platforms-reviewed","text":"The platforms reviewed below were selected because they include a free tier and have reasonably good functionality as dashboards. Keep scrolling to see examples of each. Excel/Google Sheets Dash Tableau Public Google Data Studio Power BI Fine Report Others include Plecto, Geckoboard, Chartio, Looker, Klipfolio, Holistics, Sisense, Domo, Qlik, Databox, DataHero - literally so many options. If you're able to pay for a platform, evaluate a number of options against the considerations listed above to find one that best meets your needs and budget.","title":"Platforms reviewed"},{"location":"data-visualization/dashboards/#excelgoogle-sheets","text":"You're familiar with Excel's charts and data manipulation language, your data is already in there, why not just create a dashboard here? You can use pivot charts, filters and timeline slicers to provide some interactivity. There are even basic maps you can use to show chloropleths. Just keep in mind the limitations: you'll need to share the workbook to share your dashboard and your data will go along with it - which is fine for a couple users but could be problematic with many users or with sensitive data. If you're looking for more functionality that still integrates with your data, check out Power BI (for Excel) or Data Studio (for Google Sheets).","title":"Excel/Google Sheets"},{"location":"data-visualization/dashboards/#tableau-public","text":"Tableau is capable of creating beautiful visualizations, but at the free tier will require you to share your dashboard publicly - a non-starter for personal or sensitive data. Subscriptions are $70 per user per month for what they call 'Creator', while additional users to view the dashboard (securely) can be added for $15 per user per month. But, if you don't mind the world seeing your dashboard, this is a great option. Credit: Sam Drole, Originally published on Tableau Public.","title":"Tableau Public"},{"location":"data-visualization/dashboards/#dash","text":"Dash is a Python library for building analytic web apps. In terms of simplicity vs customizability, this solution skews heavily towards customizability. However, it's free, you can host it anywhere, it integrates with almost everything, computation is nearly unlimited, and it will scale without issue. (It's also a great option for interactive reports, which I'll hope to cover in this wiki at some point.) The only catch, you'll need to know Python to develop and maintain it. See more here .","title":"Dash"},{"location":"data-visualization/dashboards/#google-data-studio","text":"Google's Data Studio is a great dashboard tool if you're already using Google Sheets and need to build something quickly. You'll probably be disappointed with it's ability to blend data and compute. The functions available are rather limited as compared with Google Sheets, and the values users pass to widgets aren't exposed for computation (e.g., the start and end dates of the filter below). However, it's a great option if you have data that is in one place and already computed (e.g., a budget sheet). There are a number of options for restricted sharing, similar to Google Sheets. Best of all, it's totally free.","title":"Google Data Studio"},{"location":"data-visualization/dashboards/#power-bi","text":"Power BI is desktop software that allows for publishing and sharing dashboards to the web. If your team is already using Microsoft this may be a good option. An individual license is free, but to share dashboards you'll need to pay $9.99 per user per month (sold as an annual subscription) to upgrade to Power BI Pro. You can only share with others who also have Power BI Pro; to share with those that do not have Power BI Pro, you'll need to have a Premium capacity which costs $5,000. Read up here before fully investing in this solution.","title":"Power BI"},{"location":"data-visualization/dashboards/#fine-report","text":"Fine Report has a lot of functionality, but I find it to be rather unappealing aesthetically. They seem to have spent most of their time developing widgets that would look good in the 2002 sci-fi thriller Minority Report but don't actually communicate any meaning. Don't build a dashboard like the one pictured below (which is from their website). However, as a truly free and no-code solution that still has a good deal of functionality, Fine Report may be your best option - just don't fall for the temptation of their 'Extended Charts'. Check out their full library of examples.","title":"Fine Report"},{"location":"data-visualization/dashboards/#arcgis-story-maps-dashboard","text":"ArcGIS StoryMaps are often used when the dashboard will include a map component. However, the map doesn't need to be the focus of the dashboard. In fact, some developers who are skilled with ArcGIS Story Maps will use this as their go-to dashboard. In my experience, it's best as a 'scrolly-telling' application, where charts are stacked one top of one another and the user scrolls through to reveal additional data. These can be created for free, but you'll want to use our license to develop the input layers for any maps you create. This is a great example . ESRI recently released ArcGIS Dashboards as a solution for dashboard-specific applications. Using this solution requires a license for ArcGIS Online.","title":"ArcGIS Story Maps &amp; Dashboard"},{"location":"data-visualization/data-visualization/","text":"Intro to Data Visualization \u00b6 Data visualization is the art of telling a story, with data, visually. A good storyteller will consider their audience, provide context, focus on what's important, and deliver the payoff in the end. As will a great visual data storyteller. This tutorial will help you on your path to being a great visual data storyteller, and provide suggested technologies to make your ideas come to life. There are thousands of resources available to you to improve your visualization skills, including online classes , YouTube videos, web articles, blogs , books , competitions , etc. You can even get an advanced degree in data visualization if you're so inclined. What follows is simply a collection of the best concepts and advice I've encountered. I encourage you to explore other sources, experiment, and even throw this right out if it doesn't speak to you. The process \u00b6 Most stories - at least the ones worth listening to - begin with a question. How did you get that scar? Great story actually\u2026 Once you've identified a data visualization need, consider the people who make up your audience: for which questions will they need answers? List all the questions you can think of and highlight the most important. You might find it useful to group similar questions. Next, inventory the data sources at your disposal. You will, after all, need data to create a data visualization. As is typically true of working with data, you may spend a disproportionate amount of your time collecting and cleaning the data you want to use. If the data you want is not attainable, you'll need to reframe your approach. Before you start cleaning data and creating visuals, stop . Step away from the computer. Find a sunny patio or a dimly lit coffee shop. Get out a piece of paper (I prefer the gridded engineering graph paper). Grab your pencils (colored pencils if you like - although I recommend designing first in black and white before judiciously adding color). Sketch . Pick a question from your list that is the most interesting to you. Given your understanding of the data available, how would you answer that question with a data visual? Which type of chart might work best? Sketch a few alternatives to see how they compare. Pick another question and repeat. Sketching first will save you time by helping you explore the story you want to tell and how you want to tell it. You can more quickly prototype ideas on paper than in any visualization software. You also won't feel hamstrung by the limitations of the software, or your limitations in working with the software. Skipping this step is how we end up with so many of the same boring bar charts in Excel's standard color scheme. Now is a good time to share your sketches with members of your intended audience to get their reactions. If you're self-conscious about your sketching ability, transfer your best sketches to a tool like Microsoft Whiteboard . While you're at it, look online for inspiration and clip charts that inspire you next to your sketches. This is also a good time to co-create visuals with others. You can work together to pick the most important questions to answer and update the sketches to more intuitively answer them. With your draft visuals in hand, you should now have a clearer understanding of what type of visualization to develop and which visualization platform to use. Prepare and explore your data to see if you can discover new insights. Quickly test your sketches . Do the data work with (or against) your sketch? If your visuals will be dynamic (as with a dashboard, for example) test out different slices of the data or create random data to test edge cases. With many rounds of iteration, you will develop your visualizations . Next, strip away distraction and focus on the key message. This is where you make sure you deliver that payoff - the information your audience needs as succinctly, intuitively, and insightfully as possible. See the techniques and approaches sections for suggestions. Finally, deliver your visualization. To borrow from Paul Valery, \"a work of art is never finished, merely abandoned\" . The process, succinctly \u00b6 List the key questions of your audience Inventory data sources Sketch potential visuals Share your sketches with members of your audience Prepare and explore the data Test your sketches Develop your visualizations through iteration Strip away distraction Deliver Types of visualizations \u00b6 There's no official taxonomy of data visualizations, but here's a quick overview of a few important visualization types in my opinion: Chart : your standard static data visualization. The static chart's primary habitat is in reports and other static media. See also graphs. Infographic : a thematic collection of multiple static visualizations, often presented as a poster or banner. Dashboard : a thematic collection of one or more dynamic visualizations, updated at some regular interval. Data Story : a presentation of data that marries narrative with data visualization. See some great examples under Inspiration. Any of the above can support interactivity, animations, and/or maps. A static map would be considered a chart (just ask any sailor). By static I mean the underlying data do not change; data change with dynamic visualizations - which provides a unique challenge for visualizations. Visualization tools \u00b6 There are dozens (hundreds?) of visualization tools available to you. If you don't already have a preferred solution, here are a few suggestions. This list focuses on tools with a free tier or for which we have a license. Of course, there's always Excel or Google Sheets - nothing wrong with these options - but I'm assuming you've already been introduced. Tableau : these folks are visualization nerds and incorporate by default the latest in visualization science in your visuals - which makes you look smart. Tableau Public is free but, you guessed it, makes your visualization public (so don't choose this solution if you're working with sensitive data, or get a paid account). Power BI : Power BI is a free desktop tool with an online service to allow you to publish your visuals and create dashboards (publishing requires a paid subscription). If your team is already using MS Office, it\u2019s a good choice for its easy integration with the Microsoft ecosystem and ability to share visuals within your organization. Google Charts & Data Studio : this is Google's answer to Microsoft's Power BI. Slick and intuitive, you can very quickly learn to create visuals and dashboards in an afternoon that look like they took a week. Best of all, it's totally free. Plotly & Dash : Plotly's open-source Chart Studio allows you to upload data and quickly create interactive web-ready charts from their standard library. However, this solution really sings if you're able to use the Dash API with Python. Use this solution to develop and deploy analytic web apps, like interactive dashboards. Infogram : an option for creating infographics, Infogram allows up to 10 projects at the free tier. See also Visme , Adobe Illustrator , and Inkscape . Matplotlib & Seaborn : for Python developers, these are your go-to libraries for data visualization. ArcGIS StoryMaps : StoryMaps include straightforward interactive maps and the eponymous Story Map , a platform for creating data stories that typically (but don't have to!) include maps and other spatial components. (There are multiple solutions for geographic visualizations, we hope to cover these in the near future.) More tips on visualization \u00b6 If you're wondering which visualization to use for a given task, start with this Visualization Vocabulary . Techniques \u00b6 Try out some of these techniques to improve your visualizations. Highlighting Transition Guidance (layering the story, point by point) Ordering Messaging Clarity Reduce but don't avoid complexity Interaction How to tell a story \u00b6 These are approaches to telling your data story. Change Over Time Drill Down Zoom Out Contrast Intersections Factors Outliers Communicating Uncertainty Inspiration \u00b6 How did I create an entire post about data visualization without a single data visualization? Check these out: Galleries \u00b6 Tableau Public Gallery Makeover Monday Gallery Dash App Gallery Philosophies \u00b6 Data Humanism Some great examples \u00b6 The Avalanche at Snow Falls (NYT) What's really warming the world (Bloomberg) Bussed Out (The Guardian)","title":"Data Visualization"},{"location":"data-visualization/data-visualization/#intro-to-data-visualization","text":"Data visualization is the art of telling a story, with data, visually. A good storyteller will consider their audience, provide context, focus on what's important, and deliver the payoff in the end. As will a great visual data storyteller. This tutorial will help you on your path to being a great visual data storyteller, and provide suggested technologies to make your ideas come to life. There are thousands of resources available to you to improve your visualization skills, including online classes , YouTube videos, web articles, blogs , books , competitions , etc. You can even get an advanced degree in data visualization if you're so inclined. What follows is simply a collection of the best concepts and advice I've encountered. I encourage you to explore other sources, experiment, and even throw this right out if it doesn't speak to you.","title":"Intro to Data Visualization"},{"location":"data-visualization/data-visualization/#the-process","text":"Most stories - at least the ones worth listening to - begin with a question. How did you get that scar? Great story actually\u2026 Once you've identified a data visualization need, consider the people who make up your audience: for which questions will they need answers? List all the questions you can think of and highlight the most important. You might find it useful to group similar questions. Next, inventory the data sources at your disposal. You will, after all, need data to create a data visualization. As is typically true of working with data, you may spend a disproportionate amount of your time collecting and cleaning the data you want to use. If the data you want is not attainable, you'll need to reframe your approach. Before you start cleaning data and creating visuals, stop . Step away from the computer. Find a sunny patio or a dimly lit coffee shop. Get out a piece of paper (I prefer the gridded engineering graph paper). Grab your pencils (colored pencils if you like - although I recommend designing first in black and white before judiciously adding color). Sketch . Pick a question from your list that is the most interesting to you. Given your understanding of the data available, how would you answer that question with a data visual? Which type of chart might work best? Sketch a few alternatives to see how they compare. Pick another question and repeat. Sketching first will save you time by helping you explore the story you want to tell and how you want to tell it. You can more quickly prototype ideas on paper than in any visualization software. You also won't feel hamstrung by the limitations of the software, or your limitations in working with the software. Skipping this step is how we end up with so many of the same boring bar charts in Excel's standard color scheme. Now is a good time to share your sketches with members of your intended audience to get their reactions. If you're self-conscious about your sketching ability, transfer your best sketches to a tool like Microsoft Whiteboard . While you're at it, look online for inspiration and clip charts that inspire you next to your sketches. This is also a good time to co-create visuals with others. You can work together to pick the most important questions to answer and update the sketches to more intuitively answer them. With your draft visuals in hand, you should now have a clearer understanding of what type of visualization to develop and which visualization platform to use. Prepare and explore your data to see if you can discover new insights. Quickly test your sketches . Do the data work with (or against) your sketch? If your visuals will be dynamic (as with a dashboard, for example) test out different slices of the data or create random data to test edge cases. With many rounds of iteration, you will develop your visualizations . Next, strip away distraction and focus on the key message. This is where you make sure you deliver that payoff - the information your audience needs as succinctly, intuitively, and insightfully as possible. See the techniques and approaches sections for suggestions. Finally, deliver your visualization. To borrow from Paul Valery, \"a work of art is never finished, merely abandoned\" .","title":"The process"},{"location":"data-visualization/data-visualization/#the-process-succinctly","text":"List the key questions of your audience Inventory data sources Sketch potential visuals Share your sketches with members of your audience Prepare and explore the data Test your sketches Develop your visualizations through iteration Strip away distraction Deliver","title":"The process, succinctly"},{"location":"data-visualization/data-visualization/#types-of-visualizations","text":"There's no official taxonomy of data visualizations, but here's a quick overview of a few important visualization types in my opinion: Chart : your standard static data visualization. The static chart's primary habitat is in reports and other static media. See also graphs. Infographic : a thematic collection of multiple static visualizations, often presented as a poster or banner. Dashboard : a thematic collection of one or more dynamic visualizations, updated at some regular interval. Data Story : a presentation of data that marries narrative with data visualization. See some great examples under Inspiration. Any of the above can support interactivity, animations, and/or maps. A static map would be considered a chart (just ask any sailor). By static I mean the underlying data do not change; data change with dynamic visualizations - which provides a unique challenge for visualizations.","title":"Types of visualizations"},{"location":"data-visualization/data-visualization/#visualization-tools","text":"There are dozens (hundreds?) of visualization tools available to you. If you don't already have a preferred solution, here are a few suggestions. This list focuses on tools with a free tier or for which we have a license. Of course, there's always Excel or Google Sheets - nothing wrong with these options - but I'm assuming you've already been introduced. Tableau : these folks are visualization nerds and incorporate by default the latest in visualization science in your visuals - which makes you look smart. Tableau Public is free but, you guessed it, makes your visualization public (so don't choose this solution if you're working with sensitive data, or get a paid account). Power BI : Power BI is a free desktop tool with an online service to allow you to publish your visuals and create dashboards (publishing requires a paid subscription). If your team is already using MS Office, it\u2019s a good choice for its easy integration with the Microsoft ecosystem and ability to share visuals within your organization. Google Charts & Data Studio : this is Google's answer to Microsoft's Power BI. Slick and intuitive, you can very quickly learn to create visuals and dashboards in an afternoon that look like they took a week. Best of all, it's totally free. Plotly & Dash : Plotly's open-source Chart Studio allows you to upload data and quickly create interactive web-ready charts from their standard library. However, this solution really sings if you're able to use the Dash API with Python. Use this solution to develop and deploy analytic web apps, like interactive dashboards. Infogram : an option for creating infographics, Infogram allows up to 10 projects at the free tier. See also Visme , Adobe Illustrator , and Inkscape . Matplotlib & Seaborn : for Python developers, these are your go-to libraries for data visualization. ArcGIS StoryMaps : StoryMaps include straightforward interactive maps and the eponymous Story Map , a platform for creating data stories that typically (but don't have to!) include maps and other spatial components. (There are multiple solutions for geographic visualizations, we hope to cover these in the near future.)","title":"Visualization tools"},{"location":"data-visualization/data-visualization/#more-tips-on-visualization","text":"If you're wondering which visualization to use for a given task, start with this Visualization Vocabulary .","title":"More tips on visualization"},{"location":"data-visualization/data-visualization/#techniques","text":"Try out some of these techniques to improve your visualizations. Highlighting Transition Guidance (layering the story, point by point) Ordering Messaging Clarity Reduce but don't avoid complexity Interaction","title":"Techniques"},{"location":"data-visualization/data-visualization/#how-to-tell-a-story","text":"These are approaches to telling your data story. Change Over Time Drill Down Zoom Out Contrast Intersections Factors Outliers Communicating Uncertainty","title":"How to tell a story"},{"location":"data-visualization/data-visualization/#inspiration","text":"How did I create an entire post about data visualization without a single data visualization? Check these out:","title":"Inspiration"},{"location":"data-visualization/data-visualization/#galleries","text":"Tableau Public Gallery Makeover Monday Gallery Dash App Gallery","title":"Galleries"},{"location":"data-visualization/data-visualization/#philosophies","text":"Data Humanism","title":"Philosophies"},{"location":"data-visualization/data-visualization/#some-great-examples","text":"The Avalanche at Snow Falls (NYT) What's really warming the world (Bloomberg) Bussed Out (The Guardian)","title":"Some great examples"},{"location":"deployment/deployment-overview/","text":"Getting Tools to Users \u00b6 This page is under development Tools are only useful if they are used. Getting tools in the hands of your users, while ensuring a good user experience with tech support and the development of new features, is at least half the battle. A number of options exist for deploying tools. The deployment environment should be decided early on in tool development. Here's a short list of options (and important supporting technologies) for deploying tools. Feel free to get creative and invent your own solutions as well. Data Visualization & Dashboards Plot.ly Bokeh Cufflinks Folium (maps for dashboards) Dash Tableau Ipywidgets, ipyleaflet, \u2026 Microsoft PowerApps PowerBI Jupyter Notebook + Binder Google Colab Web Deployment Flask Django Streamlit Heroku GitHub Pages Linode Digital Ocean Google Apps (Firebase, AppSheet, App Engine) Desktop Deployment Tkinter Cron Tab PyOxidizer Excel + VBA Command Line Tools Databases & Back-ends SQLLite PostgreSQL MySQL Google Sheets AWS S3 Environments Docker Geographic Data & Mapping Pyproj (for projection conversion) Google Earth Engine Custom script tools using ESRI ArcGIS API (arcpy) ESRI ArcGIS Story Map ESRI ArcGIS Apps Comparing the options \u00b6 Dash vs. Bokeh \u00b6 Dash and Bokeh provide similar capabilities for developing dashboards. The first step will be selecting which package to learn. Dash is maintained by Plot.ly, whereas Bokeh is maintained by Anaconda. Based on a little bit of research , it appears that Dash is a better first choice for building Dashboards. It has slightly less functionality but is 100% python and well documented with rapid development of new features. It offers a free and paid version. Dash also offers a deployment server (paid) and uses Flask as a backend, which is another advantage in deployment. Dash recommends Heroku for deployment if not using the Dash server. Ipywidgets \u00b6 Plotly plays well with ipywidgets in Notebooks, which provides a nice place to do data exploration or play with dashboard designs. You can pass chart design elements as interactions to test different marker sizes, opacities, background colors, map projections, etc. without interacting with code. See Jon Mease\u2019s talk at PyData 2018 . Folium \u00b6 Folium is a python wrapper for Leaflet, a popular JavaScript web map package. It creates HTML maps which conveniently run in Notebooks natively (ipyleaflet is an alternative if using widget integration in a map, but Folium has good interactions without the need for widgets). When combined with Dash, it allows for maps integrated into dashboards. Heroku \u00b6 Heroku is a hosting service for python web apps. It is a fairly easy to use service with a free option and paid options for higher traffic. We\u2019ll use Heroku for our web apps for now. Flask also provides a list of web deployment options . OpenShift allows one free app. The Magic Mix \u00b6 Heroku, Postgres, Flask, Dash, Plot.ly, Git The above technologies are integrated in the Heroku platform for web app deployment. Mastering these technologies will provide an important platform for quickly building web apps in the future.","title":"Deployment Overview"},{"location":"deployment/deployment-overview/#getting-tools-to-users","text":"This page is under development Tools are only useful if they are used. Getting tools in the hands of your users, while ensuring a good user experience with tech support and the development of new features, is at least half the battle. A number of options exist for deploying tools. The deployment environment should be decided early on in tool development. Here's a short list of options (and important supporting technologies) for deploying tools. Feel free to get creative and invent your own solutions as well. Data Visualization & Dashboards Plot.ly Bokeh Cufflinks Folium (maps for dashboards) Dash Tableau Ipywidgets, ipyleaflet, \u2026 Microsoft PowerApps PowerBI Jupyter Notebook + Binder Google Colab Web Deployment Flask Django Streamlit Heroku GitHub Pages Linode Digital Ocean Google Apps (Firebase, AppSheet, App Engine) Desktop Deployment Tkinter Cron Tab PyOxidizer Excel + VBA Command Line Tools Databases & Back-ends SQLLite PostgreSQL MySQL Google Sheets AWS S3 Environments Docker Geographic Data & Mapping Pyproj (for projection conversion) Google Earth Engine Custom script tools using ESRI ArcGIS API (arcpy) ESRI ArcGIS Story Map ESRI ArcGIS Apps","title":"Getting Tools to Users"},{"location":"deployment/deployment-overview/#comparing-the-options","text":"","title":"Comparing the options"},{"location":"deployment/deployment-overview/#dash-vs-bokeh","text":"Dash and Bokeh provide similar capabilities for developing dashboards. The first step will be selecting which package to learn. Dash is maintained by Plot.ly, whereas Bokeh is maintained by Anaconda. Based on a little bit of research , it appears that Dash is a better first choice for building Dashboards. It has slightly less functionality but is 100% python and well documented with rapid development of new features. It offers a free and paid version. Dash also offers a deployment server (paid) and uses Flask as a backend, which is another advantage in deployment. Dash recommends Heroku for deployment if not using the Dash server.","title":"Dash vs. Bokeh"},{"location":"deployment/deployment-overview/#ipywidgets","text":"Plotly plays well with ipywidgets in Notebooks, which provides a nice place to do data exploration or play with dashboard designs. You can pass chart design elements as interactions to test different marker sizes, opacities, background colors, map projections, etc. without interacting with code. See Jon Mease\u2019s talk at PyData 2018 .","title":"Ipywidgets"},{"location":"deployment/deployment-overview/#folium","text":"Folium is a python wrapper for Leaflet, a popular JavaScript web map package. It creates HTML maps which conveniently run in Notebooks natively (ipyleaflet is an alternative if using widget integration in a map, but Folium has good interactions without the need for widgets). When combined with Dash, it allows for maps integrated into dashboards.","title":"Folium"},{"location":"deployment/deployment-overview/#heroku","text":"Heroku is a hosting service for python web apps. It is a fairly easy to use service with a free option and paid options for higher traffic. We\u2019ll use Heroku for our web apps for now. Flask also provides a list of web deployment options . OpenShift allows one free app.","title":"Heroku"},{"location":"deployment/deployment-overview/#the-magic-mix","text":"Heroku, Postgres, Flask, Dash, Plot.ly, Git The above technologies are integrated in the Heroku platform for web app deployment. Mastering these technologies will provide an important platform for quickly building web apps in the future.","title":"The Magic Mix"},{"location":"deployment/google-api/","text":"Google API \u00b6 Google provides an API for interacting with Google Drive and its suite of office products (Sheets, Docs, Slides, etc.). An API is an 'application programming interface'. Simply put, it's the interpreter between your program and Google's products. If you want to write into a Google Doc, you can use the API to do it. Google Sheets is actually a quite powerful backend data store for web-based applications. If you're deploying to Heroku and don't have the need to configure a Postgres database, Google Sheets can get the job done quite well. That said, it's not free of issues, so beware before building anything mission critical. You can create up to 12 projects on Google Drive using the API for free. Setting up access \u00b6 This tutorial describes how to set up a project through Google's API Console and get the credentials you'll need to access it. Basically Go to the Google API Console Create a new project Click Enable API. Search for and enable the Google Drive API. Search for and enable the Google Sheets API. Create credentials for a Web Server to access Application Data. Name the service account and grant it a Project Role of Editor Download the JSON file Copy the JSON file into your project directory under a subdirectory called secrets/ In the spreadsheet on Google Drive, click the Share button and paste the client email from the JSON file into the People field to give it edit rights. Hit Send. Credit: Greg Baugues, twilio.com Interacting with the spreadsheet \u00b6 Now you can access the spreadsheet from your script. You'll need to install pygsheets and pandas ; conda install the required packages. Follow the instructions above to save your credentials into a secrets/ directory within the project. Learn more about how to handle credentials with Heroku here (step 23). Authorizing access \u00b6 import pygsheets def auth_gspread (): \"\"\"Authorize Google to access the Utilization Project :returns client \"\"\" # creds for local development try : client = pygsheets . authorize ( service_file = 'secrets/gs_credentials.json' ) # creds for heroku deployment except : client = pygsheets . authorize ( service_account_env_var = 'GOOGLE_SHEETS_CREDS_JSON' ) return client Refer to the pygsheets docs for help reading and writing to spreadsheets; or this helpful guide from Towards Data Science. Reading data to a data frame \u00b6 Use this code to read a spreadsheet as a pandas data frame use the function above to get the client variable). Note that a spreadsheet ( sh ) is synonymous with an Excel workbook, and a worksheet ( wks ) is still a worksheet. import pandas as pd def load_data ( client , spreadsheet , sheet_title , tidy = False ): \"\"\"Load data (must be in tidy format) from sheet. Empty rows are dropped. :param client: client object for accessing google :param spreadsheet: name of the spreadsheet :param sheet_title: worksheet name :param tidy: if data is well-formatted, set to True :returns pandas dataframe \"\"\" # load data from google sheet sh = client . open ( spreadsheet ) wks = sh . worksheet_by_title ( sheet_title ) if tidy : df = wks . get_as_df ( empty_values = None ) else : data = wks . get_all_records ( empty_value = None ) # get_as_df can't handle empty columns df = pd . DataFrame ( data ) df . dropna ( axis = 0 , how = 'all' , inplace = True ) return df Saving a data frame \u00b6 To save a csv in an existing worksheet (note this will overwrite any data in the worksheet): def save_data ( client , spreadsheet , sheet_title , df ) \"\"\"Save data to a worksheet, overwriting existing data :param client: client object for accessing google :param spreadsheet: name of the spreadsheet :param sheet_title: worksheet name :param df: dataframe to save :returns none \"\"\" sh = client . open ( spreadsheet ) wks = sh . worksheet_by_title ( sheet_title ) wks . set_dataframe ( df , 'A1' , fit = True )","title":"Google API"},{"location":"deployment/google-api/#google-api","text":"Google provides an API for interacting with Google Drive and its suite of office products (Sheets, Docs, Slides, etc.). An API is an 'application programming interface'. Simply put, it's the interpreter between your program and Google's products. If you want to write into a Google Doc, you can use the API to do it. Google Sheets is actually a quite powerful backend data store for web-based applications. If you're deploying to Heroku and don't have the need to configure a Postgres database, Google Sheets can get the job done quite well. That said, it's not free of issues, so beware before building anything mission critical. You can create up to 12 projects on Google Drive using the API for free.","title":"Google API"},{"location":"deployment/google-api/#setting-up-access","text":"This tutorial describes how to set up a project through Google's API Console and get the credentials you'll need to access it. Basically Go to the Google API Console Create a new project Click Enable API. Search for and enable the Google Drive API. Search for and enable the Google Sheets API. Create credentials for a Web Server to access Application Data. Name the service account and grant it a Project Role of Editor Download the JSON file Copy the JSON file into your project directory under a subdirectory called secrets/ In the spreadsheet on Google Drive, click the Share button and paste the client email from the JSON file into the People field to give it edit rights. Hit Send. Credit: Greg Baugues, twilio.com","title":"Setting up access"},{"location":"deployment/google-api/#interacting-with-the-spreadsheet","text":"Now you can access the spreadsheet from your script. You'll need to install pygsheets and pandas ; conda install the required packages. Follow the instructions above to save your credentials into a secrets/ directory within the project. Learn more about how to handle credentials with Heroku here (step 23).","title":"Interacting with the spreadsheet"},{"location":"deployment/google-api/#authorizing-access","text":"import pygsheets def auth_gspread (): \"\"\"Authorize Google to access the Utilization Project :returns client \"\"\" # creds for local development try : client = pygsheets . authorize ( service_file = 'secrets/gs_credentials.json' ) # creds for heroku deployment except : client = pygsheets . authorize ( service_account_env_var = 'GOOGLE_SHEETS_CREDS_JSON' ) return client Refer to the pygsheets docs for help reading and writing to spreadsheets; or this helpful guide from Towards Data Science.","title":"Authorizing access"},{"location":"deployment/google-api/#reading-data-to-a-data-frame","text":"Use this code to read a spreadsheet as a pandas data frame use the function above to get the client variable). Note that a spreadsheet ( sh ) is synonymous with an Excel workbook, and a worksheet ( wks ) is still a worksheet. import pandas as pd def load_data ( client , spreadsheet , sheet_title , tidy = False ): \"\"\"Load data (must be in tidy format) from sheet. Empty rows are dropped. :param client: client object for accessing google :param spreadsheet: name of the spreadsheet :param sheet_title: worksheet name :param tidy: if data is well-formatted, set to True :returns pandas dataframe \"\"\" # load data from google sheet sh = client . open ( spreadsheet ) wks = sh . worksheet_by_title ( sheet_title ) if tidy : df = wks . get_as_df ( empty_values = None ) else : data = wks . get_all_records ( empty_value = None ) # get_as_df can't handle empty columns df = pd . DataFrame ( data ) df . dropna ( axis = 0 , how = 'all' , inplace = True ) return df","title":"Reading data to a data frame"},{"location":"deployment/google-api/#saving-a-data-frame","text":"To save a csv in an existing worksheet (note this will overwrite any data in the worksheet): def save_data ( client , spreadsheet , sheet_title , df ) \"\"\"Save data to a worksheet, overwriting existing data :param client: client object for accessing google :param spreadsheet: name of the spreadsheet :param sheet_title: worksheet name :param df: dataframe to save :returns none \"\"\" sh = client . open ( spreadsheet ) wks = sh . worksheet_by_title ( sheet_title ) wks . set_dataframe ( df , 'A1' , fit = True )","title":"Saving a data frame"},{"location":"deployment/gs-app/","text":"Google Sheets Application \u00b6 Imagine you want to leverage the familiarity and ease-of-use of Google Sheets for your user interface but you also want to develop a user-friendly application-like experience for users. Google Sheets can serve as a surprisingly powerful platform for developing your solution with a few simple tricks. This tutorial will introduce you to the concepts required to extend Google Sheets to an efficient full-stack solution for your application. I am not going to use Macros, AppSheet, or Add-ons, although you may find those useful. This tutorial instead uses out-of-the-box functionality of Google Sheets. I will supply one formula for the Script editor, but it is not important to understand how it works. The benefits of using Google Sheets is that you can much more quickly build a fairly robust solution than on another platform. It is also likely familiar to user, which allows you to bring them into the development and testing process. Finally, it has a ton of out-of-the box functionality that you can leverage for your project. Google Sheets can be right for prototyping solutions and often can be the first and last stop for developing a lightweight application. Why not Excel? Excel is behind Google Sheets in two key areas: array formulas and data sharing. Array formulas allow for data aggregation and import without knowing the structure of the data. Ever copy data from one sheet to another by just dragging a =Sheet1!A1 formula down a column? You need an array formula. Data sharing between Excel sheets can be accomplished through external links or Power Query, however I find those break more than they work once a file is shared and Power Query is comparatively slow. Spreadsheet Maturity Model \u00b6 Before beginning, I want to share a model of maturity for spreadsheets I first heard in this GSuite presentation . This is a very useful model for thinking about how to advance your Google Sheet project from concept to application. Infancy - \"Capture the Idea\" - just trying to figure out what pieces of data and organization of data makes the most sense. Toddler - \"Validate the Idea\" - share the spreadsheet with others to collaborate, improve, and capture more use cases. Grade School - \"Formalize the idea\" - apply formatting, validation, and freeze rows. Generally make it easier to use and supply affordances (hints to users). Middle School - \"Build the permissions\" - protect ranges, set up permissions, and use private sheets (sharing data through IMPORTRANGE ). Consider access and privileges. High School - \"Build the process\" - build nuance to the data collection process to reflect users workflows. Set up forms instead of editing the sheet directly. College Student - \"Build the logic\" - automate some processes and improve communications throughout the workflow. (GSuite recommends AppSheet, however the pricing is ...confusing ). If you are using spreadsheets as your app platform, you'll want to have a \"College Student\" level of maturity before going to production. Separate the data entry functionality from the data storage and data analysis/visualization. This will greatly improve the user experience of your spreadsheet application. Data Design \u00b6 Data Entry Data Storage Data Visualization \"Tidy Endpoints\" \u00b6 Tidy Endpoints is a conceptual extension of RESTful endpoints, however using tidy data instead of JSON. Tidy data is a concept popularized by Hadley Wickham and an incredibly simple yet powerful concept for data management. Tidy data can also be described as 'tall' rather than 'wide'. Tidy data correlates to Codd's Third Normal Form, which can be further refined to 4th normal form for efficient data storage (as used in database applications). I've found spreadsheets are more amenable to tidy data than 4th normal form data, so we'll use the tidy data concept for our architecture. The power of the Tidy Endpoints concept is that it allows us to format our data in such a way that it can be efficiently and safely shared with other spreadsheets without worrying about the structure of those sheets or the total amount of data shared. (However, note that Google Sheets has limits on how much data can be shared.) Tidy data meets the following conditions: Each variable forms a column Each observation forms a row Each level of observation forms a table Array Formulas \u00b6 To unlock the power of Google Sheets, you must understand array formulas . Array formulas return an array, rather than a single value. In many ways, the lack of array formulas was the primary constraint of spreadsheets as compared to higher level programming interfaces. Without array formulas, you are acting as a low-level memory manager for the application, allocating a single piece of information to a single cell. With array formulas, you have the power of many modern programing languages as the memory management function is abstracted away. The simplest way to return an array in Google Sheets is to use curly braces {} . For example, this would return all of the data in columns A and C in side-by-side columns: ={Sheet1!A:A, Sheet1C:C} To return data stacked vertically, rather than side-by-side, use a semi-colon: ={Sheet1A1:A10, Sheet1!A20:A30} Alternatively, you can use ARRAYFORMULA(Sheet1A1:A10, Sheet1!A20:A30) for the same purpose. Array Formula examples \u00b6 Let's try out some array formulas. Note that you won't need to worry about fixed references ( $ ) because you won't be dragging the formula around. Multiply a column by a scalar \u00b6 =arrayformula(A2:A7*10) Multiply an array by a scalar \u00b6 =arrayformula(A2:F7*10) Product table \u00b6 =arrayformula(A2:A7*B1:F1) Supporting Formulas \u00b6 We'll use the following formulas in combination with array formulas to aggregate, format, and share data. Filter \u00b6 FILTER is my go-to formula for concatenating. It improves upon the basic array formula by allowing you to filter what is returned in the array. Most often, I filter null or zero values, for example: =FILTER(Sheet1!A:Z, Sheet1!A:A > 0) will return the array in Sheet1!A:Z where column A is greater than zero. To pull data from multiple sheets, we can use: = { FILTER(Sheet1! A : Z , Sheet1 ! A : A > 0 ); FILTER(Sheet2! A2 : Z , Sheet2 !: A2 : A > 0 )} Note that sheets 2 and beyond should only pull from row 2 on to avoid duplicating headers in your array. Sheet 1 and Sheet 2 should have the same headers. Filtering the results of an array formula \u00b6 Sometimes you want to filter the results of an array formula. You can use INDEX to get the right column to filter for ={FILTER(ARRAYFORMULA(Sheet1!A:Z), INDEX(ARRAYFORMULA(Sheet1!A:Z),0,3)<>\"\")} will filter the third column of the returned array that are blank. This is useful when the array formula is manipulating the structure of the data, and you do not have access to that column (common with UNPIVOT , see below). However, note that this is now calling the array formula twice! This can be very expensive. Use a helper table instead. If you're importing using IMPORTRANGE , use a QUERY . Filter based on a list \u00b6 =FILTER(A:C, COUNTIF(E:E, A:A)) where E is the list to check and A is the column to check against E . Query \u00b6 QUERY is a SQL -like syntax for querying arrays. If you are familiar with SQL you will want to explore this function. Why isn't it my go-to then? Primarily because it behaves unexpectedly when a column's data types are mixed. Imagine your company uses identifiers that contain a mix of letters and numbers, but some (most) are numeric. Query will simply ignore any data that are not of the dominant type in a column (and it won't pay attention to how you have formatted the column!). Thus, you may end up missing values (without warning) in your results. QUERY relies on a SELECT statement passed as a string to return your array. You will not have access to the column headers for the SELECT statement, so you must refer to columns by their column letter (i.e., A , B ). Alternatively (and if you are combining a QUERY with IMPORTRANGE ) you can wrap your range in curly braces and use 'Col1', 'Col2', ... to refer to columns by order. The reason this works is that the range provided is now no longer considered part of the spreadsheet, and so you don't have the column letter identifiers. Note that you cannot use 'Col1', 'Col2', ... without wrapping the range in curly braces. An example QUERY might be =QUERY(Sheet1!A:Z, \"SELECT A, sum(B) where A is not null group by A\") or =QUERY({Sheet1!A:Z}, SELECT Col1, sum(Col2) where Col1 is not null group by Col1) This would return the sum of all values in column B grouped by column A where A is not null (use <> '' if the column contains text rather than numerical data). Using column headers in SELECT statements \u00b6 If you need to substitute a column header for the column letter, you can use this slightly complicated formula SUBSTITUTE(ADDRESS(1, MATCH(\"<Column Name>\",Sheet1!A$1:$Z$1,0),4),1,\"\") in the SELECT statement using concatenation =QUERY(Sheet1!A:Z, \"SELECT\" & <paste SUBSTITUTE formula here>) Indirect \u00b6 Indirect is another way to return a cell address using a string as input, in essence it converts \"A1\" to A1 so that Google Sheets treats it like a cell address. Advanced Queries \u00b6 Group by \u00b6 Query can be used with SQL-like grammar for very powerful calculations like groupby. =query(eventsByStaff!A:O, \"select A, B, sum(D), sum(E), sum(F), sum(G), sum(H), sum(I), sum(J), sum(K), sum(L), sum(M), sum(N), sum(O) where A is not null group by A, B\", 1) Filtering Imported data \u00b6 =QUERY(IMPORTRANGE(\"url\", \"Sheet1!A:Z\") \"SELECT * WHERE Col1 = 'TRUE'\") Note the quotes around True, otherwise it will look for a column named TRUE (which it won't find). Unpivot \u00b6 Unpivot is a custom function written using the Script editor. Unpivoting, or 'melting' data is a common practice in data manipulation to get tidy data. If you have columns that are actually dimensions, you can unpivot the data to get a column with dimensions and a column with corresponding value. I've provided the code below (not my code, credit ). Simply open the Script editor, paste the contents of the code block below, hit save, and the unpivot function will be available to you in the sheet. /** * Unpivot a pivot table of any size. * * @param {A1:D30} data The pivot table. * @param {1} fixColumns Number of columns, after which pivoted values begin. Default 1. * @param {1} fixRows Number of rows (1 or 2), after which pivoted values begin. Default 1. * @param {\"city\"} titlePivot The title of horizontal pivot values. Default \"column\". * @param {\"distance\"[,...]} titleValue The title of pivot table values. Default \"value\". * @return The unpivoted table * @customfunction * https://stackoverflow.com/questions/24954722/how-do-you-create-a-reverse-pivot-in-google-sheets */ function unpivot ( data , fixColumns , fixRows , titlePivot , titleValue ) { var fixColumns = fixColumns || 1 ; // how many columns are fixed var fixRows = fixRows || 1 ; // how many rows are fixed var titlePivot = titlePivot || 'column' ; var titleValue = titleValue || 'value' ; var ret = [] , i , j , row , uniqueCols = 1 ; // we handle only 2 dimension arrays if ( ! Array . isArray ( data ) || data . length < fixRows || ! Array . isArray ( data [ 0 ] ) || data [ 0 ] . length < fixColumns ) throw new Error ( 'no data' ); // we handle max 2 fixed rows if ( fixRows > 2 ) throw new Error ( 'max 2 fixed rows are allowed' ); // fill empty cells in the first row with value set last in previous columns ( for 2 fixed rows ) var tmp = '' ; for ( j = 0 ; j < data [ 0 ] . length ; j ++ ) if ( data [ 0 ][ j ] != '' ) tmp = data [ 0 ][ j ] ; else data [ 0 ][ j ] = tmp ; // for 2 fixed rows calculate unique column number if ( fixRows == 2 ) { uniqueCols = 0 ; tmp = {} ; for ( j = fixColumns ; j < data [ 1 ] . length ; j ++ ) if ( typeof tmp [ data[1 ][ j ] ] == 'undefined' ) { tmp [ data[1 ][ j ] ] = 1 ; uniqueCols ++ ; } } // return first row : fix column titles + pivoted values column title + values column title ( s ) row = [] ; for ( j = 0 ; j < fixColumns ; j ++ ) row . push ( fixRows == 2 ? data [ 0 ][ j ]|| data [ 1 ][ j ] : data [ 0 ][ j ] ); // for 2 fixed rows we try to find the title in row 1 and row 2 for ( j = 3 ; j < arguments . length ; j ++ ) row . push ( arguments [ j ] ); ret . push ( row ); // processing rows ( skipping the fixed columns , then dedicating a new row for each pivoted value ) for ( i = fixRows ; i < data . length && data [ i ] . length > 0 ; i ++ ) { // skip totally empty or only whitespace containing rows if ( data [ i ] . join ( '' ). replace ( / \\ s +/ g , '' ). length == 0 ) continue ; // unpivot the row row = [] ; for ( j = 0 ; j < fixColumns && j < data [ i ] . length ; j ++ ) row . push ( data [ i ][ j ] ); for ( j = fixColumns ; j < data [ i ] . length ; j += uniqueCols ) ret . push ( row . concat ( [ data[0 ][ j ] ] ) // the first row title value . concat ( data [ i ] . slice ( j , j + uniqueCols )) // pivoted values ); } return ret ; } Simply specify the data source, number of fixed columns/rows, and optionally the name of the new dimension column and value column. =UNPIVOT(Sheet1!A:Z, 2, 1, \"my dim\", \"my val\") Here the first two columns will be treated as fixed and the first row will be treated as the header. Note that only two header rows are supported. If you need to add more structure to your sheet, consider adding hidden rows/columns around your variable data to pull them closer to the data you need to unpivot. Non-array Formulas of note \u00b6 Index Match \u00b6 Combining the INDEX and MATCH formula is a powerhouse formula for spreadsheets, allowing you to lookup a value from one column based on a value from another column. If you don't know it, check it out, there are lots of great resources online. You cannot use Index Match in an array formula however. Pivot Tables \u00b6 Unpivoted data is great for sharing and analysis, but not always optimal for data visualizing. Use Google Sheets built in Pivot tables to return data to its original format if needed. Joins \u00b6 If you're familiar with SQL, you'll likely sorely miss the ability to join data, especially when the join is many-to-many. Here's how you can do it in Google Sheets: =FILTER( {Sheet1!A:C, VLOOKUP(Sheet1!C:C, {Sheet2!A:A, Sheet2!B:E}, {2,3,4,5}, false)}, Sheet1!C:C<>\"\") First note that you will be combining two results side by side (Sheet1!A:C and the result of the VLOOKUP). That will be filtered where Sheet1 column C is not blank. The VLOOKUP table is in Sheet2, where column A is the join index for Sheet2 and column C is the join index for Sheet1. Columns B:E is Sheet2 will be joined to Sheet1 wherever the join indices match. The general form is below: =FILTER( {<Table1>, VLOOKUP(<Table1 Join Index>, {<Table2 Join Index>, <Table2 Columns to Join>}, {<2, 3 ... length of Table2 Columns to Join>}, false)}, <Filter condition>) Formula efficiency \u00b6 Use helper columns and helper tables to avoid re-calculating the same thing multiple times. This can speed up your spreadsheet immensely. Don't: ={FILTER(IMPORTRANGE(\"url\", \"SheetName!A:Z\"), INDEX(IMPORTRANGE(\"url\", \"SheetName!A:Z\"),0,1)<>'')} Do (in two separate sheets): =IMPORTRANGE(\"url\", \"SheetName!A:Z\") // in Sheet1 of the new spreadsheet =FILTER(Sheet1!A:Z, Sheet1!A:Z <> '') In the Don't example, you are importing the range twice, which is very expensive. In the second example, you import the range once into a helper table, and then filter the result, thus importing only once. Don't drag this formula down a column to calculate percent of time worked: =A1/NETWORKDAYS(A1, EDATE(A1, 1)-1) Do create a helper column in B with the result of NETWORKDAY and then in column C drag down =A1/B1 Testing spreadsheet efficiency \u00b6 In your spreadsheet, use Ctrl+Shift+I to open the developer tools. Under performance, record the spreadsheet loading (with data in it). Make a change to improve your formulas and reload the spreadsheet again. You can then see the performance improvement in ms. Note that this can be quite stochastic depending on your wifi at the exact moment, so run the test a few times for each configuration. If still sluggish, consider deleting unused rows, columns in your spreadsheet, limit conditional formatting, or see other ideas here . Evolution of a formula \u00b6 Like spreadsheets, formulas should be built iteratively. First, just make sure the formula does what you want it to do. Then start formalizing it such that it allows for user customization and is robust to changes in the spreadsheet. [Example] Write basic Extract fixed values Use named ranges Substitute variables to allow extension (with caution) Basic Set Up \u00b6 Although not necessary, it can be helpful to create a new folder on Google Drive to store all of your spreadsheets. We'll set up four spreadsheets, one for input values, two for data entry, and one for aggregation and data visualization. Your folder should look like this: [Insert drive image] Now let's add some data. We'll support two different data entry formats to better illustrate the power of Google Sheets. Aggregating Data \u00b6 You can support multiple 'rooms' as different sheets in the spreadsheet to make data entry easier for users. You must ensure that the rooms are identical (for the data you want to aggregate). = { Sheet1! A : Z ; Sheet2! A2 : Z > } Note that the second and all subsequent ranges should exclude the headers. If you want to change the headers, you can use the first row to specify new header names and then in row 2 set the import to start at row 2 of Sheet1. Tidying data \u00b6 Tidy data using the UNPIVOT custom function defined above and then filtering null values. =UNPIVOT(Sheet3!A:E, 1, 1, \"<parameter>\", \"<value>\") // in Sheet4!A1 =FILTER(Sheet4!A:E, Sheet4!A:A<>\"\") // in Sheet 5!A1 Your data is now all tidied up and ready for export! Sharing data \u00b6 The magic of Google Sheets is being able to treat multiple spreadsheets as a single system. IMPORTRANGE allows you to pull data together using the URL of each spreadsheet. Once access is granted, any editor on the destination spreadsheet can use IMPORTRANGE to pull from any part of the source spreadsheet. The access remains in effect until the user who granted access is removed from the source. If the data you are trying to import is too large, you may get an error. Managing connections \u00b6 connectionLog Working in Shared Sheets \u00b6 Filter Views Slicers","title":"Google Sheets App"},{"location":"deployment/gs-app/#google-sheets-application","text":"Imagine you want to leverage the familiarity and ease-of-use of Google Sheets for your user interface but you also want to develop a user-friendly application-like experience for users. Google Sheets can serve as a surprisingly powerful platform for developing your solution with a few simple tricks. This tutorial will introduce you to the concepts required to extend Google Sheets to an efficient full-stack solution for your application. I am not going to use Macros, AppSheet, or Add-ons, although you may find those useful. This tutorial instead uses out-of-the-box functionality of Google Sheets. I will supply one formula for the Script editor, but it is not important to understand how it works. The benefits of using Google Sheets is that you can much more quickly build a fairly robust solution than on another platform. It is also likely familiar to user, which allows you to bring them into the development and testing process. Finally, it has a ton of out-of-the box functionality that you can leverage for your project. Google Sheets can be right for prototyping solutions and often can be the first and last stop for developing a lightweight application. Why not Excel? Excel is behind Google Sheets in two key areas: array formulas and data sharing. Array formulas allow for data aggregation and import without knowing the structure of the data. Ever copy data from one sheet to another by just dragging a =Sheet1!A1 formula down a column? You need an array formula. Data sharing between Excel sheets can be accomplished through external links or Power Query, however I find those break more than they work once a file is shared and Power Query is comparatively slow.","title":"Google Sheets Application"},{"location":"deployment/gs-app/#spreadsheet-maturity-model","text":"Before beginning, I want to share a model of maturity for spreadsheets I first heard in this GSuite presentation . This is a very useful model for thinking about how to advance your Google Sheet project from concept to application. Infancy - \"Capture the Idea\" - just trying to figure out what pieces of data and organization of data makes the most sense. Toddler - \"Validate the Idea\" - share the spreadsheet with others to collaborate, improve, and capture more use cases. Grade School - \"Formalize the idea\" - apply formatting, validation, and freeze rows. Generally make it easier to use and supply affordances (hints to users). Middle School - \"Build the permissions\" - protect ranges, set up permissions, and use private sheets (sharing data through IMPORTRANGE ). Consider access and privileges. High School - \"Build the process\" - build nuance to the data collection process to reflect users workflows. Set up forms instead of editing the sheet directly. College Student - \"Build the logic\" - automate some processes and improve communications throughout the workflow. (GSuite recommends AppSheet, however the pricing is ...confusing ). If you are using spreadsheets as your app platform, you'll want to have a \"College Student\" level of maturity before going to production. Separate the data entry functionality from the data storage and data analysis/visualization. This will greatly improve the user experience of your spreadsheet application.","title":"Spreadsheet Maturity Model"},{"location":"deployment/gs-app/#data-design","text":"Data Entry Data Storage Data Visualization","title":"Data Design"},{"location":"deployment/gs-app/#tidy-endpoints","text":"Tidy Endpoints is a conceptual extension of RESTful endpoints, however using tidy data instead of JSON. Tidy data is a concept popularized by Hadley Wickham and an incredibly simple yet powerful concept for data management. Tidy data can also be described as 'tall' rather than 'wide'. Tidy data correlates to Codd's Third Normal Form, which can be further refined to 4th normal form for efficient data storage (as used in database applications). I've found spreadsheets are more amenable to tidy data than 4th normal form data, so we'll use the tidy data concept for our architecture. The power of the Tidy Endpoints concept is that it allows us to format our data in such a way that it can be efficiently and safely shared with other spreadsheets without worrying about the structure of those sheets or the total amount of data shared. (However, note that Google Sheets has limits on how much data can be shared.) Tidy data meets the following conditions: Each variable forms a column Each observation forms a row Each level of observation forms a table","title":"\"Tidy Endpoints\""},{"location":"deployment/gs-app/#array-formulas","text":"To unlock the power of Google Sheets, you must understand array formulas . Array formulas return an array, rather than a single value. In many ways, the lack of array formulas was the primary constraint of spreadsheets as compared to higher level programming interfaces. Without array formulas, you are acting as a low-level memory manager for the application, allocating a single piece of information to a single cell. With array formulas, you have the power of many modern programing languages as the memory management function is abstracted away. The simplest way to return an array in Google Sheets is to use curly braces {} . For example, this would return all of the data in columns A and C in side-by-side columns: ={Sheet1!A:A, Sheet1C:C} To return data stacked vertically, rather than side-by-side, use a semi-colon: ={Sheet1A1:A10, Sheet1!A20:A30} Alternatively, you can use ARRAYFORMULA(Sheet1A1:A10, Sheet1!A20:A30) for the same purpose.","title":"Array Formulas"},{"location":"deployment/gs-app/#array-formula-examples","text":"Let's try out some array formulas. Note that you won't need to worry about fixed references ( $ ) because you won't be dragging the formula around.","title":"Array Formula examples"},{"location":"deployment/gs-app/#multiply-a-column-by-a-scalar","text":"=arrayformula(A2:A7*10)","title":"Multiply a column by a scalar"},{"location":"deployment/gs-app/#multiply-an-array-by-a-scalar","text":"=arrayformula(A2:F7*10)","title":"Multiply an array by a scalar"},{"location":"deployment/gs-app/#product-table","text":"=arrayformula(A2:A7*B1:F1)","title":"Product table"},{"location":"deployment/gs-app/#supporting-formulas","text":"We'll use the following formulas in combination with array formulas to aggregate, format, and share data.","title":"Supporting Formulas"},{"location":"deployment/gs-app/#filter","text":"FILTER is my go-to formula for concatenating. It improves upon the basic array formula by allowing you to filter what is returned in the array. Most often, I filter null or zero values, for example: =FILTER(Sheet1!A:Z, Sheet1!A:A > 0) will return the array in Sheet1!A:Z where column A is greater than zero. To pull data from multiple sheets, we can use: = { FILTER(Sheet1! A : Z , Sheet1 ! A : A > 0 ); FILTER(Sheet2! A2 : Z , Sheet2 !: A2 : A > 0 )} Note that sheets 2 and beyond should only pull from row 2 on to avoid duplicating headers in your array. Sheet 1 and Sheet 2 should have the same headers.","title":"Filter"},{"location":"deployment/gs-app/#filtering-the-results-of-an-array-formula","text":"Sometimes you want to filter the results of an array formula. You can use INDEX to get the right column to filter for ={FILTER(ARRAYFORMULA(Sheet1!A:Z), INDEX(ARRAYFORMULA(Sheet1!A:Z),0,3)<>\"\")} will filter the third column of the returned array that are blank. This is useful when the array formula is manipulating the structure of the data, and you do not have access to that column (common with UNPIVOT , see below). However, note that this is now calling the array formula twice! This can be very expensive. Use a helper table instead. If you're importing using IMPORTRANGE , use a QUERY .","title":"Filtering the results of an array formula"},{"location":"deployment/gs-app/#filter-based-on-a-list","text":"=FILTER(A:C, COUNTIF(E:E, A:A)) where E is the list to check and A is the column to check against E .","title":"Filter based on a list"},{"location":"deployment/gs-app/#query","text":"QUERY is a SQL -like syntax for querying arrays. If you are familiar with SQL you will want to explore this function. Why isn't it my go-to then? Primarily because it behaves unexpectedly when a column's data types are mixed. Imagine your company uses identifiers that contain a mix of letters and numbers, but some (most) are numeric. Query will simply ignore any data that are not of the dominant type in a column (and it won't pay attention to how you have formatted the column!). Thus, you may end up missing values (without warning) in your results. QUERY relies on a SELECT statement passed as a string to return your array. You will not have access to the column headers for the SELECT statement, so you must refer to columns by their column letter (i.e., A , B ). Alternatively (and if you are combining a QUERY with IMPORTRANGE ) you can wrap your range in curly braces and use 'Col1', 'Col2', ... to refer to columns by order. The reason this works is that the range provided is now no longer considered part of the spreadsheet, and so you don't have the column letter identifiers. Note that you cannot use 'Col1', 'Col2', ... without wrapping the range in curly braces. An example QUERY might be =QUERY(Sheet1!A:Z, \"SELECT A, sum(B) where A is not null group by A\") or =QUERY({Sheet1!A:Z}, SELECT Col1, sum(Col2) where Col1 is not null group by Col1) This would return the sum of all values in column B grouped by column A where A is not null (use <> '' if the column contains text rather than numerical data).","title":"Query"},{"location":"deployment/gs-app/#using-column-headers-in-select-statements","text":"If you need to substitute a column header for the column letter, you can use this slightly complicated formula SUBSTITUTE(ADDRESS(1, MATCH(\"<Column Name>\",Sheet1!A$1:$Z$1,0),4),1,\"\") in the SELECT statement using concatenation =QUERY(Sheet1!A:Z, \"SELECT\" & <paste SUBSTITUTE formula here>)","title":"Using column headers in SELECT statements"},{"location":"deployment/gs-app/#indirect","text":"Indirect is another way to return a cell address using a string as input, in essence it converts \"A1\" to A1 so that Google Sheets treats it like a cell address.","title":"Indirect"},{"location":"deployment/gs-app/#advanced-queries","text":"","title":"Advanced Queries"},{"location":"deployment/gs-app/#group-by","text":"Query can be used with SQL-like grammar for very powerful calculations like groupby. =query(eventsByStaff!A:O, \"select A, B, sum(D), sum(E), sum(F), sum(G), sum(H), sum(I), sum(J), sum(K), sum(L), sum(M), sum(N), sum(O) where A is not null group by A, B\", 1)","title":"Group by"},{"location":"deployment/gs-app/#filtering-imported-data","text":"=QUERY(IMPORTRANGE(\"url\", \"Sheet1!A:Z\") \"SELECT * WHERE Col1 = 'TRUE'\") Note the quotes around True, otherwise it will look for a column named TRUE (which it won't find).","title":"Filtering Imported data"},{"location":"deployment/gs-app/#unpivot","text":"Unpivot is a custom function written using the Script editor. Unpivoting, or 'melting' data is a common practice in data manipulation to get tidy data. If you have columns that are actually dimensions, you can unpivot the data to get a column with dimensions and a column with corresponding value. I've provided the code below (not my code, credit ). Simply open the Script editor, paste the contents of the code block below, hit save, and the unpivot function will be available to you in the sheet. /** * Unpivot a pivot table of any size. * * @param {A1:D30} data The pivot table. * @param {1} fixColumns Number of columns, after which pivoted values begin. Default 1. * @param {1} fixRows Number of rows (1 or 2), after which pivoted values begin. Default 1. * @param {\"city\"} titlePivot The title of horizontal pivot values. Default \"column\". * @param {\"distance\"[,...]} titleValue The title of pivot table values. Default \"value\". * @return The unpivoted table * @customfunction * https://stackoverflow.com/questions/24954722/how-do-you-create-a-reverse-pivot-in-google-sheets */ function unpivot ( data , fixColumns , fixRows , titlePivot , titleValue ) { var fixColumns = fixColumns || 1 ; // how many columns are fixed var fixRows = fixRows || 1 ; // how many rows are fixed var titlePivot = titlePivot || 'column' ; var titleValue = titleValue || 'value' ; var ret = [] , i , j , row , uniqueCols = 1 ; // we handle only 2 dimension arrays if ( ! Array . isArray ( data ) || data . length < fixRows || ! Array . isArray ( data [ 0 ] ) || data [ 0 ] . length < fixColumns ) throw new Error ( 'no data' ); // we handle max 2 fixed rows if ( fixRows > 2 ) throw new Error ( 'max 2 fixed rows are allowed' ); // fill empty cells in the first row with value set last in previous columns ( for 2 fixed rows ) var tmp = '' ; for ( j = 0 ; j < data [ 0 ] . length ; j ++ ) if ( data [ 0 ][ j ] != '' ) tmp = data [ 0 ][ j ] ; else data [ 0 ][ j ] = tmp ; // for 2 fixed rows calculate unique column number if ( fixRows == 2 ) { uniqueCols = 0 ; tmp = {} ; for ( j = fixColumns ; j < data [ 1 ] . length ; j ++ ) if ( typeof tmp [ data[1 ][ j ] ] == 'undefined' ) { tmp [ data[1 ][ j ] ] = 1 ; uniqueCols ++ ; } } // return first row : fix column titles + pivoted values column title + values column title ( s ) row = [] ; for ( j = 0 ; j < fixColumns ; j ++ ) row . push ( fixRows == 2 ? data [ 0 ][ j ]|| data [ 1 ][ j ] : data [ 0 ][ j ] ); // for 2 fixed rows we try to find the title in row 1 and row 2 for ( j = 3 ; j < arguments . length ; j ++ ) row . push ( arguments [ j ] ); ret . push ( row ); // processing rows ( skipping the fixed columns , then dedicating a new row for each pivoted value ) for ( i = fixRows ; i < data . length && data [ i ] . length > 0 ; i ++ ) { // skip totally empty or only whitespace containing rows if ( data [ i ] . join ( '' ). replace ( / \\ s +/ g , '' ). length == 0 ) continue ; // unpivot the row row = [] ; for ( j = 0 ; j < fixColumns && j < data [ i ] . length ; j ++ ) row . push ( data [ i ][ j ] ); for ( j = fixColumns ; j < data [ i ] . length ; j += uniqueCols ) ret . push ( row . concat ( [ data[0 ][ j ] ] ) // the first row title value . concat ( data [ i ] . slice ( j , j + uniqueCols )) // pivoted values ); } return ret ; } Simply specify the data source, number of fixed columns/rows, and optionally the name of the new dimension column and value column. =UNPIVOT(Sheet1!A:Z, 2, 1, \"my dim\", \"my val\") Here the first two columns will be treated as fixed and the first row will be treated as the header. Note that only two header rows are supported. If you need to add more structure to your sheet, consider adding hidden rows/columns around your variable data to pull them closer to the data you need to unpivot.","title":"Unpivot"},{"location":"deployment/gs-app/#non-array-formulas-of-note","text":"","title":"Non-array Formulas of note"},{"location":"deployment/gs-app/#index-match","text":"Combining the INDEX and MATCH formula is a powerhouse formula for spreadsheets, allowing you to lookup a value from one column based on a value from another column. If you don't know it, check it out, there are lots of great resources online. You cannot use Index Match in an array formula however.","title":"Index Match"},{"location":"deployment/gs-app/#pivot-tables","text":"Unpivoted data is great for sharing and analysis, but not always optimal for data visualizing. Use Google Sheets built in Pivot tables to return data to its original format if needed.","title":"Pivot Tables"},{"location":"deployment/gs-app/#joins","text":"If you're familiar with SQL, you'll likely sorely miss the ability to join data, especially when the join is many-to-many. Here's how you can do it in Google Sheets: =FILTER( {Sheet1!A:C, VLOOKUP(Sheet1!C:C, {Sheet2!A:A, Sheet2!B:E}, {2,3,4,5}, false)}, Sheet1!C:C<>\"\") First note that you will be combining two results side by side (Sheet1!A:C and the result of the VLOOKUP). That will be filtered where Sheet1 column C is not blank. The VLOOKUP table is in Sheet2, where column A is the join index for Sheet2 and column C is the join index for Sheet1. Columns B:E is Sheet2 will be joined to Sheet1 wherever the join indices match. The general form is below: =FILTER( {<Table1>, VLOOKUP(<Table1 Join Index>, {<Table2 Join Index>, <Table2 Columns to Join>}, {<2, 3 ... length of Table2 Columns to Join>}, false)}, <Filter condition>)","title":"Joins"},{"location":"deployment/gs-app/#formula-efficiency","text":"Use helper columns and helper tables to avoid re-calculating the same thing multiple times. This can speed up your spreadsheet immensely. Don't: ={FILTER(IMPORTRANGE(\"url\", \"SheetName!A:Z\"), INDEX(IMPORTRANGE(\"url\", \"SheetName!A:Z\"),0,1)<>'')} Do (in two separate sheets): =IMPORTRANGE(\"url\", \"SheetName!A:Z\") // in Sheet1 of the new spreadsheet =FILTER(Sheet1!A:Z, Sheet1!A:Z <> '') In the Don't example, you are importing the range twice, which is very expensive. In the second example, you import the range once into a helper table, and then filter the result, thus importing only once. Don't drag this formula down a column to calculate percent of time worked: =A1/NETWORKDAYS(A1, EDATE(A1, 1)-1) Do create a helper column in B with the result of NETWORKDAY and then in column C drag down =A1/B1","title":"Formula efficiency"},{"location":"deployment/gs-app/#testing-spreadsheet-efficiency","text":"In your spreadsheet, use Ctrl+Shift+I to open the developer tools. Under performance, record the spreadsheet loading (with data in it). Make a change to improve your formulas and reload the spreadsheet again. You can then see the performance improvement in ms. Note that this can be quite stochastic depending on your wifi at the exact moment, so run the test a few times for each configuration. If still sluggish, consider deleting unused rows, columns in your spreadsheet, limit conditional formatting, or see other ideas here .","title":"Testing spreadsheet efficiency"},{"location":"deployment/gs-app/#evolution-of-a-formula","text":"Like spreadsheets, formulas should be built iteratively. First, just make sure the formula does what you want it to do. Then start formalizing it such that it allows for user customization and is robust to changes in the spreadsheet. [Example] Write basic Extract fixed values Use named ranges Substitute variables to allow extension (with caution)","title":"Evolution of a formula"},{"location":"deployment/gs-app/#basic-set-up","text":"Although not necessary, it can be helpful to create a new folder on Google Drive to store all of your spreadsheets. We'll set up four spreadsheets, one for input values, two for data entry, and one for aggregation and data visualization. Your folder should look like this: [Insert drive image] Now let's add some data. We'll support two different data entry formats to better illustrate the power of Google Sheets.","title":"Basic Set Up"},{"location":"deployment/gs-app/#aggregating-data","text":"You can support multiple 'rooms' as different sheets in the spreadsheet to make data entry easier for users. You must ensure that the rooms are identical (for the data you want to aggregate). = { Sheet1! A : Z ; Sheet2! A2 : Z > } Note that the second and all subsequent ranges should exclude the headers. If you want to change the headers, you can use the first row to specify new header names and then in row 2 set the import to start at row 2 of Sheet1.","title":"Aggregating Data"},{"location":"deployment/gs-app/#tidying-data","text":"Tidy data using the UNPIVOT custom function defined above and then filtering null values. =UNPIVOT(Sheet3!A:E, 1, 1, \"<parameter>\", \"<value>\") // in Sheet4!A1 =FILTER(Sheet4!A:E, Sheet4!A:A<>\"\") // in Sheet 5!A1 Your data is now all tidied up and ready for export!","title":"Tidying data"},{"location":"deployment/gs-app/#sharing-data","text":"The magic of Google Sheets is being able to treat multiple spreadsheets as a single system. IMPORTRANGE allows you to pull data together using the URL of each spreadsheet. Once access is granted, any editor on the destination spreadsheet can use IMPORTRANGE to pull from any part of the source spreadsheet. The access remains in effect until the user who granted access is removed from the source. If the data you are trying to import is too large, you may get an error.","title":"Sharing data"},{"location":"deployment/gs-app/#managing-connections","text":"connectionLog","title":"Managing connections"},{"location":"deployment/gs-app/#working-in-shared-sheets","text":"Filter Views Slicers","title":"Working in Shared Sheets"},{"location":"deployment/heroku/","text":"Heroku \u00b6 Heroku is an app hosting platform that supports multiple programming languages, including python. To get the most out of it, you'll need to develop a user interface for your app with a package like streamlit , dash , django or flask . Anything that renders your application in HTML will suffice. Heroku loads all of the software (i.e., python, the packages you need, and your scripts) into a container on its servers in what they call a 'dyno'. That way, your users can run the software without having to load it on their own machine. Heroku is similar to Shiny Apps, if you're used to the R ecosystem. It's also worth noting that Heroku does not store data for you. In other words, you won't be able to access the .csv files or .png files you have been passing to your program. You'll need to find a cloud-based hosting service like Google Drive or Amazon S3 for those assets. Tutorial \u00b6 This tutorial will walk you through the process of deploying a streamlit app on Heroku. You'll need to set up a virtual environment , understand the basics of git, and be comfortable working from the command line. You should also review the Google API tutorial to set up Google Sheets as a back-end. Step-by-step \u00b6 Create a new project folder Open an Anaconda Prompt window within the project folder Initialize git ( git init ) Create a .gitignore file ( code .gitignore ) Add (one per line): *.pyc , secrets/ , and any other files or folders you want ignored (e.g., notebooks/ , data/ if you have will be working in jupyter notebooks or with local data while you build your app) Create a virtual environment ( conda create -n <project-name> python ) Activate the environment ( conda activate <project-name> ) Install any packages you'll need using the conda package manager Build your project locally, working within the project's environment Create a requirements.txt file. To do this, you will first need to install pip into your environment ( install pip ). Then use the command pip freeze > requirements.txt . As an alternative, you can simply list the top-level libraries you are using (i.e., the packages you import in your script). If, when deploying to Heroku, you get an error that a package cannot be found, delete that package from requirements.txt . I've found fewer issues with just listing top-level libraries, but you risk version issues cropping up later. Optionally, create a runtime.txt file to tell Heroku which python version to install. Only certain versions are supported by Heroku, so check the documentation first. Use the command python -V > runtime.txt , or type in the python version you'd prefer Heroku install. Create a Procfile, which tells Heroku how to actually run your app. The Procfile is run when the app is initialized in each session. Type code Profile to create a new file and add the command Heroku will use to run your app. It's different for each technology, but for a streamlit app the text is sh setup.sh && streamlit run <my_app.py> . The app will run from the root folder, so make sure if your script is stored in a folder, you include it in the path (e.g, sh setup && streamlit run <scripts/my_app.py> ). Also, make sure that the relative paths in your script are relative to the root folder, NOT the script itself. For a streamlit app, you'll need to create a setup.sh file. Create a new file code setup.sh and paste the below into it (note language is bash): mkdir -p ~/.streamlit/ echo \"\\ [general]\\n\\ email = \\\"your-email@domain.com\\\"\\n\\ \" > ~/.streamlit/credentials.toml echo \"\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT \\n\\ \" > ~/.streamlit/config.toml Create a Procfile for local development, if needed: code Procfile.windows web: streamlit run scripts/<my_app.py> runserver 0.0.0.0:5000 Commit changes to git ( git add . , git commit -m \"message\" ) Create the Heroku app: heroku create <app name> where <app name> will serve as the base of the url and the name of the app on Heroku's dashboard Deploy the code to Heroku: git push heroku master Spin up one dyno: heroku ps:scale web=1 (use heroku ps:scale web=0 ) to shut it down. Use heroku ps to see how many dynos are running. Open the app: heroku open . If the app isn't up yet, or something looks wrong, check the logs with heroku logs --tail . You can close the logs with ctrl+c . If you need to run the app locally, you can use the command heroku local web -f Procfile.windows . If you make changes to the app, commit everything to git and then push it back up to Heroku. Every time you push to Heroku, it rebuilds the dyno from scratch, which can take some time depending on how many packages you need to load, so try to do that sparingly. If you have API tokens or other sensitive information, you'll need to configure Heroku's configuration variables. This is equivalent to setting an environment variable on your local machine. These variables will only be exposed at runtime, and won't be accessible to your users. Simply type heroku config:set <VAR_NAME> = <VAR_VALUE> . For JSON blobs, it's easiest to add them through Heroku's online interface (under the 'Settings' of the app.) You can paste in the prettified JSON as a new variable there. See the Google API page for more info on accessing those variables within your scripts. Your app should now be up and running on Heroku. If you're on a free tier, it may take 10-20 seconds for the app to load each time because the server needs to spin it up. It will be active for 30 minutes, but will sleep again after that time. You can upgrade to a Hobby license for $7 per dyno per month. Update an existing app \u00b6 Updating an existing app is as simple as running git push heroku master from the project's root directory. However, if you have cloned the repository (or you, say, deployed it last time from a different computer), you'll need to establish the remote connection with heroku first. Type heroku login to login (a browser window will open) and then heroku git:remote -a <app-name> to connect to the app. The <app-name> can be found on your heroku page . Notes \u00b6 Note that in recent updates the setup.sh may not be required, instead you can use this command in the Procfile: web: streamlit run --server.enableCORS false my_script.py (but I haven't tested it).","title":"Heroku"},{"location":"deployment/heroku/#heroku","text":"Heroku is an app hosting platform that supports multiple programming languages, including python. To get the most out of it, you'll need to develop a user interface for your app with a package like streamlit , dash , django or flask . Anything that renders your application in HTML will suffice. Heroku loads all of the software (i.e., python, the packages you need, and your scripts) into a container on its servers in what they call a 'dyno'. That way, your users can run the software without having to load it on their own machine. Heroku is similar to Shiny Apps, if you're used to the R ecosystem. It's also worth noting that Heroku does not store data for you. In other words, you won't be able to access the .csv files or .png files you have been passing to your program. You'll need to find a cloud-based hosting service like Google Drive or Amazon S3 for those assets.","title":"Heroku"},{"location":"deployment/heroku/#tutorial","text":"This tutorial will walk you through the process of deploying a streamlit app on Heroku. You'll need to set up a virtual environment , understand the basics of git, and be comfortable working from the command line. You should also review the Google API tutorial to set up Google Sheets as a back-end.","title":"Tutorial"},{"location":"deployment/heroku/#step-by-step","text":"Create a new project folder Open an Anaconda Prompt window within the project folder Initialize git ( git init ) Create a .gitignore file ( code .gitignore ) Add (one per line): *.pyc , secrets/ , and any other files or folders you want ignored (e.g., notebooks/ , data/ if you have will be working in jupyter notebooks or with local data while you build your app) Create a virtual environment ( conda create -n <project-name> python ) Activate the environment ( conda activate <project-name> ) Install any packages you'll need using the conda package manager Build your project locally, working within the project's environment Create a requirements.txt file. To do this, you will first need to install pip into your environment ( install pip ). Then use the command pip freeze > requirements.txt . As an alternative, you can simply list the top-level libraries you are using (i.e., the packages you import in your script). If, when deploying to Heroku, you get an error that a package cannot be found, delete that package from requirements.txt . I've found fewer issues with just listing top-level libraries, but you risk version issues cropping up later. Optionally, create a runtime.txt file to tell Heroku which python version to install. Only certain versions are supported by Heroku, so check the documentation first. Use the command python -V > runtime.txt , or type in the python version you'd prefer Heroku install. Create a Procfile, which tells Heroku how to actually run your app. The Procfile is run when the app is initialized in each session. Type code Profile to create a new file and add the command Heroku will use to run your app. It's different for each technology, but for a streamlit app the text is sh setup.sh && streamlit run <my_app.py> . The app will run from the root folder, so make sure if your script is stored in a folder, you include it in the path (e.g, sh setup && streamlit run <scripts/my_app.py> ). Also, make sure that the relative paths in your script are relative to the root folder, NOT the script itself. For a streamlit app, you'll need to create a setup.sh file. Create a new file code setup.sh and paste the below into it (note language is bash): mkdir -p ~/.streamlit/ echo \"\\ [general]\\n\\ email = \\\"your-email@domain.com\\\"\\n\\ \" > ~/.streamlit/credentials.toml echo \"\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT \\n\\ \" > ~/.streamlit/config.toml Create a Procfile for local development, if needed: code Procfile.windows web: streamlit run scripts/<my_app.py> runserver 0.0.0.0:5000 Commit changes to git ( git add . , git commit -m \"message\" ) Create the Heroku app: heroku create <app name> where <app name> will serve as the base of the url and the name of the app on Heroku's dashboard Deploy the code to Heroku: git push heroku master Spin up one dyno: heroku ps:scale web=1 (use heroku ps:scale web=0 ) to shut it down. Use heroku ps to see how many dynos are running. Open the app: heroku open . If the app isn't up yet, or something looks wrong, check the logs with heroku logs --tail . You can close the logs with ctrl+c . If you need to run the app locally, you can use the command heroku local web -f Procfile.windows . If you make changes to the app, commit everything to git and then push it back up to Heroku. Every time you push to Heroku, it rebuilds the dyno from scratch, which can take some time depending on how many packages you need to load, so try to do that sparingly. If you have API tokens or other sensitive information, you'll need to configure Heroku's configuration variables. This is equivalent to setting an environment variable on your local machine. These variables will only be exposed at runtime, and won't be accessible to your users. Simply type heroku config:set <VAR_NAME> = <VAR_VALUE> . For JSON blobs, it's easiest to add them through Heroku's online interface (under the 'Settings' of the app.) You can paste in the prettified JSON as a new variable there. See the Google API page for more info on accessing those variables within your scripts. Your app should now be up and running on Heroku. If you're on a free tier, it may take 10-20 seconds for the app to load each time because the server needs to spin it up. It will be active for 30 minutes, but will sleep again after that time. You can upgrade to a Hobby license for $7 per dyno per month.","title":"Step-by-step"},{"location":"deployment/heroku/#update-an-existing-app","text":"Updating an existing app is as simple as running git push heroku master from the project's root directory. However, if you have cloned the repository (or you, say, deployed it last time from a different computer), you'll need to establish the remote connection with heroku first. Type heroku login to login (a browser window will open) and then heroku git:remote -a <app-name> to connect to the app. The <app-name> can be found on your heroku page .","title":"Update an existing app"},{"location":"deployment/heroku/#notes","text":"Note that in recent updates the setup.sh may not be required, instead you can use this command in the Procfile: web: streamlit run --server.enableCORS false my_script.py (but I haven't tested it).","title":"Notes"},{"location":"deployment/lightweight-backends/","text":"Lightweight Backends \u00b6 For some applications, you don't need or want to go through the extra effort of setting up a database backend. You have a few options for hosting data for web-based applications and similar tools. This topic covers a few of those options. Google Sheets Download (no API required) \u00b6 Google Sheets can be downloaded automatically as CSV files using a URL that you can construct from the sheet\u2019s key. Take this sheet, for example: https://docs.google.com/spreadsheets/d/1qrtaGu5dwljVbHJeR4RIMWtdZKOauJfJ6nODlPwMRno/edit?usp=sharing The \u201ckey\u201d is the long string appearing after \u201c/d/\u201d: 1qrtaGu5dwljVbHJeR4RIMWtdZKOauJfJ6nODlPwMRno You can create a CSV download link for that sheet, by plugging the \u201ckey\u201d into this URL structure: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv Tip You can also get a download link by downloading the file as csv from Google Sheets (FIle> Download> Comma separated values), navigating to your downloads folder (Ctrl+J), right clicking the link and selecting 'Copy Link Address'. Any application with that link will download a csv file directly. If you have multiple sheets, specify the sheet by appending &sheet=<sheet> . You can also pass data from one Google Sheet to another by using the IMPORTRANGE() function. In the cell you want to import these data, type: = importrange ( \"<key>\" , \"Sheet1!A:Z\" ) To use Google Sheets as a full-stack application solution, see the Google Sheets Application guide . Google Sheets (with pygsheets) \u00b6 Google Sheets can be a great lightweight cloud database for you application. Check out the Google API tutorial for an in depth discussion of how to set this up. GitHub \u00b6 GitHub makes available any csvs committed to a repo through a 'raw' link. Navigate to the csv file and select the 'raw' link. Provide that url to your application. This is a good option if you have a very stable database (i.e., doesn't require changes) or you will be pushing updates to the files regularly. Be careful not to get your test environment and development environment confused with this option.","title":"Lightweight Backends"},{"location":"deployment/lightweight-backends/#lightweight-backends","text":"For some applications, you don't need or want to go through the extra effort of setting up a database backend. You have a few options for hosting data for web-based applications and similar tools. This topic covers a few of those options.","title":"Lightweight Backends"},{"location":"deployment/lightweight-backends/#google-sheets-download-no-api-required","text":"Google Sheets can be downloaded automatically as CSV files using a URL that you can construct from the sheet\u2019s key. Take this sheet, for example: https://docs.google.com/spreadsheets/d/1qrtaGu5dwljVbHJeR4RIMWtdZKOauJfJ6nODlPwMRno/edit?usp=sharing The \u201ckey\u201d is the long string appearing after \u201c/d/\u201d: 1qrtaGu5dwljVbHJeR4RIMWtdZKOauJfJ6nODlPwMRno You can create a CSV download link for that sheet, by plugging the \u201ckey\u201d into this URL structure: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv Tip You can also get a download link by downloading the file as csv from Google Sheets (FIle> Download> Comma separated values), navigating to your downloads folder (Ctrl+J), right clicking the link and selecting 'Copy Link Address'. Any application with that link will download a csv file directly. If you have multiple sheets, specify the sheet by appending &sheet=<sheet> . You can also pass data from one Google Sheet to another by using the IMPORTRANGE() function. In the cell you want to import these data, type: = importrange ( \"<key>\" , \"Sheet1!A:Z\" ) To use Google Sheets as a full-stack application solution, see the Google Sheets Application guide .","title":"Google Sheets Download (no API required)"},{"location":"deployment/lightweight-backends/#google-sheets-with-pygsheets","text":"Google Sheets can be a great lightweight cloud database for you application. Check out the Google API tutorial for an in depth discussion of how to set this up.","title":"Google Sheets (with pygsheets)"},{"location":"deployment/lightweight-backends/#github","text":"GitHub makes available any csvs committed to a repo through a 'raw' link. Navigate to the csv file and select the 'raw' link. Provide that url to your application. This is a good option if you have a very stable database (i.e., doesn't require changes) or you will be pushing updates to the files regularly. Be careful not to get your test environment and development environment confused with this option.","title":"GitHub"},{"location":"development/command-line-tools/","text":"Command Line Tools \u00b6 Once you get comfortable with the command line, why not create your own utilities for quick tasks? The easy way \u00b6 The easiest way to use the command line to run python programs is simply to write a script and run it from the CLI with python: python <my_app.py> If you don't want to provide the fully-qualified path to your script, open the command prompt from within the project's directory, or cd into it. Nothing more to it than that. With arguments \u00b6 If you need to pass arguments to your application (e.g., a file to save output), you'll need to be able to handle those arguments. The command line will pass anything you type after python <my_app.py> as strings to the program, but without a way of parsing and validating those strings, those arguments just get ignored. There are a number of packages that you can explore for developing command line tools: fire argpars click fire may be the easiest to use right out of the box, while click has more control for validation and providing help at the command line. Check out these tutorials on command line interface apps to get started: How to write python command line interfaces like a pro Python command line tools","title":"Command Line Tools"},{"location":"development/command-line-tools/#command-line-tools","text":"Once you get comfortable with the command line, why not create your own utilities for quick tasks?","title":"Command Line Tools"},{"location":"development/command-line-tools/#the-easy-way","text":"The easiest way to use the command line to run python programs is simply to write a script and run it from the CLI with python: python <my_app.py> If you don't want to provide the fully-qualified path to your script, open the command prompt from within the project's directory, or cd into it. Nothing more to it than that.","title":"The easy way"},{"location":"development/command-line-tools/#with-arguments","text":"If you need to pass arguments to your application (e.g., a file to save output), you'll need to be able to handle those arguments. The command line will pass anything you type after python <my_app.py> as strings to the program, but without a way of parsing and validating those strings, those arguments just get ignored. There are a number of packages that you can explore for developing command line tools: fire argpars click fire may be the easiest to use right out of the box, while click has more control for validation and providing help at the command line. Check out these tutorials on command line interface apps to get started: How to write python command line interfaces like a pro Python command line tools","title":"With arguments"},{"location":"development/environment-variables/","text":"Environment Variables \u00b6 Environment variables allow you to use sensitive or secret information (credentials, passwords, tokens, etc.) in a script without hard-coding those values into your script. Typically, you won't want to store something like a password as a string directly in your code. If you commit that code to GitHub it becomes a permanent part of your code's history and thus accessible to anyone who accesses your project on GitHub. There are web crawlers that sift through GitHub repositories looking for information like this, so be careful. Setting Environment Variables \u00b6 Environment variables are set differently on different operating systems. On Windows, just type 'environment variables' into the start menu and open the System Properties dialog box ('Advanced' tab). Click on 'Environment Variables' to edit. Adding as a user variable, rather than a system variable, is generally sufficient. Conventionally, both the name and the value are uppercase. Saving Environment Variables in a conda environment \u00b6 You can set up environment variables within a conda environment. See here . Using Environment Variables \u00b6 Python reads the environment variables when os is imported. Environment variables are passed as a dictionary. Since this is simply a dictionary, you can use os.environ.get('<KEY>') to access the value with the name of the variable (where '<KEY>' is the name you specified for the environment variable). For example: import os my_password = os . environ . get ( 'PASS_KEY' ) Using environment variables in Heroku \u00b6 In Heroku, environment variables are set under 'Settings' by clicking 'Reveal Config Vars'. These are accessed in exactly the same way as above. A caveat \u00b6 Environment variables are commonly used, but not always the best option for storing secrets. They can be subject to mangling by the operating system, and some access tokens don't conform to their limitations (e.g., my google access token had a '=' character, which is not allowed). Especially if your secrets are stored as a JSON blob, you might consider saving them in a JSON file and reading them directly from the file. For the A/V Crowd \u00b6 This video captures the essentials of Environment Variables in 5 minutes. Worth a watch for anything I missed.","title":"Environment Variables"},{"location":"development/environment-variables/#environment-variables","text":"Environment variables allow you to use sensitive or secret information (credentials, passwords, tokens, etc.) in a script without hard-coding those values into your script. Typically, you won't want to store something like a password as a string directly in your code. If you commit that code to GitHub it becomes a permanent part of your code's history and thus accessible to anyone who accesses your project on GitHub. There are web crawlers that sift through GitHub repositories looking for information like this, so be careful.","title":"Environment Variables"},{"location":"development/environment-variables/#setting-environment-variables","text":"Environment variables are set differently on different operating systems. On Windows, just type 'environment variables' into the start menu and open the System Properties dialog box ('Advanced' tab). Click on 'Environment Variables' to edit. Adding as a user variable, rather than a system variable, is generally sufficient. Conventionally, both the name and the value are uppercase.","title":"Setting Environment Variables"},{"location":"development/environment-variables/#saving-environment-variables-in-a-conda-environment","text":"You can set up environment variables within a conda environment. See here .","title":"Saving Environment Variables in a conda environment"},{"location":"development/environment-variables/#using-environment-variables","text":"Python reads the environment variables when os is imported. Environment variables are passed as a dictionary. Since this is simply a dictionary, you can use os.environ.get('<KEY>') to access the value with the name of the variable (where '<KEY>' is the name you specified for the environment variable). For example: import os my_password = os . environ . get ( 'PASS_KEY' )","title":"Using Environment Variables"},{"location":"development/environment-variables/#using-environment-variables-in-heroku","text":"In Heroku, environment variables are set under 'Settings' by clicking 'Reveal Config Vars'. These are accessed in exactly the same way as above.","title":"Using environment variables in Heroku"},{"location":"development/environment-variables/#a-caveat","text":"Environment variables are commonly used, but not always the best option for storing secrets. They can be subject to mangling by the operating system, and some access tokens don't conform to their limitations (e.g., my google access token had a '=' character, which is not allowed). Especially if your secrets are stored as a JSON blob, you might consider saving them in a JSON file and reading them directly from the file.","title":"A caveat"},{"location":"development/environment-variables/#for-the-av-crowd","text":"This video captures the essentials of Environment Variables in 5 minutes. Worth a watch for anything I missed.","title":"For the A/V Crowd"},{"location":"development/knowledge-base/","text":"Knowledge Base \u00b6 Knowledge Base services \u00b6 Wikipedia \u00b6 Google API \u00b6 Wikidata \u00b6 Entity Linking \u00b6 Named entity recognition \u00b6 Entity Linking \u00b6 TagMe \u00b6 Cite applications that use TagMe: M. Assante et al. (2019) Enacting open science by D4Science. Future Gener. Comput. Syst. 101: 555-563 10.1016/j.future.2019.05.063","title":"Knowledge Base"},{"location":"development/knowledge-base/#knowledge-base","text":"","title":"Knowledge Base"},{"location":"development/knowledge-base/#knowledge-base-services","text":"","title":"Knowledge Base services"},{"location":"development/knowledge-base/#wikipedia","text":"","title":"Wikipedia"},{"location":"development/knowledge-base/#google-api","text":"","title":"Google API"},{"location":"development/knowledge-base/#wikidata","text":"","title":"Wikidata"},{"location":"development/knowledge-base/#entity-linking","text":"","title":"Entity Linking"},{"location":"development/knowledge-base/#named-entity-recognition","text":"","title":"Named entity recognition"},{"location":"development/knowledge-base/#entity-linking_1","text":"","title":"Entity Linking"},{"location":"development/knowledge-base/#tagme","text":"Cite applications that use TagMe: M. Assante et al. (2019) Enacting open science by D4Science. Future Gener. Comput. Syst. 101: 555-563 10.1016/j.future.2019.05.063","title":"TagMe"},{"location":"development/scheduling-tasks/","text":"Scheduling Tasks \u00b6 If you have a task that needs to be completed regularly, and it's something you can automate with a python script, you can schedule the task to occur at set times to save you the hassle. It also helps guarantee that the task gets done when it should. Python has packages available for scheduling tasks, such as python-chrontab . However, on a Windows machine it's possibly easiest to use the Windows Task Scheduler . Alternatively, you can use schedule task to run when you start up your computer . Windows Task Scheduler \u00b6 1. Create a .bat file \u00b6 To use the Windows Task Scheduler to , you'll first need to create a .bat file. Open a new file on VS Code or notepad and type: \" <Path to python executable> \" \" <Path to script> \" pause The path to the python executable for me is: \"C:\\Users\\Erik\\Anaconda3\\python.exe\" The path to your script is the fully qualified path to your script, don't forget to include the script name and .py at the end. Save the file and change the extension to .bat (e.g., run_me.bat ). You can run your script by double clicking on the file. Running a .bat file is a lot like typing python <my_script.py> into the CLI. 2. Schedule the task \u00b6 To schedule the task, type 'schedule task' in the Windows start menu and open the Task Scheduler. Select 'Create Basic Task...' and work through the interface. You can open the settings dialog box for more options (see below) before saving. Additional settings available \u00b6 General : Provide any name you want Triggers : Add a New Trigger and select the type of trigger you'd prefer. The task can be run at a specific time and day, but also upon logging on to the computer or other options. Actions : Add a New Action and select 'Start a Program'. Conditions : You can specify additional conditions here. Settings : Additional settings here. I recommend selecting 'Run task as soon as possible after a scheduled start is missed', in case the computer isn't on when the task should have been run. The Startup Folder \u00b6 Navigate to your .bat file, right-click and select copy. Use the Windows key + R keyboard shortcut to open the Run command. Type in shell:startup and click OK. Paste a shortcut to the .bat file in this folder and it will run on every startup (you'll need to restart for this to take effect). Note that if you shut down your computer more than once a day, this will run the script multiple times that day. If you just want a daily task to run, use the Task Scheduler. Read more about using batch files in Windows . Getting an error? \u00b6 In some cases (especially if you have multiple python versions installed on your computer), you may need to activate the base Anaconda environment before attempting to run your script in the .bat file. To do this, simply add a line at the beginning of your .bat file (edit the path as necessary to for your machine) so it looks like this: call \"C:\\Users\\Erik\\Anaconda3\\Scripts\\activate.bat\" \"C:\\Users\\Erik\\Anaconda3\\python.exe\" \"<Path to script>\" The call function is used to execute another .bat file from within a .bat file, and in this case will activate the base anaconda environment which will allow your machine to find the packages you need to run your script. If you don't do this, you may get a DLL load error when attempting to run the .bat file.","title":"Scheduling Tasks"},{"location":"development/scheduling-tasks/#scheduling-tasks","text":"If you have a task that needs to be completed regularly, and it's something you can automate with a python script, you can schedule the task to occur at set times to save you the hassle. It also helps guarantee that the task gets done when it should. Python has packages available for scheduling tasks, such as python-chrontab . However, on a Windows machine it's possibly easiest to use the Windows Task Scheduler . Alternatively, you can use schedule task to run when you start up your computer .","title":"Scheduling Tasks"},{"location":"development/scheduling-tasks/#windows-task-scheduler","text":"","title":"Windows Task Scheduler"},{"location":"development/scheduling-tasks/#1-create-a-bat-file","text":"To use the Windows Task Scheduler to , you'll first need to create a .bat file. Open a new file on VS Code or notepad and type: \" <Path to python executable> \" \" <Path to script> \" pause The path to the python executable for me is: \"C:\\Users\\Erik\\Anaconda3\\python.exe\" The path to your script is the fully qualified path to your script, don't forget to include the script name and .py at the end. Save the file and change the extension to .bat (e.g., run_me.bat ). You can run your script by double clicking on the file. Running a .bat file is a lot like typing python <my_script.py> into the CLI.","title":"1. Create a .bat file"},{"location":"development/scheduling-tasks/#2-schedule-the-task","text":"To schedule the task, type 'schedule task' in the Windows start menu and open the Task Scheduler. Select 'Create Basic Task...' and work through the interface. You can open the settings dialog box for more options (see below) before saving.","title":"2. Schedule the task"},{"location":"development/scheduling-tasks/#additional-settings-available","text":"General : Provide any name you want Triggers : Add a New Trigger and select the type of trigger you'd prefer. The task can be run at a specific time and day, but also upon logging on to the computer or other options. Actions : Add a New Action and select 'Start a Program'. Conditions : You can specify additional conditions here. Settings : Additional settings here. I recommend selecting 'Run task as soon as possible after a scheduled start is missed', in case the computer isn't on when the task should have been run.","title":"Additional settings available"},{"location":"development/scheduling-tasks/#the-startup-folder","text":"Navigate to your .bat file, right-click and select copy. Use the Windows key + R keyboard shortcut to open the Run command. Type in shell:startup and click OK. Paste a shortcut to the .bat file in this folder and it will run on every startup (you'll need to restart for this to take effect). Note that if you shut down your computer more than once a day, this will run the script multiple times that day. If you just want a daily task to run, use the Task Scheduler. Read more about using batch files in Windows .","title":"The Startup Folder"},{"location":"development/scheduling-tasks/#getting-an-error","text":"In some cases (especially if you have multiple python versions installed on your computer), you may need to activate the base Anaconda environment before attempting to run your script in the .bat file. To do this, simply add a line at the beginning of your .bat file (edit the path as necessary to for your machine) so it looks like this: call \"C:\\Users\\Erik\\Anaconda3\\Scripts\\activate.bat\" \"C:\\Users\\Erik\\Anaconda3\\python.exe\" \"<Path to script>\" The call function is used to execute another .bat file from within a .bat file, and in this case will activate the base anaconda environment which will allow your machine to find the packages you need to run your script. If you don't do this, you may get a DLL load error when attempting to run the .bat file.","title":"Getting an error?"},{"location":"development/secrets/","text":"Secrets \u00b6 Secrets are any pieces of information you don't want to store directly in your code. Generally, this includes passwords and other credentials for logging into third-party applications. If you need to provide a password to sign into a database, how do you make the password available without storing it in the code itself? The secrets/ folder \u00b6 I add a secrets/ folder to my projects and save credentials there as JSON files. MAKE SURE TO INCLUDE secrets/ IN YOUR .gitignore FILE (but add a .keep file )! Here's what a my_secrets_file.json file would look like with a secret key: { \"USERNAME\" : \"<save_key_here>\" , \"PASSWORD\" : \"<save_password_here\" } Note that JSON files require double quotes. Accessing secrets \u00b6 You can read that into python just like a dictionary: import json with open ( 'secrets/my_secrets_file.json' ) as f : my_creds = json . load ( f ) USERNAME = my_creds . get ( \"USERNAME\" ) PASSWORD = my_creds . get ( \"PASSWORD\" ) Don't commit secrets \u00b6 When you create the secrets/ folder, go ahead and add it to your .gitignore file, even before you save anything there, so you don't forget. Add a .keep file so that collaborators know that there are credentials required for this project. Add these lines to the .gitignore file: secrets /* !secrets/.keep If you want, you can save the structure of the credentials in the .keep file to help collaborators set it up correctly (obviously don't actually put the secret info here): - my_secrets_file.json: { \"USERNAME\": \"<save_key_here>\", \"PASSWORD\": \"<save_password_here\" } - credentials.json: ... Remove secrets from Github \u00b6 You will, sooner or later, commit a secret to GitHub. GitHub (or the third-party app) may even email you right after you did it (e.g., Google will email you if you commit google credentials to GitHub). To remove the file, use git rm -r --cached <filename> . Then commit and push the changes to GitHub. Auth packages \u00b6 Some packages such as google-auth or oauthlib access and use credentials stored as a JSON file. Environment Variables \u00b6 Another way to store secrets is to use environment variables . You will need to use environment variables (rather than JSON files) with some platforms like Heroku. Here's how you can structure code to seamlessly transition between local testing and production: # Local dev try : with open ( 'secrets/passwords.json' ) as f : VALID_USERNAME_PASSWORD_PAIRS = json . load ( f ) # Heroku dev except : json_creds = os . environ . get ( \"VALID_USERNAME_PASSWORD_PAIRS\" ) VALID_USERNAME_PASSWORD_PAIRS = json . loads ( json_creds )","title":"Secrets"},{"location":"development/secrets/#secrets","text":"Secrets are any pieces of information you don't want to store directly in your code. Generally, this includes passwords and other credentials for logging into third-party applications. If you need to provide a password to sign into a database, how do you make the password available without storing it in the code itself?","title":"Secrets"},{"location":"development/secrets/#the-secrets-folder","text":"I add a secrets/ folder to my projects and save credentials there as JSON files. MAKE SURE TO INCLUDE secrets/ IN YOUR .gitignore FILE (but add a .keep file )! Here's what a my_secrets_file.json file would look like with a secret key: { \"USERNAME\" : \"<save_key_here>\" , \"PASSWORD\" : \"<save_password_here\" } Note that JSON files require double quotes.","title":"The secrets/ folder"},{"location":"development/secrets/#accessing-secrets","text":"You can read that into python just like a dictionary: import json with open ( 'secrets/my_secrets_file.json' ) as f : my_creds = json . load ( f ) USERNAME = my_creds . get ( \"USERNAME\" ) PASSWORD = my_creds . get ( \"PASSWORD\" )","title":"Accessing secrets"},{"location":"development/secrets/#dont-commit-secrets","text":"When you create the secrets/ folder, go ahead and add it to your .gitignore file, even before you save anything there, so you don't forget. Add a .keep file so that collaborators know that there are credentials required for this project. Add these lines to the .gitignore file: secrets /* !secrets/.keep If you want, you can save the structure of the credentials in the .keep file to help collaborators set it up correctly (obviously don't actually put the secret info here): - my_secrets_file.json: { \"USERNAME\": \"<save_key_here>\", \"PASSWORD\": \"<save_password_here\" } - credentials.json: ...","title":"Don't commit secrets"},{"location":"development/secrets/#remove-secrets-from-github","text":"You will, sooner or later, commit a secret to GitHub. GitHub (or the third-party app) may even email you right after you did it (e.g., Google will email you if you commit google credentials to GitHub). To remove the file, use git rm -r --cached <filename> . Then commit and push the changes to GitHub.","title":"Remove secrets from Github"},{"location":"development/secrets/#auth-packages","text":"Some packages such as google-auth or oauthlib access and use credentials stored as a JSON file.","title":"Auth packages"},{"location":"development/secrets/#environment-variables","text":"Another way to store secrets is to use environment variables . You will need to use environment variables (rather than JSON files) with some platforms like Heroku. Here's how you can structure code to seamlessly transition between local testing and production: # Local dev try : with open ( 'secrets/passwords.json' ) as f : VALID_USERNAME_PASSWORD_PAIRS = json . load ( f ) # Heroku dev except : json_creds = os . environ . get ( \"VALID_USERNAME_PASSWORD_PAIRS\" ) VALID_USERNAME_PASSWORD_PAIRS = json . loads ( json_creds )","title":"Environment Variables"},{"location":"development/virtual-environments/","text":"Virtual Environments \u00b6 Virtual environments allow you to maintain a controlled development environment and are useful when deploying your application. They're actually quite easy to set up and use. See the basic workflow for a quick reference. Why use a virtual environment? \u00b6 Imagine you have built an app using Python 2.7. You decide to upgrade your Anaconda distribution, which upgrades all of your packages and upgrades Python to 3.6. All of the sudden, all of your print statements return an error because they are missing parentheses! To avoid this, you can create virtual environments for each of your projects. A virtual environment simply saves all of the packages you're using, as well as the Python version (if specified), in a unique folder rather than with the base Anaconda packages. This way, you can always be sure that the packages you're using aren't upgraded (or downgraded) in the future when you upgrade or install new packages to your base environment. The base environment \u00b6 If you're not working in a virtual environment you have created, you're working in the base conda environment (assuming you're working from an Anaconda distribution). You'll notice that every time you open an Anaconda Prompt window, it automatically activates the base environment with the command conda activate base . When you conda install a package from the base environment, it is stored in this base environment. Creating virtual environments \u00b6 To create a virtual environment, simply use the command conda create -n <env_name> . If you want a specific python version, specify it (for example: conda create -n <env_name> python=3.8 ). This environment is stored in Anaconda's environment directory ( C:\\Users\\<User>\\Anaconda3\\envs ), rather than the project's folder. Be sure to use the Anaconda Prompt window when creating virtual environments. If you're not using Anaconda, you can use the venv or virtualenv packages (however they do not work with the Anaconda distribution.) venv is the preferred package and should work fine, unless you need to specify a python version, then use virtualenv . For more info on those packages, see their documentation. Working in virtual environments \u00b6 To activate an environment, simply type conda activate <env_name> . To deactivate, type conda deactivate . If you're using VS Code, you should tell it that you are working in a virtual environment so it can properly lint (highlight errors) based on the packages in your environment. Here's how: Ctrl+Shift+P to open the command palette Search python: select interpreter Select the environment from the dropdown list You will likely be prompted to install pylint into this environment, go ahead and do so using conda. Virtual environment workflow \u00b6 Here's a quick workflow reference for setting up and working with virtual environments: Open an Anaconda Prompt window Create the environment: conda create -n <name-of-environment> [python=3.8] (note the square brackets denote an optional parameter, if you want to include a python package, ) Activate the environment: conda activate <name-of-environment> Install necessary packages: conda install -c conda-forge <package-name> (Check Anaconda's package repo for the correct channel to install from) Build your project Export the environment to an environment.yml file: conda env export > environment.yml If you need to deactivate the environment, use conda deactivate or conda activate base to go back to the base environment (both have the same result). For packages not on Anaconda's package repo, you may need to install pip before using a pip install command. Once you start using pip , don't go back to using conda to install packages. Do not use pip in the base environment. You'll note that the environment.yml file will specify which packages must be installed using pip . Sharing virtual environments \u00b6 Another great thing about virtual environments is that they are easy to share. And, if you're planning to deploy an app on a platform like Heroku, you must specify the environment (by specifying the necessary packages in a requirements.txt file). Exporting a virtual environment \u00b6 Virtual environments are shared with a requirements.txt or environment.yml file. Others can use this to recreate the environment on their own machines. To create this file, make sure the environment is activated and type conda env export > environment.yml or pip freeze > requirements.txt . This file will be saved in the current working directory. Generally, you should save your environment in an environment.yml file since we'll be using conda environments most of the time. Heroku, however, requires a requirements.txt file. It's ok to have both in a project. If you know you will be working on the same operating system (same machine or another machine), you can create an exact replica of the environment using conda list --explicit > environment.yml . This will ensure that all dependencies are of the same version. This likely won't work if you are recreating the environment on a different OS version (e.g., Windows 10 vs. 8.1). If instead you may be working on completely different operating systems (e.g., Windows vs. OSx), you can be extra cautious and use conda env export --from-history > environment.yml . This will only include packages specifically installed into the environment, and allow conda to resolve any dependencies. Duplicate virtual environment from file \u00b6 To duplicate a virtual environment from the environment.yml file, use the command conda env create -f environment.yml . This will create the environment with the same name as the original environment. To duplicate a virtual environment from the requirements.txt file, first create an empty conda environment conda create --name <env name> , install pip conda install pip and then install the packages from the file pip install -r requirements.txt . Cloning an environment \u00b6 To clone an environment, simply conda create --name <new_env_name> --clone <old_env_name> . Update a virtual environment \u00b6 You can add packages to the environment simply by using conda install <package> or pip install <package> as you're building the project. Make sure to export the environment after installing new packages so that it's always up-to-date! If changes were made to the environment by a collaborator or on a cloned project, you can install the new packages from the updated environemnt.yml file with the command conda env update --prefix ./env --file environment.yml --prune . The --prune option causes conda to remove old dependencies. Common issues \u00b6 If you are trying to recreate an environment created on another OS platform (e.g., Windows vs. OSx, Windows 10 vs. 8.1) from an environment.yml file, the specific dependencies may not be available on your platform. In this case, go back to the original project and use conda env export --from-history > environment.yml . This will create a file using only the packages specifically imported (excluding any dependencies) and conda will resolve those dependencies on its own. Note that the two environments might not be identical, but it's better than not having the environment at all! For a quick fix, try removing the version number from the packages in the environment.yml or requirements.txt file, and/or deleting packages you know to be dependencies (i.e., numpy is a dependency of pandas , only list pandas in the file). When to create an environment \u00b6 I recommend creating a new environment for any long-lived project using conda create -n <env_name> , saving the name of the environment as the same as the root folder of the project. Install as many packages as possible with conda, then install pip and use pip as the package manager (see here for why). The base conda environment ( conda activate base ) is useful for quick projects or ongoing-development type projects that you don't mind breaking from time to time. Always keep the environments.yml file up-to-date! Anytime you install a package, export the environment. If you code a project without a virtual environment and don't come back to it for a year, there's a good chance you'll find the code doesn't work anymore, or worse yet behaves unexpectedly, and you'll need to go through and update it--which might be more trouble than it's worth. So use virtual environments to save you the hassle and keep the requirements.txt or environment.yml file up to date in the root folder of the project. More commands \u00b6 To see your environments: conda env list . To see packages in an environment: conda list -n <env_name> or simply conda list if the environment is active. To remove an environment: conda remove -n <env_name> --all . To update conda: conda update conda . To update a package: conda update <package> . To change python version: conda install python=2.7 (make sure the environment is activated or this will downgrade/upgrade python system-wide!) You can set up environment variables within an environment (although this is not my preferred way of handling secrets ). See here . Resources \u00b6 CONDA CHEATSHEET","title":"Virtual Environments"},{"location":"development/virtual-environments/#virtual-environments","text":"Virtual environments allow you to maintain a controlled development environment and are useful when deploying your application. They're actually quite easy to set up and use. See the basic workflow for a quick reference.","title":"Virtual Environments"},{"location":"development/virtual-environments/#why-use-a-virtual-environment","text":"Imagine you have built an app using Python 2.7. You decide to upgrade your Anaconda distribution, which upgrades all of your packages and upgrades Python to 3.6. All of the sudden, all of your print statements return an error because they are missing parentheses! To avoid this, you can create virtual environments for each of your projects. A virtual environment simply saves all of the packages you're using, as well as the Python version (if specified), in a unique folder rather than with the base Anaconda packages. This way, you can always be sure that the packages you're using aren't upgraded (or downgraded) in the future when you upgrade or install new packages to your base environment.","title":"Why use a virtual environment?"},{"location":"development/virtual-environments/#the-base-environment","text":"If you're not working in a virtual environment you have created, you're working in the base conda environment (assuming you're working from an Anaconda distribution). You'll notice that every time you open an Anaconda Prompt window, it automatically activates the base environment with the command conda activate base . When you conda install a package from the base environment, it is stored in this base environment.","title":"The base environment"},{"location":"development/virtual-environments/#creating-virtual-environments","text":"To create a virtual environment, simply use the command conda create -n <env_name> . If you want a specific python version, specify it (for example: conda create -n <env_name> python=3.8 ). This environment is stored in Anaconda's environment directory ( C:\\Users\\<User>\\Anaconda3\\envs ), rather than the project's folder. Be sure to use the Anaconda Prompt window when creating virtual environments. If you're not using Anaconda, you can use the venv or virtualenv packages (however they do not work with the Anaconda distribution.) venv is the preferred package and should work fine, unless you need to specify a python version, then use virtualenv . For more info on those packages, see their documentation.","title":"Creating virtual environments"},{"location":"development/virtual-environments/#working-in-virtual-environments","text":"To activate an environment, simply type conda activate <env_name> . To deactivate, type conda deactivate . If you're using VS Code, you should tell it that you are working in a virtual environment so it can properly lint (highlight errors) based on the packages in your environment. Here's how: Ctrl+Shift+P to open the command palette Search python: select interpreter Select the environment from the dropdown list You will likely be prompted to install pylint into this environment, go ahead and do so using conda.","title":"Working in virtual environments"},{"location":"development/virtual-environments/#virtual-environment-workflow","text":"Here's a quick workflow reference for setting up and working with virtual environments: Open an Anaconda Prompt window Create the environment: conda create -n <name-of-environment> [python=3.8] (note the square brackets denote an optional parameter, if you want to include a python package, ) Activate the environment: conda activate <name-of-environment> Install necessary packages: conda install -c conda-forge <package-name> (Check Anaconda's package repo for the correct channel to install from) Build your project Export the environment to an environment.yml file: conda env export > environment.yml If you need to deactivate the environment, use conda deactivate or conda activate base to go back to the base environment (both have the same result). For packages not on Anaconda's package repo, you may need to install pip before using a pip install command. Once you start using pip , don't go back to using conda to install packages. Do not use pip in the base environment. You'll note that the environment.yml file will specify which packages must be installed using pip .","title":"Virtual environment workflow"},{"location":"development/virtual-environments/#sharing-virtual-environments","text":"Another great thing about virtual environments is that they are easy to share. And, if you're planning to deploy an app on a platform like Heroku, you must specify the environment (by specifying the necessary packages in a requirements.txt file).","title":"Sharing virtual environments"},{"location":"development/virtual-environments/#exporting-a-virtual-environment","text":"Virtual environments are shared with a requirements.txt or environment.yml file. Others can use this to recreate the environment on their own machines. To create this file, make sure the environment is activated and type conda env export > environment.yml or pip freeze > requirements.txt . This file will be saved in the current working directory. Generally, you should save your environment in an environment.yml file since we'll be using conda environments most of the time. Heroku, however, requires a requirements.txt file. It's ok to have both in a project. If you know you will be working on the same operating system (same machine or another machine), you can create an exact replica of the environment using conda list --explicit > environment.yml . This will ensure that all dependencies are of the same version. This likely won't work if you are recreating the environment on a different OS version (e.g., Windows 10 vs. 8.1). If instead you may be working on completely different operating systems (e.g., Windows vs. OSx), you can be extra cautious and use conda env export --from-history > environment.yml . This will only include packages specifically installed into the environment, and allow conda to resolve any dependencies.","title":"Exporting a virtual environment"},{"location":"development/virtual-environments/#duplicate-virtual-environment-from-file","text":"To duplicate a virtual environment from the environment.yml file, use the command conda env create -f environment.yml . This will create the environment with the same name as the original environment. To duplicate a virtual environment from the requirements.txt file, first create an empty conda environment conda create --name <env name> , install pip conda install pip and then install the packages from the file pip install -r requirements.txt .","title":"Duplicate virtual environment from file"},{"location":"development/virtual-environments/#cloning-an-environment","text":"To clone an environment, simply conda create --name <new_env_name> --clone <old_env_name> .","title":"Cloning an environment"},{"location":"development/virtual-environments/#update-a-virtual-environment","text":"You can add packages to the environment simply by using conda install <package> or pip install <package> as you're building the project. Make sure to export the environment after installing new packages so that it's always up-to-date! If changes were made to the environment by a collaborator or on a cloned project, you can install the new packages from the updated environemnt.yml file with the command conda env update --prefix ./env --file environment.yml --prune . The --prune option causes conda to remove old dependencies.","title":"Update a virtual environment"},{"location":"development/virtual-environments/#common-issues","text":"If you are trying to recreate an environment created on another OS platform (e.g., Windows vs. OSx, Windows 10 vs. 8.1) from an environment.yml file, the specific dependencies may not be available on your platform. In this case, go back to the original project and use conda env export --from-history > environment.yml . This will create a file using only the packages specifically imported (excluding any dependencies) and conda will resolve those dependencies on its own. Note that the two environments might not be identical, but it's better than not having the environment at all! For a quick fix, try removing the version number from the packages in the environment.yml or requirements.txt file, and/or deleting packages you know to be dependencies (i.e., numpy is a dependency of pandas , only list pandas in the file).","title":"Common issues"},{"location":"development/virtual-environments/#when-to-create-an-environment","text":"I recommend creating a new environment for any long-lived project using conda create -n <env_name> , saving the name of the environment as the same as the root folder of the project. Install as many packages as possible with conda, then install pip and use pip as the package manager (see here for why). The base conda environment ( conda activate base ) is useful for quick projects or ongoing-development type projects that you don't mind breaking from time to time. Always keep the environments.yml file up-to-date! Anytime you install a package, export the environment. If you code a project without a virtual environment and don't come back to it for a year, there's a good chance you'll find the code doesn't work anymore, or worse yet behaves unexpectedly, and you'll need to go through and update it--which might be more trouble than it's worth. So use virtual environments to save you the hassle and keep the requirements.txt or environment.yml file up to date in the root folder of the project.","title":"When to create an environment"},{"location":"development/virtual-environments/#more-commands","text":"To see your environments: conda env list . To see packages in an environment: conda list -n <env_name> or simply conda list if the environment is active. To remove an environment: conda remove -n <env_name> --all . To update conda: conda update conda . To update a package: conda update <package> . To change python version: conda install python=2.7 (make sure the environment is activated or this will downgrade/upgrade python system-wide!) You can set up environment variables within an environment (although this is not my preferred way of handling secrets ). See here .","title":"More commands"},{"location":"development/virtual-environments/#resources","text":"CONDA CHEATSHEET","title":"Resources"},{"location":"git/additional-reference/","text":"Additional Reference \u00b6 Adding empty folders \u00b6 Empty folders are automatically ignored by git, since it only stores files. Sometimes you want to include an empty folder if you want to commit a folder structure before populating it with files or add a folder for temporarily storing data. To do this you need to add an empty file to the folder. By convention, we can call this .keep or .gitkeep . Create a new text document within the folder you want to keep, leave it empty, and save it as .keep or .gitkeep . If you also want to ignore the contents of this folder, but keep the folder (e.g., for log files or temporary data that isn't deleted), add the following to the .gitignore file: # ignore the files in the folder foo foo /* # but keep the folder !foo/.keep Git fetch \u00b6 git pull is in fact two commands, git fetch and git merge . If you have changes in a remote repository that are conflicting with your local repository git pull can be destructive to your changes. You can use git fetch to bring down the remote changes and git status to see what has changed. You can then use git pull to merge changes after you have reviewed them. Some people have strong opinions about whether git fetch is best practice, or if git pull is just fine. Prune \u00b6 git fetch -p will prune (delete) stale references if you made changes on GitHub that aren't reflected in your local repository, such as deleting a branch. Transfer project \u00b6 Repositories can be transferred from a personal accont to an organization (or vice versa). Under the repo settings, in the 'Danger Zone', select transfer. You might then want to fork the repo back to the original account. TO DO \u00b6 Rebase Organizations & Teams","title":"Additional Reference"},{"location":"git/additional-reference/#additional-reference","text":"","title":"Additional Reference"},{"location":"git/additional-reference/#adding-empty-folders","text":"Empty folders are automatically ignored by git, since it only stores files. Sometimes you want to include an empty folder if you want to commit a folder structure before populating it with files or add a folder for temporarily storing data. To do this you need to add an empty file to the folder. By convention, we can call this .keep or .gitkeep . Create a new text document within the folder you want to keep, leave it empty, and save it as .keep or .gitkeep . If you also want to ignore the contents of this folder, but keep the folder (e.g., for log files or temporary data that isn't deleted), add the following to the .gitignore file: # ignore the files in the folder foo foo /* # but keep the folder !foo/.keep","title":"Adding empty folders"},{"location":"git/additional-reference/#git-fetch","text":"git pull is in fact two commands, git fetch and git merge . If you have changes in a remote repository that are conflicting with your local repository git pull can be destructive to your changes. You can use git fetch to bring down the remote changes and git status to see what has changed. You can then use git pull to merge changes after you have reviewed them. Some people have strong opinions about whether git fetch is best practice, or if git pull is just fine.","title":"Git fetch"},{"location":"git/additional-reference/#prune","text":"git fetch -p will prune (delete) stale references if you made changes on GitHub that aren't reflected in your local repository, such as deleting a branch.","title":"Prune"},{"location":"git/additional-reference/#transfer-project","text":"Repositories can be transferred from a personal accont to an organization (or vice versa). Under the repo settings, in the 'Danger Zone', select transfer. You might then want to fork the repo back to the original account.","title":"Transfer project"},{"location":"git/additional-reference/#to-do","text":"Rebase Organizations & Teams","title":"TO DO"},{"location":"git/collaborating-with-git/","text":"Collaborating with Git \u00b6 There are two common ways of collaborating: Fork the repo and submit pull requests to the repo owner Add collaborators to your repo to give others push authority Fork the repo \u00b6 Forking the repo and submitting pull requests is the safest, as the repo owner is in charge of reviewing all proposed changes before integrating them into the repo. However, that can create a lot of work for the repo owner depending on the frequency of pull requests. Add collaborators \u00b6 Adding collaborators can be done in the Settings tab of a repo (Settings > Collaborators) . This allows anyone listed as a collaborator to work on the repo as if it was their own. This will streamline the workflow, but you risk missing simple mistakes, severe mistakes, and malicious intent. https://kbroman.org/github_tutorial/pages/fork.html Working on someone else's project \u00b6 If you are not a collaborator, you will need to fork the project to your GitHub. Navigate to the repository on GitHub Select 'Fork', which will copy the project to your own repository on GitHub Clone your version of the repo to your computer using git clone <project> in Git Bash (see Initializing Git for help here) Work as you normally would , working on a branch rather than master. Be sure to always pull changes before working on your feature AND before pushing changes. Use git push -u origin <branch_name> to push changes the first time, otherwise git push origin <branch_name> . This will push the changes to your GitHub repository. On GitHub, click the green 'Compare and Pull Request' button to open a pull request. Provide a comment that describes the request. Click 'Create Pull Request'. The target repository will open up in your browser. Update the Pull Request if needed. You can continue adding commits to this pull request by working on this branch, if you want. You may be requested by the project owner to make changes to your code before the pull request is accepted, which can be done by working on this branch. Working as a collaborator \u00b6 As a collaborator, you will be able to work on this project as if it were your own. Clone the project to your computer and push changes as you normally would. Accepting pull requests \u00b6 As the project owner, you'll receive changes as pull requests from others working on your project. GitHub allows you to comment on the request, review the commits, and see files changed. You can even provide in-line code comments when reviewing files changed. GitHub will display whether the pull request can be merged automatically (i.e., there are no conflicting changes). Once you're satisfied with the changes, click 'Confirm Merge' to accept the changes. You can revert the changes until you refresh or navigate away from the page. Delete branches after they have been merged successfully. You can also set up a version of the other repo as a remote connection git remote add <remote_name> <remote location> and simply pull changes git pull <remote_name> master then push back to your repository git push origin master . See more here . Merges \u00b6 When working with others, you'll be merging their changes to your files whenever you use git pull . Git has three flavors of merges. Fast forward merge: when there are no changes in parent branch, the commits are added as if you were working on the parent branch the entire time. Automatic merges: when there are no conflicting changes in parent branch, a new merge commit will be created and you will be asked for a message for this commit. Manual merge: when Git can't resolve conflicting changes it moves into merging state. You must resolve conflicts before proceeding. If you've configured VSCode as your merge tool, you will be prompted to address the conflicting changes one file at a time. Ideally, you'll be communicating with your team members to avoid creating conflicting changes that require manual merges. Replicating virtual environments \u00b6 After forking a repo, you may need to set up a virtual environment to match the environment of the project. The project should contain a requirements.txt or environment.yml file in the root directory ( see here for how to create this file ). This can be used to replicate the environment. After forking the repo, open the prompt and type: conda env create --f environment.yml See the page on virtual environments if you encounter issues or only have a requirements.txt file. Working on two machines \u00b6 If you're simply wanting to work on the same project across two devices, you can easily copy the project to a directory on each device. We recommend keeping this directory out of any other version control software (e.g., Sharepoint or OneDrive). Go to GitHub and select the green 'Code' split button, then copy the HTTPS location of the repo. Navigate to the directory in which you want the project folder stored ( c_dev on my computer), then open a prompt window and clone the project into that directory: git clone <https://> Finally, replicate the virtual environment and get started. Don't forget to always push changes before switching devices and pull changes after! If you update the environment, make sure to freeze it into requirements.txt and push that as well.","title":"Collaborating with Git"},{"location":"git/collaborating-with-git/#collaborating-with-git","text":"There are two common ways of collaborating: Fork the repo and submit pull requests to the repo owner Add collaborators to your repo to give others push authority","title":"Collaborating with Git"},{"location":"git/collaborating-with-git/#fork-the-repo","text":"Forking the repo and submitting pull requests is the safest, as the repo owner is in charge of reviewing all proposed changes before integrating them into the repo. However, that can create a lot of work for the repo owner depending on the frequency of pull requests.","title":"Fork the repo"},{"location":"git/collaborating-with-git/#add-collaborators","text":"Adding collaborators can be done in the Settings tab of a repo (Settings > Collaborators) . This allows anyone listed as a collaborator to work on the repo as if it was their own. This will streamline the workflow, but you risk missing simple mistakes, severe mistakes, and malicious intent. https://kbroman.org/github_tutorial/pages/fork.html","title":"Add collaborators"},{"location":"git/collaborating-with-git/#working-on-someone-elses-project","text":"If you are not a collaborator, you will need to fork the project to your GitHub. Navigate to the repository on GitHub Select 'Fork', which will copy the project to your own repository on GitHub Clone your version of the repo to your computer using git clone <project> in Git Bash (see Initializing Git for help here) Work as you normally would , working on a branch rather than master. Be sure to always pull changes before working on your feature AND before pushing changes. Use git push -u origin <branch_name> to push changes the first time, otherwise git push origin <branch_name> . This will push the changes to your GitHub repository. On GitHub, click the green 'Compare and Pull Request' button to open a pull request. Provide a comment that describes the request. Click 'Create Pull Request'. The target repository will open up in your browser. Update the Pull Request if needed. You can continue adding commits to this pull request by working on this branch, if you want. You may be requested by the project owner to make changes to your code before the pull request is accepted, which can be done by working on this branch.","title":"Working on someone else's project"},{"location":"git/collaborating-with-git/#working-as-a-collaborator","text":"As a collaborator, you will be able to work on this project as if it were your own. Clone the project to your computer and push changes as you normally would.","title":"Working as a collaborator"},{"location":"git/collaborating-with-git/#accepting-pull-requests","text":"As the project owner, you'll receive changes as pull requests from others working on your project. GitHub allows you to comment on the request, review the commits, and see files changed. You can even provide in-line code comments when reviewing files changed. GitHub will display whether the pull request can be merged automatically (i.e., there are no conflicting changes). Once you're satisfied with the changes, click 'Confirm Merge' to accept the changes. You can revert the changes until you refresh or navigate away from the page. Delete branches after they have been merged successfully. You can also set up a version of the other repo as a remote connection git remote add <remote_name> <remote location> and simply pull changes git pull <remote_name> master then push back to your repository git push origin master . See more here .","title":"Accepting pull requests"},{"location":"git/collaborating-with-git/#merges","text":"When working with others, you'll be merging their changes to your files whenever you use git pull . Git has three flavors of merges. Fast forward merge: when there are no changes in parent branch, the commits are added as if you were working on the parent branch the entire time. Automatic merges: when there are no conflicting changes in parent branch, a new merge commit will be created and you will be asked for a message for this commit. Manual merge: when Git can't resolve conflicting changes it moves into merging state. You must resolve conflicts before proceeding. If you've configured VSCode as your merge tool, you will be prompted to address the conflicting changes one file at a time. Ideally, you'll be communicating with your team members to avoid creating conflicting changes that require manual merges.","title":"Merges"},{"location":"git/collaborating-with-git/#replicating-virtual-environments","text":"After forking a repo, you may need to set up a virtual environment to match the environment of the project. The project should contain a requirements.txt or environment.yml file in the root directory ( see here for how to create this file ). This can be used to replicate the environment. After forking the repo, open the prompt and type: conda env create --f environment.yml See the page on virtual environments if you encounter issues or only have a requirements.txt file.","title":"Replicating virtual environments"},{"location":"git/collaborating-with-git/#working-on-two-machines","text":"If you're simply wanting to work on the same project across two devices, you can easily copy the project to a directory on each device. We recommend keeping this directory out of any other version control software (e.g., Sharepoint or OneDrive). Go to GitHub and select the green 'Code' split button, then copy the HTTPS location of the repo. Navigate to the directory in which you want the project folder stored ( c_dev on my computer), then open a prompt window and clone the project into that directory: git clone <https://> Finally, replicate the virtual environment and get started. Don't forget to always push changes before switching devices and pull changes after! If you update the environment, make sure to freeze it into requirements.txt and push that as well.","title":"Working on two machines"},{"location":"git/common-workflow/","text":"Git Workflow \u00b6 Once you work with Git a few times, you'll have this workflow memorized. See the next section for more on Collaborating with Git , and the Additional Reference if you don't find what you need here. Common workflow on master \u00b6 The master branch is the main branch of the project. When you're working on your own project without collaborators or your project is not very complex, it's common to work on master. Otherwise, you'll want to work on a branch . Navigate to root project folder in Windows Explorer. Right-click and select 'Git Bash Here' from context menu. Type git status : commit and push any changes if found (see next steps), but there shouldn't be any since you'll always commit and push all changes at the end of each session. Pull any changes from GitHub with git pull origin master . Work on your feature. You can open, add, and remove files using the Windows Explorer GUI or using unix commands in Git Bash. Commit changes locally: If new files were created, git add -A then git commit -m \"Message\" Otherwise, express commit git commit -am \"Message\" Push to GitHub: git pull origin master (in case others are working on it) git push origin master If you work on multiple features in one session without committing changes, you might want to commit each file one at a time so that your commit messages are associated with just the relevant changes. Use git status to see where you have changes, and use git add <filename> to add only the relevant files to the staging area to commit, commit those changes, and then add the next file or set of files. You can pass in folders (which will include all files within the folder) or globbing patterns (i.e., wildcards) to select multiple files at once. You can wait until all changes are committed to push your changes to GitHub. Messages \u00b6 Git messages typically are written as directives, rather than in past tense, such as 'rename dataframe columns'. Summary \u00b6 Here's that list of commands again: git status git pull origin master # work on your feature git add -A git commit -m \"<message>\" git pull origin master git push origin master Common workflow on branch \u00b6 Now that you're familiar with working on master, we'll quickly summarize the standard workflow for working on a branch. Creating and committing a branch \u00b6 git checkout -b <branch_name> # creates new branch and switches to it # work on your feature git add -A git commit -m \"<message>\" git push -u origin <branch_name> # the -u flag is required only on Working on a branch \u00b6 Use git branch -a to see all branches. git checkout <branch_name> # work on your feature git add -A git commit -m \"<message>\" git push origin <branch_name> Merging to master \u00b6 After you've tested the branch, you will need to merge with master. git checkout master # move from working on branch to master git pull origin master # update master branch git merge <branch_name> # merges branch with master git push origin master git branch -d <branch_name> # delete branch locally git push origin --delete <branch_name> # delete branch remotely Workflow Commands \u00b6 Add files to staging area \u00b6 Use git add <filename> to add a file. Use git add -A to add everything in the working directory. Remove files from the staging area \u00b6 Use git reset HEAD <filename> to remove from the staging area or git reset HEAD to remove everything. Regular commit \u00b6 git commit -m \"Message\" If -m \"Message\" is excluded, the default editor will be opened and a message should be inputted. No need to add quotes or anything else. Line breaks will be ignored. Express commit \u00b6 git commit -am \"Message\" For any files already tracked (use git ls-files if unsure or git status to see what's in the staging area). Ammend last commit message \u00b6 git commit --amend -m \"<ammended commit message\" Rename files \u00b6 git mv <file1> <file2> Use git add -A if changes (renames, deletions) were made outside of Git. Git is smart enough to know if a new file in the staging area is actually a renamed file that was deleted. Delete files (from Git tracking) \u00b6 git rm filename Use git add -u if files were deleted outside of Git Bash. Remove files from Git history \u00b6 If you want to remove a file that has already been committed: Add the file to the .gitignore file Use the command git rm -r --cached <filename> Undo changes \u00b6 Use git checkout -- <filename> to undo the changes made since the last commit. Compare differences \u00b6 Use git diff <filename> to shows recent changes to a file. Use git diff <commit1> <commit2> to compare all changes between two commits (use git hist to git the sha-1 codes for commits). Use HEAD as the second sha-1 hash to compare to the last commit. For a more comprehensive overview, see here \u00b6 Example workflow Other Common Commands \u00b6 Help git <action> --help List files git ls-files Open file with default editor start <file name> Open file with VSCode code <file name> Merges \u00b6 If you encounter a merge issue after pulling from the repo, and it can't be automatically merged, use VSCode (your merge tool) to address any conflicts. While Git is in a merging state, launch your merge tool with git mergetool . Git will open each conflicting file in VSCode one at a time. Conflicting code will be highlighted. Address conflicts, save and close the editor. You can now add the file to the staging area and push all changes to the repo. You might find a new file with .orig extension created by Git after this process. This is the original version of the conflicting file. Delete this file using git rm <filename.orig> . You might also want to add this file extension to your .gitignore file. Tags \u00b6 Git supports tagging to mark major milestones in your repository. Say you're finally ready to deploy, and you want to make sure you know the version of your project that is getting deployed. You could then create a tag each time the code is deployed. Use git tag -a <tag_name> -m \"<tag message>\" to create a tag. For example, you could tag v1.0 of your code using git tag -a v1.0 -m \"Version 1.0 Release\" . To see tags, use git tag --list . You can also see tags in context of your history with git hist . If you want to see the details associated with a specific tag, use git show <tag_name> .","title":"Common Workflow"},{"location":"git/common-workflow/#git-workflow","text":"Once you work with Git a few times, you'll have this workflow memorized. See the next section for more on Collaborating with Git , and the Additional Reference if you don't find what you need here.","title":"Git Workflow"},{"location":"git/common-workflow/#common-workflow-on-master","text":"The master branch is the main branch of the project. When you're working on your own project without collaborators or your project is not very complex, it's common to work on master. Otherwise, you'll want to work on a branch . Navigate to root project folder in Windows Explorer. Right-click and select 'Git Bash Here' from context menu. Type git status : commit and push any changes if found (see next steps), but there shouldn't be any since you'll always commit and push all changes at the end of each session. Pull any changes from GitHub with git pull origin master . Work on your feature. You can open, add, and remove files using the Windows Explorer GUI or using unix commands in Git Bash. Commit changes locally: If new files were created, git add -A then git commit -m \"Message\" Otherwise, express commit git commit -am \"Message\" Push to GitHub: git pull origin master (in case others are working on it) git push origin master If you work on multiple features in one session without committing changes, you might want to commit each file one at a time so that your commit messages are associated with just the relevant changes. Use git status to see where you have changes, and use git add <filename> to add only the relevant files to the staging area to commit, commit those changes, and then add the next file or set of files. You can pass in folders (which will include all files within the folder) or globbing patterns (i.e., wildcards) to select multiple files at once. You can wait until all changes are committed to push your changes to GitHub.","title":"Common workflow on master"},{"location":"git/common-workflow/#messages","text":"Git messages typically are written as directives, rather than in past tense, such as 'rename dataframe columns'.","title":"Messages"},{"location":"git/common-workflow/#summary","text":"Here's that list of commands again: git status git pull origin master # work on your feature git add -A git commit -m \"<message>\" git pull origin master git push origin master","title":"Summary"},{"location":"git/common-workflow/#common-workflow-on-branch","text":"Now that you're familiar with working on master, we'll quickly summarize the standard workflow for working on a branch.","title":"Common workflow on branch"},{"location":"git/common-workflow/#creating-and-committing-a-branch","text":"git checkout -b <branch_name> # creates new branch and switches to it # work on your feature git add -A git commit -m \"<message>\" git push -u origin <branch_name> # the -u flag is required only on","title":"Creating and committing a branch"},{"location":"git/common-workflow/#working-on-a-branch","text":"Use git branch -a to see all branches. git checkout <branch_name> # work on your feature git add -A git commit -m \"<message>\" git push origin <branch_name>","title":"Working on a branch"},{"location":"git/common-workflow/#merging-to-master","text":"After you've tested the branch, you will need to merge with master. git checkout master # move from working on branch to master git pull origin master # update master branch git merge <branch_name> # merges branch with master git push origin master git branch -d <branch_name> # delete branch locally git push origin --delete <branch_name> # delete branch remotely","title":"Merging to master"},{"location":"git/common-workflow/#workflow-commands","text":"","title":"Workflow Commands"},{"location":"git/common-workflow/#add-files-to-staging-area","text":"Use git add <filename> to add a file. Use git add -A to add everything in the working directory.","title":"Add files to staging area"},{"location":"git/common-workflow/#remove-files-from-the-staging-area","text":"Use git reset HEAD <filename> to remove from the staging area or git reset HEAD to remove everything.","title":"Remove files from the staging area"},{"location":"git/common-workflow/#regular-commit","text":"git commit -m \"Message\" If -m \"Message\" is excluded, the default editor will be opened and a message should be inputted. No need to add quotes or anything else. Line breaks will be ignored.","title":"Regular commit"},{"location":"git/common-workflow/#express-commit","text":"git commit -am \"Message\" For any files already tracked (use git ls-files if unsure or git status to see what's in the staging area).","title":"Express commit"},{"location":"git/common-workflow/#ammend-last-commit-message","text":"git commit --amend -m \"<ammended commit message\"","title":"Ammend last commit message"},{"location":"git/common-workflow/#rename-files","text":"git mv <file1> <file2> Use git add -A if changes (renames, deletions) were made outside of Git. Git is smart enough to know if a new file in the staging area is actually a renamed file that was deleted.","title":"Rename files"},{"location":"git/common-workflow/#delete-files-from-git-tracking","text":"git rm filename Use git add -u if files were deleted outside of Git Bash.","title":"Delete files (from Git tracking)"},{"location":"git/common-workflow/#remove-files-from-git-history","text":"If you want to remove a file that has already been committed: Add the file to the .gitignore file Use the command git rm -r --cached <filename>","title":"Remove files from Git history"},{"location":"git/common-workflow/#undo-changes","text":"Use git checkout -- <filename> to undo the changes made since the last commit.","title":"Undo changes"},{"location":"git/common-workflow/#compare-differences","text":"Use git diff <filename> to shows recent changes to a file. Use git diff <commit1> <commit2> to compare all changes between two commits (use git hist to git the sha-1 codes for commits). Use HEAD as the second sha-1 hash to compare to the last commit.","title":"Compare differences"},{"location":"git/common-workflow/#for-a-more-comprehensive-overview-see-here","text":"Example workflow","title":"For a more comprehensive overview, see here"},{"location":"git/common-workflow/#other-common-commands","text":"Help git <action> --help List files git ls-files Open file with default editor start <file name> Open file with VSCode code <file name>","title":"Other Common Commands"},{"location":"git/common-workflow/#merges","text":"If you encounter a merge issue after pulling from the repo, and it can't be automatically merged, use VSCode (your merge tool) to address any conflicts. While Git is in a merging state, launch your merge tool with git mergetool . Git will open each conflicting file in VSCode one at a time. Conflicting code will be highlighted. Address conflicts, save and close the editor. You can now add the file to the staging area and push all changes to the repo. You might find a new file with .orig extension created by Git after this process. This is the original version of the conflicting file. Delete this file using git rm <filename.orig> . You might also want to add this file extension to your .gitignore file.","title":"Merges"},{"location":"git/common-workflow/#tags","text":"Git supports tagging to mark major milestones in your repository. Say you're finally ready to deploy, and you want to make sure you know the version of your project that is getting deployed. You could then create a tag each time the code is deployed. Use git tag -a <tag_name> -m \"<tag message>\" to create a tag. For example, you could tag v1.0 of your code using git tag -a v1.0 -m \"Version 1.0 Release\" . To see tags, use git tag --list . You can also see tags in context of your history with git hist . If you want to see the details associated with a specific tag, use git show <tag_name> .","title":"Tags"},{"location":"git/initializing-git/","text":"This section describes how to begin a new project and track it on Github. We have two options: Create the project locally and push it up to Github* Create the project on Github first and clone it down to your computer *If you already have a project started, you've already started locally. Starting Locally \u00b6 If you have already started a project or prefer to work from your local machine first, follow these steps: 1. Create the project directory \u00b6 In Windows Explorer, navigate to the folder in which you will create your project directory (or the root of an existing project directory). Right-click and select 'GitBash Here' to open an instance of Bash. Skip the following paragraph if you already have a project started . Use the command mkdir <project-name> (replace <project-name> with the name of the directory for the project) to create a folder. Next, use the command cd <project-name> to move into that folder. 2. Initialize git \u00b6 Use the command git init to initialize git. A new repository named .git will be created within the directory which stores all of the git version control. You don't need to have GitBash open during your editing sessions, this folder will take care of all of your version control. If for some reason you want to stop tracking with git and lose all of your previous versions, just delete this folder. 3. Create a .gitignore and README.md file \u00b6 Use the command touch .gitignore to create a new file named .gitignore. This file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. Use the command touch README.md to create a README file. This file will automatically be displayed on the repository page on Github. Using the Markdown language, populate this README file with important info as necessary. 4. Commit your initial file(s) \u00b6 First, we'll need to add all files to be tracked by git. Use the command: git add -A Next we'll use a commit to get the first batch of files/folders into our git repository. Use the following command: git commit -m \"Initial Commit\" You will learn more about commits and other features of git in the Using Git section. 5. Create a Github repository \u00b6 Navigate to your Github.com page and login. Click the green Create New Repository button. Name it with the same name you used in step 1 for the project directory. Provide a description. The repository will be public. DO NOT create a README or .gitignore file. After you click 'Ok', Github will provide instructions for importing your project files into the repository. Copy the url provided to your clipboard. You'll use two commands in GitBash to accomplish this: git remote add origin <url> git push -u origin master --tags The first line establishes a remote connection to the repository. Replace the url with the url provided by github to your project repository. Note that you can change the name 'origin' to anything you'd like, but origin is used by convention. You will be prompted to sign into your Github account. The second line 'pushes' your files/folders up to Github. The -u flag tells Github to track these files/folders along with master, while the --tags flag will move any messages from previous commits. You will be prompted for your username and password when you use git push (see other authentication options here ). If this fails, it may be because you already have a remote established named 'origin'. Try git remove rm origin to remove any existing connections, or change the name origin to something unique. To confirm your remote connection was established, use the command git remote -v . 6. Create your project structure and begin coding! \u00b6 At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work. Starting Remote \u00b6 In this option, we'll start by creating a repository on Github and then clone that down to our computer. You can copy files/folders into this newly created repository if you'd like. 1. Create a Gihub repository \u00b6 Navigate to your Github.com page and login. Click the green Create New Repository button. Provide a description. The repository will be public. Create a README and .gitignore file. Optionally add a license. 2. Clone the remote repository \u00b6 Within the project repository on Github, click the 'Clone or Download' button. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. In Windows Explorer, navigate to the folder in which you will create your project directory. Right-click and select 'GitBash Here' to open an instance of Bash. Clone the repository into this folder with the command: git clone <url> where you replace <url> with the copied url (use Shift+Insert to paste into Bash). You may provide a name for the project following the url if you'd like the folder on your local drive to have a different name than the repository on Github. Run ls to confirm that the repository was created. Type the command cd <repository-name> to move into the repository folder (replace <repository name> with the correct name). If you need to copy files/folders into the repository, use the command cp -R <~/path/> . . Replace <~/path/> with the correct path of the source folder/file. The -R flag signifies recursive and copies everything from within the folders as well. The . simply means the current directory, so your copying everything from the path provided to the current directory. 3. Populate the .gitignore file \u00b6 The .gitignore file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. 4. Create your project structure and begin coding! \u00b6 At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work. Additional Resources \u00b6 Corey Shafer's excellent series on Git","title":"Create Project"},{"location":"git/initializing-git/#starting-locally","text":"If you have already started a project or prefer to work from your local machine first, follow these steps:","title":"Starting Locally"},{"location":"git/initializing-git/#1-create-the-project-directory","text":"In Windows Explorer, navigate to the folder in which you will create your project directory (or the root of an existing project directory). Right-click and select 'GitBash Here' to open an instance of Bash. Skip the following paragraph if you already have a project started . Use the command mkdir <project-name> (replace <project-name> with the name of the directory for the project) to create a folder. Next, use the command cd <project-name> to move into that folder.","title":"1. Create the project directory"},{"location":"git/initializing-git/#2-initialize-git","text":"Use the command git init to initialize git. A new repository named .git will be created within the directory which stores all of the git version control. You don't need to have GitBash open during your editing sessions, this folder will take care of all of your version control. If for some reason you want to stop tracking with git and lose all of your previous versions, just delete this folder.","title":"2. Initialize git"},{"location":"git/initializing-git/#3-create-a-gitignore-and-readmemd-file","text":"Use the command touch .gitignore to create a new file named .gitignore. This file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. Use the command touch README.md to create a README file. This file will automatically be displayed on the repository page on Github. Using the Markdown language, populate this README file with important info as necessary.","title":"3. Create a .gitignore and README.md file"},{"location":"git/initializing-git/#4-commit-your-initial-files","text":"First, we'll need to add all files to be tracked by git. Use the command: git add -A Next we'll use a commit to get the first batch of files/folders into our git repository. Use the following command: git commit -m \"Initial Commit\" You will learn more about commits and other features of git in the Using Git section.","title":"4. Commit your initial file(s)"},{"location":"git/initializing-git/#5-create-a-github-repository","text":"Navigate to your Github.com page and login. Click the green Create New Repository button. Name it with the same name you used in step 1 for the project directory. Provide a description. The repository will be public. DO NOT create a README or .gitignore file. After you click 'Ok', Github will provide instructions for importing your project files into the repository. Copy the url provided to your clipboard. You'll use two commands in GitBash to accomplish this: git remote add origin <url> git push -u origin master --tags The first line establishes a remote connection to the repository. Replace the url with the url provided by github to your project repository. Note that you can change the name 'origin' to anything you'd like, but origin is used by convention. You will be prompted to sign into your Github account. The second line 'pushes' your files/folders up to Github. The -u flag tells Github to track these files/folders along with master, while the --tags flag will move any messages from previous commits. You will be prompted for your username and password when you use git push (see other authentication options here ). If this fails, it may be because you already have a remote established named 'origin'. Try git remove rm origin to remove any existing connections, or change the name origin to something unique. To confirm your remote connection was established, use the command git remote -v .","title":"5. Create a Github repository"},{"location":"git/initializing-git/#6-create-your-project-structure-and-begin-coding","text":"At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work.","title":"6. Create your project structure and begin coding!"},{"location":"git/initializing-git/#starting-remote","text":"In this option, we'll start by creating a repository on Github and then clone that down to our computer. You can copy files/folders into this newly created repository if you'd like.","title":"Starting Remote"},{"location":"git/initializing-git/#1-create-a-gihub-repository","text":"Navigate to your Github.com page and login. Click the green Create New Repository button. Provide a description. The repository will be public. Create a README and .gitignore file. Optionally add a license.","title":"1. Create a Gihub repository"},{"location":"git/initializing-git/#2-clone-the-remote-repository","text":"Within the project repository on Github, click the 'Clone or Download' button. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. In Windows Explorer, navigate to the folder in which you will create your project directory. Right-click and select 'GitBash Here' to open an instance of Bash. Clone the repository into this folder with the command: git clone <url> where you replace <url> with the copied url (use Shift+Insert to paste into Bash). You may provide a name for the project following the url if you'd like the folder on your local drive to have a different name than the repository on Github. Run ls to confirm that the repository was created. Type the command cd <repository-name> to move into the repository folder (replace <repository name> with the correct name). If you need to copy files/folders into the repository, use the command cp -R <~/path/> . . Replace <~/path/> with the correct path of the source folder/file. The -R flag signifies recursive and copies everything from within the folders as well. The . simply means the current directory, so your copying everything from the path provided to the current directory.","title":"2. Clone the remote repository"},{"location":"git/initializing-git/#3-populate-the-gitignore-file","text":"The .gitignore file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore.","title":"3. Populate the .gitignore file"},{"location":"git/initializing-git/#4-create-your-project-structure-and-begin-coding","text":"At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work.","title":"4. Create your project structure and begin coding!"},{"location":"git/initializing-git/#additional-resources","text":"Corey Shafer's excellent series on Git","title":"Additional Resources"},{"location":"git/install-configure-git/","text":"Installing & Configuring Git \u00b6 Git is version control software for code. When paired with Github, Git is a convenient way to collaborate on coding projects. This series will help you get comfortable with the basic workflow and commands, and includes a helpful [reference] for when you get stuck. Download & Install Git \u00b6 Go to git-scm.com to download Git. Run the installer. I recommend installing only Git Bash in the context menu. I also recommend checking the option to ' Use Git from the Windows Command Prompt '. Open Git Bash (right-click to bring up the context menu or find it in your programs folder) and type git --version to make sure it's successfully installed. Configure Git \u00b6 To get the most from Git, you can configure a few of its global setings. In Git Bash, type the following commands: git config --global user.name <name> git config --global user.email <email> Replace <name> and <email> with your name and email. Your name and email will be associated with any commits you make (i.e., when you save and publish edits to code). Use git config --list to see all settings available to you. Git Log \u00b6 The log is helpful when you need to see the history of changes made or roll back to a previous change. These commands help you navigate the logs. git log | all commits git show | last commit git ls-files | lists all files that git is tracking git log --oneline --graph --decorate --all |more detailed history of commits Create an alias for the git history command \u00b6 We'll customize the hist command to provide more information when we use it. You can use this pattern to alias other commands as well. In Git Bash, type: git config --global alias.hist \"log --oneline --graph --decorate --all\" Now use git hist to see the same command, note it still accepts additional arguments (for example, use git hist <filename> to see history for one file). History will be served line by line, type q to quit at any time. Setup your preferred editor \u00b6 You must provide a message describing your changes when you commit. You can usually type this directly into Git Bash, but sometimes you'll want to provide a lengthier message. You can set up your default editor for commit messages. This will set up VSCode as the default editor (note that --wait is only required for VSCode, and other editors may not require that flag). git config --global core.editor \"code --wait\" Type git config --global -e to confirm this worked. VSCode should open and display the contents of Git's configuration file. Info You can open files with the command start <filepath> using your system's default editor for that filetype. If you want to open a project in VSCode from the command line (or Git Bash), type code . , where the period represents the current folder. This will open VSCode within the context of the project's folder. See the documentation for other ways you can open files/folder from the command line. Setup Diff & Merge Tool \u00b6 When there are conflicting changes to a file, you'll need to tell Git which changes to keep using Diff and Merge tools. Use the command git config --global -e to open Git's global configuration file and paste the following into it: [merge] tool = vscode [mergetool \"vscode\"] cmd = code --wait $MERGED [diff] tool = vscode [difftool \"vscode\"] cmd = code --wait --diff $LOCAL $REMOTE Info You can also use VSCode to compare two files using the command code --diff <filepath1> <filepath2> . This option is available from within VSCode through the context menu as well. Connect to GitHub \u00b6 You'll want to set up an account on GitHub . You can either create a single profile for both your personal and professional projects, or you can set up a profile specifically for your professional projects. You will also want to join the Environmental Incentives organization. Once you're set up, let someone on the Metrics team know your GitHub user name and we'll send you an invite. The next section will walk through connecting your local Git repository to GitHub.","title":"Install Git"},{"location":"git/install-configure-git/#installing-configuring-git","text":"Git is version control software for code. When paired with Github, Git is a convenient way to collaborate on coding projects. This series will help you get comfortable with the basic workflow and commands, and includes a helpful [reference] for when you get stuck.","title":"Installing &amp; Configuring Git"},{"location":"git/install-configure-git/#download-install-git","text":"Go to git-scm.com to download Git. Run the installer. I recommend installing only Git Bash in the context menu. I also recommend checking the option to ' Use Git from the Windows Command Prompt '. Open Git Bash (right-click to bring up the context menu or find it in your programs folder) and type git --version to make sure it's successfully installed.","title":"Download &amp; Install Git"},{"location":"git/install-configure-git/#configure-git","text":"To get the most from Git, you can configure a few of its global setings. In Git Bash, type the following commands: git config --global user.name <name> git config --global user.email <email> Replace <name> and <email> with your name and email. Your name and email will be associated with any commits you make (i.e., when you save and publish edits to code). Use git config --list to see all settings available to you.","title":"Configure Git"},{"location":"git/install-configure-git/#git-log","text":"The log is helpful when you need to see the history of changes made or roll back to a previous change. These commands help you navigate the logs. git log | all commits git show | last commit git ls-files | lists all files that git is tracking git log --oneline --graph --decorate --all |more detailed history of commits","title":"Git Log"},{"location":"git/install-configure-git/#create-an-alias-for-the-git-history-command","text":"We'll customize the hist command to provide more information when we use it. You can use this pattern to alias other commands as well. In Git Bash, type: git config --global alias.hist \"log --oneline --graph --decorate --all\" Now use git hist to see the same command, note it still accepts additional arguments (for example, use git hist <filename> to see history for one file). History will be served line by line, type q to quit at any time.","title":"Create an alias for the git history command"},{"location":"git/install-configure-git/#setup-your-preferred-editor","text":"You must provide a message describing your changes when you commit. You can usually type this directly into Git Bash, but sometimes you'll want to provide a lengthier message. You can set up your default editor for commit messages. This will set up VSCode as the default editor (note that --wait is only required for VSCode, and other editors may not require that flag). git config --global core.editor \"code --wait\" Type git config --global -e to confirm this worked. VSCode should open and display the contents of Git's configuration file. Info You can open files with the command start <filepath> using your system's default editor for that filetype. If you want to open a project in VSCode from the command line (or Git Bash), type code . , where the period represents the current folder. This will open VSCode within the context of the project's folder. See the documentation for other ways you can open files/folder from the command line.","title":"Setup your preferred editor"},{"location":"git/install-configure-git/#setup-diff-merge-tool","text":"When there are conflicting changes to a file, you'll need to tell Git which changes to keep using Diff and Merge tools. Use the command git config --global -e to open Git's global configuration file and paste the following into it: [merge] tool = vscode [mergetool \"vscode\"] cmd = code --wait $MERGED [diff] tool = vscode [difftool \"vscode\"] cmd = code --wait --diff $LOCAL $REMOTE Info You can also use VSCode to compare two files using the command code --diff <filepath1> <filepath2> . This option is available from within VSCode through the context menu as well.","title":"Setup Diff &amp; Merge Tool"},{"location":"git/install-configure-git/#connect-to-github","text":"You'll want to set up an account on GitHub . You can either create a single profile for both your personal and professional projects, or you can set up a profile specifically for your professional projects. You will also want to join the Environmental Incentives organization. Once you're set up, let someone on the Metrics team know your GitHub user name and we'll send you an invite. The next section will walk through connecting your local Git repository to GitHub.","title":"Connect to GitHub"},{"location":"guides/dash/","text":"Dash \u00b6 Dash is a Python framework for building analytic web applications. As it's name suggests, it's especially good for building interactive dashboards. The strengths of this package lie in how it bridges the gaps between your app's analytic functionality, the web, and your user. Dash obviously integrates with Python and all of its functionality, but also provides a fairly intuitive interface to interact with the DOM (more on that below) and includes user interface components (i.e., widgets ) that invite interactivity. I recommend taking a day to work through the examples in the User Guide and exploring what's possible in the gallery . Once you've done that and built your first app, you can come back here for a quick refresher. Also worth noting that Dash 1.0 is relatively new, and a lot of the guidance you'll find in forums and on YouTube is outdated, so start with the official documentation and the very helpful Dash community . The DOM DOM is short for document-object-model. The DOM is how web browsers display information on web pages. The DOM is represented by HTML (Hyper-Text Markup Language). Having a decent grasp on HTML will help you as you customize your Dash app. Check out w3schools or codeacademy for a crash course. You don't need to know everything, but it's helpful to know how Dash is converting your Python code to HTML, and the options you have through Dash's HTML components . Installation \u00b6 Dash isn't distributed through conda (yet), so you'll need to install using pip . Might as well initialize a git repository and create a virtual environment while you're at it. Use virtualenv si 0nce this is a pip environment and that's what the Dash documentation suggests. Make sure your prompt is in the root folder of your project directory (or mkdir and cd into it). virtualenv venv source venv / bin / activate pip install dash == 1 . 8 . 0 # check the docs for the latest version pip install plotly Here are the files they recommend including in your .gitignore file. venv * .pyc .DS_Store .env Patterns \u00b6 A Dash app consists of a layout, which describes the organization of the app, and callbacks, which create interactive functionality. Layout \u00b6 The Dash layout consists of both Core Components and HTML Components . Core components represent your user interface widgets. HTML components expose the HTML tags used in the DOM as Python classes. Here's a basic template layout to get you started. # -*- coding: utf-8 -*- import dash import dash_core_components as dcc import dash_html_components as html external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ( children = [ # YOUR APP GOES HERE ]) if __name__ == '__main__' : app . run_server ( debug = True ) Within the app.layout variable, you'll define all of your core components and html components within a div HTML element as a list. app.layout essentially defines the body of the DOM. Everything you want on your webpage must be included here. The external stylesheet provides styling through CSS to make your app look a bit better. You can also provide custom CSS and custom JavaScript . Core Components \u00b6 Dash's core components facilitate user interaction and include dropdowns, sliders, a variety of inputs, checkboxes, radio items, buttons, date pickers, file uploaders, tabs, dialogs, and graphs (i.e., charts). The tables component , recently released, allows for interacting with and even editing tables. As well, you can pass Markdown through the Markdown component. There are a few additional libraries of components available, check out the list under Open Source Component Library (I recommend checking out Dash DAQ library). Each component also has an array of properties that can be set (or updated through callbacks), refer to each component's documentation. HTML Components \u00b6 The HTML components provide all of the other functionality you'll need to create a web page by exposing a majority of the available html tags. You'll most often be using html.Div to create areas within the web page to show widgets, content, and even to store information without showing it. Styling \u00b6 The CSS provided by the external stylesheet offers some basic styling and a grid feature. Check out the codepen to see the documentation and example styles available to you. To use the CSS, you need to assign a class to the component. You can do this by including a variable className in the list of component properties. Convert any dots ( . ) in the CSS to spaces in your code. For example, a primary button ( primary.button ) is created with: html . Button ( children = 'Click me!' , id = 'my_button' , className = 'primary button' ) Grids \u00b6 To create side-by-side elements, access the grid. First, create a div where you will include the side-by-side elements. Assign to each component the number of 'columns' you want the component to occupy out of a possible twelve. Assign to the div element the class name of 'row'. This example would create two side-by-side buttons, each occupying one half of the screen: html.Div( className='row', children=[ html.Button( className='six columns', id='submit-button', children='Submit' ), html.Button( className='six columns', id='undo-button', children='Undo' ) ] Bootstrap \u00b6 Another stylesheet makes available Bootstrap (a popular user interface library for the web), called dash-bootstrap-components . This stylesheet changes the development patterns with new syntax for the layout, so decide if you want to use Bootstrap before getting too far in development. Callbacks \u00b6 Callbacks handle the interactivity of the app. Callbacks listen for events, like clicking on a button, and then handle the action required to make the button do it's job. You can update almost any property of any component through a callback. For example, you might want to update the figure property of a dcc.Graph component based on the value of a slider or other input. In other words, you will change the way the graph looks whenever the user interacts with the relevant widget. To create a callback, import Input and Output from dash.dependencies , use the callback decorator, provide one or more outputs and one or more inputs, and then define a function that handles the interaction. from dash.dependencies import Input , Output # Put callbacks below the layout or in a separate file @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): return first_input ^ 2 If you have more than one output, they should be passed as a list. The inputs need to be included in a list, regardless of how many there are. The inputs are passed to the function in the order they are listed. Two parameters are passed to the Output or Input , first the id of the component and second the property of interest. Thus, any component that supports interactivity must have an id property assigned (also, all components in your callbacks must also be represented in the layout, or you'll get an error). Callbacks can also be chained to create a cascade of changes to create dynamic UIs where one input component updates the available options of another input component. Provide the Output of one callback as the Input of another callback. Note that you won't provide the value for the Output components you will be updating within app.layout . This is because Dash automatically fires all of the callbacks upon page load, so anything you provide up front will simply be overwritten. For example, if you have a callback that updates the figure of a dcc.Graph component, don't include a figure= property for that graph in the app.layout section, you only need provide it an id= property. Warning Don't update variables that are outside of the callback's scope, as this can introduce unusual behavior. State \u00b6 Callbacks fire whenever there is a change to the input component(s). You might instead want to, say, wait for the user to finish updating a form before you fire the callbacks associated with those input fields. For this, you can use a State variable. Include State variables as you would Input variables, in a list below the Input variables. from dash.dependencies import Input , Output , State @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )], [ State ( 'my-input-state' , 'value' )] ) def my_function ( first_input , state_input ): return first_input ^ state_input This example raises the value of the my-input component by the my-input-state component, which allows the user to specify the power to raise to without re-calculating all of the outputs. The result is used to update the children property of the my-output component (children properties are common for text outputs, so this would print the result to the screen). PreventUpdate & NoUpdate \u00b6 If you want to include logic that prevents updating a component based on the property of some input, you can use PreventUpdate . from dash.exceptions import PreventUpdate @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): if first_input is None : raise PreventUpdate else : return first_input ^ 2 Here, if the input is not filled in, the output component will not be updated. Alternatively, you can use dash.no_update to tell Dash to skip updating a specific output based on an input value. For example, if you want to update one output, but not another, based on an input, return dash.no_update for that output. Interactive Graphing \u00b6 One really great feature of Dash is its ability to update multiple graphs based on the built-in interactivity of Plotly figures. Plotly figures support hovering, clicking, selecting, and zooming on charts and their data. Using Dash, you can update any component based on the user's interaction with the graph. This is probably best illustrated. Note the charts on the right are updated based on where the cursor hovers. See the documentation for details. Running the app \u00b6 Run the app in the terminal with: python < app . py > where <app.py> is the path to your python script containing the application. By custom, this is called app.py . Visit localhost:8050 in your browser (or whichever port the terminal states). Deployment \u00b6 While Dash does provide a paid hosting platform, it's better to host on Heroku. Warning Don't deploy with Debug set to True (In app.py, set app.run_server(debug=False)) Heroku \u00b6 Check out our Heroku tutorial for general guidance and the step-by-step tutorial in the official documentation (under Heroku example ). There are a few configuration options and gotchas specific to Dash you'll need to know about that are covered in the tutorial. Flask \u00b6 Dash is written on top of flask , and so integrates fairly easily with Flask applications. Deploying your Flask application is exactly the same as deploying your Dash application. Embedding in existing websites \u00b6 Dash apps (and any other apps hosted through Heroku) can be inserted into a webpage using the iframe tag. < iframe src = \"<path-to-my-app.com>\" , height = 800, width = 700 ></ iframe > Note that with the free tier, the page may timeout before the app is spun up and loaded. You'll need to reload the site, which isn't the best user experience, so consider upgrading any app that you want to embed in this way. Tips & Tricks \u00b6 Storing data & performance \u00b6 If your app includes expensive analytics, you may want to separate that process from the interactivity of your app to avoid expensive computations with every change of inputs. One option is to store the data in a hidden div . This will store the data in the user's browser for the duration of their session. Data has to be converted to a string like JSON for storage. You can either store the fully computed data or you can perform some computation, save the intermediate results, and then with a chained callback access that information to complete the interaction. The pre-processed data will be available to subsequent callbacks where you stored it. Finally, you can cache the data using flask . This will also allow for persisting the data between sessions. Once you're ready to worry about optimizing performance, see the docs . Multi-page Apps \u00b6 If you need basic functionality for multiple pages, check out the Tabs core component. If you really need a multi-page app, you can either embed multiple Dash apps in a Django or Flask app, or check out this guidance . I'd also recommend this helpful Towards Data Science article. Extending \u00b6 You can write your own Dash components in React JS, a popular JavaScript library for building user interfaces. You'll need to fork the sample component library and, of course, be comfortable working with React JS. Custom CSS \u00b6 To add your own CSS, save the CSS files in a folder named assets in the project's root directory. Dash will automatically serve any CSS it finds here. Note that the CSS files are added in alphanumeric order, so if the order matters, prepend numeric values to the filenames to get the loading order you want. You can also copy the provided CSS into this folder and edit it. Custom JavaScript \u00b6 To add custom JavaScript, simply include a JavaScript file in the assets folder in the project's root directory. Dash will automatically include it. However, they recommend building new Dash components for any significant functionality you want to add.","title":"Dash"},{"location":"guides/dash/#dash","text":"Dash is a Python framework for building analytic web applications. As it's name suggests, it's especially good for building interactive dashboards. The strengths of this package lie in how it bridges the gaps between your app's analytic functionality, the web, and your user. Dash obviously integrates with Python and all of its functionality, but also provides a fairly intuitive interface to interact with the DOM (more on that below) and includes user interface components (i.e., widgets ) that invite interactivity. I recommend taking a day to work through the examples in the User Guide and exploring what's possible in the gallery . Once you've done that and built your first app, you can come back here for a quick refresher. Also worth noting that Dash 1.0 is relatively new, and a lot of the guidance you'll find in forums and on YouTube is outdated, so start with the official documentation and the very helpful Dash community . The DOM DOM is short for document-object-model. The DOM is how web browsers display information on web pages. The DOM is represented by HTML (Hyper-Text Markup Language). Having a decent grasp on HTML will help you as you customize your Dash app. Check out w3schools or codeacademy for a crash course. You don't need to know everything, but it's helpful to know how Dash is converting your Python code to HTML, and the options you have through Dash's HTML components .","title":"Dash"},{"location":"guides/dash/#installation","text":"Dash isn't distributed through conda (yet), so you'll need to install using pip . Might as well initialize a git repository and create a virtual environment while you're at it. Use virtualenv si 0nce this is a pip environment and that's what the Dash documentation suggests. Make sure your prompt is in the root folder of your project directory (or mkdir and cd into it). virtualenv venv source venv / bin / activate pip install dash == 1 . 8 . 0 # check the docs for the latest version pip install plotly Here are the files they recommend including in your .gitignore file. venv * .pyc .DS_Store .env","title":"Installation"},{"location":"guides/dash/#patterns","text":"A Dash app consists of a layout, which describes the organization of the app, and callbacks, which create interactive functionality.","title":"Patterns"},{"location":"guides/dash/#layout","text":"The Dash layout consists of both Core Components and HTML Components . Core components represent your user interface widgets. HTML components expose the HTML tags used in the DOM as Python classes. Here's a basic template layout to get you started. # -*- coding: utf-8 -*- import dash import dash_core_components as dcc import dash_html_components as html external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ( children = [ # YOUR APP GOES HERE ]) if __name__ == '__main__' : app . run_server ( debug = True ) Within the app.layout variable, you'll define all of your core components and html components within a div HTML element as a list. app.layout essentially defines the body of the DOM. Everything you want on your webpage must be included here. The external stylesheet provides styling through CSS to make your app look a bit better. You can also provide custom CSS and custom JavaScript .","title":"Layout"},{"location":"guides/dash/#core-components","text":"Dash's core components facilitate user interaction and include dropdowns, sliders, a variety of inputs, checkboxes, radio items, buttons, date pickers, file uploaders, tabs, dialogs, and graphs (i.e., charts). The tables component , recently released, allows for interacting with and even editing tables. As well, you can pass Markdown through the Markdown component. There are a few additional libraries of components available, check out the list under Open Source Component Library (I recommend checking out Dash DAQ library). Each component also has an array of properties that can be set (or updated through callbacks), refer to each component's documentation.","title":"Core Components"},{"location":"guides/dash/#html-components","text":"The HTML components provide all of the other functionality you'll need to create a web page by exposing a majority of the available html tags. You'll most often be using html.Div to create areas within the web page to show widgets, content, and even to store information without showing it.","title":"HTML Components"},{"location":"guides/dash/#styling","text":"The CSS provided by the external stylesheet offers some basic styling and a grid feature. Check out the codepen to see the documentation and example styles available to you. To use the CSS, you need to assign a class to the component. You can do this by including a variable className in the list of component properties. Convert any dots ( . ) in the CSS to spaces in your code. For example, a primary button ( primary.button ) is created with: html . Button ( children = 'Click me!' , id = 'my_button' , className = 'primary button' )","title":"Styling"},{"location":"guides/dash/#grids","text":"To create side-by-side elements, access the grid. First, create a div where you will include the side-by-side elements. Assign to each component the number of 'columns' you want the component to occupy out of a possible twelve. Assign to the div element the class name of 'row'. This example would create two side-by-side buttons, each occupying one half of the screen: html.Div( className='row', children=[ html.Button( className='six columns', id='submit-button', children='Submit' ), html.Button( className='six columns', id='undo-button', children='Undo' ) ]","title":"Grids"},{"location":"guides/dash/#bootstrap","text":"Another stylesheet makes available Bootstrap (a popular user interface library for the web), called dash-bootstrap-components . This stylesheet changes the development patterns with new syntax for the layout, so decide if you want to use Bootstrap before getting too far in development.","title":"Bootstrap"},{"location":"guides/dash/#callbacks","text":"Callbacks handle the interactivity of the app. Callbacks listen for events, like clicking on a button, and then handle the action required to make the button do it's job. You can update almost any property of any component through a callback. For example, you might want to update the figure property of a dcc.Graph component based on the value of a slider or other input. In other words, you will change the way the graph looks whenever the user interacts with the relevant widget. To create a callback, import Input and Output from dash.dependencies , use the callback decorator, provide one or more outputs and one or more inputs, and then define a function that handles the interaction. from dash.dependencies import Input , Output # Put callbacks below the layout or in a separate file @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): return first_input ^ 2 If you have more than one output, they should be passed as a list. The inputs need to be included in a list, regardless of how many there are. The inputs are passed to the function in the order they are listed. Two parameters are passed to the Output or Input , first the id of the component and second the property of interest. Thus, any component that supports interactivity must have an id property assigned (also, all components in your callbacks must also be represented in the layout, or you'll get an error). Callbacks can also be chained to create a cascade of changes to create dynamic UIs where one input component updates the available options of another input component. Provide the Output of one callback as the Input of another callback. Note that you won't provide the value for the Output components you will be updating within app.layout . This is because Dash automatically fires all of the callbacks upon page load, so anything you provide up front will simply be overwritten. For example, if you have a callback that updates the figure of a dcc.Graph component, don't include a figure= property for that graph in the app.layout section, you only need provide it an id= property. Warning Don't update variables that are outside of the callback's scope, as this can introduce unusual behavior.","title":"Callbacks"},{"location":"guides/dash/#state","text":"Callbacks fire whenever there is a change to the input component(s). You might instead want to, say, wait for the user to finish updating a form before you fire the callbacks associated with those input fields. For this, you can use a State variable. Include State variables as you would Input variables, in a list below the Input variables. from dash.dependencies import Input , Output , State @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )], [ State ( 'my-input-state' , 'value' )] ) def my_function ( first_input , state_input ): return first_input ^ state_input This example raises the value of the my-input component by the my-input-state component, which allows the user to specify the power to raise to without re-calculating all of the outputs. The result is used to update the children property of the my-output component (children properties are common for text outputs, so this would print the result to the screen).","title":"State"},{"location":"guides/dash/#preventupdate-noupdate","text":"If you want to include logic that prevents updating a component based on the property of some input, you can use PreventUpdate . from dash.exceptions import PreventUpdate @app . callback ( Output ( 'my-output' , 'children' ), [ Input ( 'my-input' , 'value' )] ) def my_function ( first_input ): if first_input is None : raise PreventUpdate else : return first_input ^ 2 Here, if the input is not filled in, the output component will not be updated. Alternatively, you can use dash.no_update to tell Dash to skip updating a specific output based on an input value. For example, if you want to update one output, but not another, based on an input, return dash.no_update for that output.","title":"PreventUpdate &amp; NoUpdate"},{"location":"guides/dash/#interactive-graphing","text":"One really great feature of Dash is its ability to update multiple graphs based on the built-in interactivity of Plotly figures. Plotly figures support hovering, clicking, selecting, and zooming on charts and their data. Using Dash, you can update any component based on the user's interaction with the graph. This is probably best illustrated. Note the charts on the right are updated based on where the cursor hovers. See the documentation for details.","title":"Interactive Graphing"},{"location":"guides/dash/#running-the-app","text":"Run the app in the terminal with: python < app . py > where <app.py> is the path to your python script containing the application. By custom, this is called app.py . Visit localhost:8050 in your browser (or whichever port the terminal states).","title":"Running the app"},{"location":"guides/dash/#deployment","text":"While Dash does provide a paid hosting platform, it's better to host on Heroku. Warning Don't deploy with Debug set to True (In app.py, set app.run_server(debug=False))","title":"Deployment"},{"location":"guides/dash/#heroku","text":"Check out our Heroku tutorial for general guidance and the step-by-step tutorial in the official documentation (under Heroku example ). There are a few configuration options and gotchas specific to Dash you'll need to know about that are covered in the tutorial.","title":"Heroku"},{"location":"guides/dash/#flask","text":"Dash is written on top of flask , and so integrates fairly easily with Flask applications. Deploying your Flask application is exactly the same as deploying your Dash application.","title":"Flask"},{"location":"guides/dash/#embedding-in-existing-websites","text":"Dash apps (and any other apps hosted through Heroku) can be inserted into a webpage using the iframe tag. < iframe src = \"<path-to-my-app.com>\" , height = 800, width = 700 ></ iframe > Note that with the free tier, the page may timeout before the app is spun up and loaded. You'll need to reload the site, which isn't the best user experience, so consider upgrading any app that you want to embed in this way.","title":"Embedding in existing websites"},{"location":"guides/dash/#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"guides/dash/#storing-data-performance","text":"If your app includes expensive analytics, you may want to separate that process from the interactivity of your app to avoid expensive computations with every change of inputs. One option is to store the data in a hidden div . This will store the data in the user's browser for the duration of their session. Data has to be converted to a string like JSON for storage. You can either store the fully computed data or you can perform some computation, save the intermediate results, and then with a chained callback access that information to complete the interaction. The pre-processed data will be available to subsequent callbacks where you stored it. Finally, you can cache the data using flask . This will also allow for persisting the data between sessions. Once you're ready to worry about optimizing performance, see the docs .","title":"Storing data &amp; performance"},{"location":"guides/dash/#multi-page-apps","text":"If you need basic functionality for multiple pages, check out the Tabs core component. If you really need a multi-page app, you can either embed multiple Dash apps in a Django or Flask app, or check out this guidance . I'd also recommend this helpful Towards Data Science article.","title":"Multi-page Apps"},{"location":"guides/dash/#extending","text":"You can write your own Dash components in React JS, a popular JavaScript library for building user interfaces. You'll need to fork the sample component library and, of course, be comfortable working with React JS.","title":"Extending"},{"location":"guides/dash/#custom-css","text":"To add your own CSS, save the CSS files in a folder named assets in the project's root directory. Dash will automatically serve any CSS it finds here. Note that the CSS files are added in alphanumeric order, so if the order matters, prepend numeric values to the filenames to get the loading order you want. You can also copy the provided CSS into this folder and edit it.","title":"Custom CSS"},{"location":"guides/dash/#custom-javascript","text":"To add custom JavaScript, simply include a JavaScript file in the assets folder in the project's root directory. Dash will automatically include it. However, they recommend building new Dash components for any significant functionality you want to add.","title":"Custom JavaScript"},{"location":"guides/py2neo/","text":"Py2neo \u00b6 Py2neo is a python package for working with graph databases on Neo4j. It's not officially supported by Neo4j (as the package neo4j is), but it offers a worthwhile suite of tools for working with graph databases in a more pythonic way. Before you get too invested, a word of warning. The package is not as well used as something like pandas , and it has very little in the way of usage documentation--just a straightforward documentation of its API. You will struggle to find current documentation for problems your facing (besides this wonderful guide, of course). When you do find a solution, it will almost certainly be out of date and no longer work. There's almost nothing on YouTube posted in the last 5 years. You will probably end up digging through its source code at some point trying to figure something out. If you're still here, read on! Tip Always start at the home page of the py2neo handbook ( py2neo.org ) when looking for documentation. If you search google for the specific function or issue you are having, the documentation will be out of date. The missing quick start guide \u00b6 To get up-to-speed, we'll work through a quick canonical example. 0. Installation \u00b6 To install the latest release of py2neo, simply use: pip install --upgrade py2neo 1. Create a graph database \u00b6 First, you'll need to create an empty graph database using Neo4j Desktop. Check out the Neo4j tutorial for the first steps. Start the database. The database should be exposed on port 7687. If not, you will have to update the URI in the next step. Make sure you remember the password. 2. Connect with py2neo \u00b6 Once the graph database has been started, it will be available to connect. from py2neo import Graph URI = 'bolt://localhost:7687' USERNAME = 'neo4j' PASSWORD = '<password>' graph = Graph ( URI , auth = ( USERNAME , PASSWORD )) The first argument to graph is the URI string for the graph. The auth parameter is a tuple of the username (generally 'neo4j') and your password. 3. Create nodes and relationships \u00b6 It is quite easy to create nodes and relationships. from py2neo import Node , Relationship a = Node ( \"Person\" , name = \"Alice\" , age = 33 ) b = Node ( \"Person\" , name = \"Bob\" , age = 44 ) KNOWS = Relationship . type ( \"KNOWS\" ) graph . merge ( KNOWS ( a , b ), \"Person\" , \"name\" ) The nodes assigned to a and b and the relationship KNOWS are not actually created until the merge command is called. Let's look at an alternative pattern that uses a transaction to create the nodes and relationships. This is going to be safer in most cases, as a transaction will be rolled back if any part fails. from py2neo import Node , Relationship tx = graph . begin () a = Node ( \"Person\" , name = \"Alice\" ) tx . create ( a ) b = Node ( \"Person\" , name = \"Bob\" ) ab = Relationship ( a , \"KNOWS\" , b ) tx . create ( ab ) tx . commit () Note some commands are transactions, and you shouldn't use both at the same time (e.g., update properties of node with graph.push()). If you have up to 20,000 operations to run you can batch those with one transaction. This is both safe and efficient. Over 20,000 you may run into a memory issue and you'll need to periodically commit as you are running (committing each of significantly more than 20,000 transaction might take a long time). See more on adding nodes . 4. Query graph \u00b6 You can run any cypher query with the graph.run() command: results = graph . run ( \"MATCH(p:Person {name='Alice'})-[k:KNOWS]-(p2) RETURN p2.name\" ) Usage \u00b6 Add nodes \u00b6 A node has: a label (e.g., 'Person'), a suite of properties (e.g., name='Alice'), a primary key (the property with which to identify the node, e.g., 'name'), and a primary label (e.g., 'Person') tx = graph . begin () new_node = Node ( '<LABEL>' , < id_property >=<> , primary_key = '<id_property>' , primary_label = '<LABEL>' ) tx . merge ( new_node , '<LABEL' , '<id_property>' ) tx . commit () You can also use a dictionary to add properties. Any kwargs are read as properties, any args are read as labels. This can be helpful when reading from a pandas dataframe (see Import from DataFrame rows below). property_dict = { '<property1>' : '<prop_val1>' , '<property2>' : '<prop_val2>' } new_node = Node ( '<LABEL>' , < id_property >=<> , primary_key = '<id_property>' , primary_label = '<LABEL>' , ** property_dict ) Add relationships \u00b6 If Match Nodes \u00b6 To match nodes, first set up a matcher object: matcher = NodeMatcher ( graph ) Update properties (one property) \u00b6 This runs as a transaction, so don't wrap in transaction. Note that updates to properties occur only locally until pushed using graph.push . node = matcher . match ( '<LABEL>' , < id_property >=<> ) . first () if node : node [ < property > ] = <> graph . push ( node ) Update properties (multiple properties) \u00b6 node = matcher . match ( '<LABEL>' , < id_property >=<> ) . first () if node : node . update ( ** properties ) graph . push ( node ) Delete nodes from list \u00b6 tx = graph . begin () for node in bad_node_list : node_matches = matcher . match ( '<LABEL>' , < id_property >= node ) for node in node_matches : graph . delete ( node ) tx . commit () Query Graph \u00b6 Cypher Query Cheatsheet To get data from the graph results = graph . run ( \"<CYPHER QUERY>\" ) # returns cursor to stream results for result in results : # do something Instead of streaming results, data can be read to a list of dictionaries results = graph . run ( f \"\"\" MATCH(n:Node)-[]-() WHERE n.name = \" { < name > } \" RETURN n.name, n.prop_1, n.prop_2 \"\"\" ) . data () # returns: [ { 'n.name' : '' , 'n.prop_1' : '' , 'n.prop_2' : '' }, { 'n.name' : '' , 'n.prop_1' : '' , 'n.prop_2' : '' }, ... ] If your graph has spaces in the properties, use indexing: results = graph . run ( f \"\"\" MATCH(n:Node)-[]-() WHERE n.name = \" { < name > } \" RETURN n['my name'], n['my prop_1'], n['my prop_2'] \"\"\" ) . data () If labels have spaces, use backticks results = graph . run ( f \"\"\" MATCH(n:`My Node`)-[]-() WHERE n.name = \" { < name > } \" RETURN n['my name'], n['my prop_1'], n['my prop_2'] \"\"\" ) . data () If you need relationships from one central node to multiple other nodes, use OPTIONAL MATCH: results = graph . run ( f \"\"\" MATCH(n:Node)-[]-(o:other_node) WHERE n.name=\" { < name > } \" OPTIONAL MATCH (o)-[]-(p) WHERE p:<Label1> OR p:<Label2> RETURN n.name, o.name, labels(p), p.name \"\"\" ) . data () This query gives you nodes with labels Label1 or Label2 related to node with label node that is connected through other_node. Note that in the above example the identifying property for all additional nodes must be the same, namely name . Working with pandas \u00b6 Py2neo integrates well with pandas. Export to DataFrame \u00b6 Note that nodes may not conform well to pandas expectations, and unexpected errors can occur. df = graph . run ( \"MATCH (a:Person) RETURN a.name, a.born\" ) . to_data_frame () Import nodes from DataFrame columns \u00b6 If lists of nodes of the same type are stored in DataFrame columns, you can create a unique list from each column and create a node using that list with the label as the column header. Labels may contain spaces. tx = graph . begin () for column in df . columns : node_set = df [ column ] . unique () . astype ( str ) # some data types not supported as labels for node in node_set : new_node = Node ( column , < id_property >= node , primary_key = '<id_property>' , primary_label = column ) tx . merge ( new_node , column , '<id_property>' ) tx . commit () https://stackoverflow.com/questions/45738180/neo4j-create-nodes-and-relationships-from-pandas-dataframe-with-py2neo Import nodes from DataFrame rows \u00b6 Alternatively, if the nodes correspond to rows and columns are properties (you should find this with tidy datasets), read the df with the primary label as the index column, convert to a dictionary, and iterate through the dictionary items to add nodes. Note that the index column must be unique. df = pd . read_csv ( 'path/to/data' , index_col = '<id_property>' ) node_dict = df . to_dict ( 'index' ) tx = graph . begin () for node , properties in node_dict . items (): node = Node ( '<LABEL>' , < id_property >= node , primary_key = '<id_property>' , primary_label = '<LABEL>' , ** properties ) tx . merge ( node , '<LABEL>' , '<id_property>' ) tx . commit () # needs to be tested Using **properties passes the dictionary of properties, where each column is a key and the data in the cell is a value, to the Node object. Add relationships from DataFrame columns \u00b6 Where nodes are stored in columns, and nodes have already been imported, you can use either df.iterrows() or convert the DataFrame to a dictionary to relate all nodes in a single row. from py2neo import NodeMatcher matcher = NodeMatcher ( graph ) df = pd . read_csv ( 'path/to/data' , index_col = '<id_property>' ) entity_dict = df . to_dict () tx = graph . begin () for node_label , node_dict in entity_dict . items (): for project_id in entity_dict [ node_label ]: project_node = matcher . match ( 'Project' , project_number = project_id ) . first () entity_node = matcher . match ( node_label , name = node_dict . get ( project_id )) . first () if project_node and entity_node : relationship = Relationship ( project_node , \"IN\" , entity_node ) tx . create ( relationship ) tx . commit () How this works: The entity_dict looks like: { 'Country' : { 'AID-512-A-00-08-00005' : 'Brazil' , 'AID-512-A-00-08-00015' : 'Brazil' , 'AID-512-A-10-00004' : 'Brazil' , 'AID-512-A-11-00004' : 'Brazil' , 'AID-512-A-16-00001' : 'Brazil' }, 'Income Group' : { 'AID-512-A-00-08-00005' : 'Upper Middle Income Country' , 'AID-512-A-00-08-00015' : 'Upper Middle Income Country' , 'AID-512-A-10-00004' : 'Upper Middle Income Country' , 'AID-512-A-11-00004' : 'Upper Middle Income Country' , 'AID-512-A-16-00001' : 'Upper Middle Income Country' } } Each column has the relationship required between the index (in this case a project number) and the node of the type contained in that column. You can create all of the relationships required and then repeat the process by specifying a new index column, if needed. For each column (node_label), we use the dictionary associated to match each project id and each node. If a match is found for both, we create a relationship. Don't forget to commit the transaction. You can use another dictionary to specify the label for the relationship if you want to have different relationship labels for different columns. Simply lookup the relationship name in place of \"IN\". Using objects \u00b6 from py2neo.ogm import GraphObject , Property class Person ( GraphObject ): name = Property () born = Property () [( a . name , a . born ) for a in Person . match ( graph ) . limit ( 3 )] # returns [( 'Laurence Fishburne' , 1961 ), ( 'Hugo Weaving' , 1960 ), ( 'Lilly Wachowski' , 1967 )] \u00b6","title":"py2neo"},{"location":"guides/py2neo/#py2neo","text":"Py2neo is a python package for working with graph databases on Neo4j. It's not officially supported by Neo4j (as the package neo4j is), but it offers a worthwhile suite of tools for working with graph databases in a more pythonic way. Before you get too invested, a word of warning. The package is not as well used as something like pandas , and it has very little in the way of usage documentation--just a straightforward documentation of its API. You will struggle to find current documentation for problems your facing (besides this wonderful guide, of course). When you do find a solution, it will almost certainly be out of date and no longer work. There's almost nothing on YouTube posted in the last 5 years. You will probably end up digging through its source code at some point trying to figure something out. If you're still here, read on! Tip Always start at the home page of the py2neo handbook ( py2neo.org ) when looking for documentation. If you search google for the specific function or issue you are having, the documentation will be out of date.","title":"Py2neo"},{"location":"guides/py2neo/#the-missing-quick-start-guide","text":"To get up-to-speed, we'll work through a quick canonical example.","title":"The missing quick start guide"},{"location":"guides/py2neo/#0-installation","text":"To install the latest release of py2neo, simply use: pip install --upgrade py2neo","title":"0. Installation"},{"location":"guides/py2neo/#1-create-a-graph-database","text":"First, you'll need to create an empty graph database using Neo4j Desktop. Check out the Neo4j tutorial for the first steps. Start the database. The database should be exposed on port 7687. If not, you will have to update the URI in the next step. Make sure you remember the password.","title":"1. Create a graph database"},{"location":"guides/py2neo/#2-connect-with-py2neo","text":"Once the graph database has been started, it will be available to connect. from py2neo import Graph URI = 'bolt://localhost:7687' USERNAME = 'neo4j' PASSWORD = '<password>' graph = Graph ( URI , auth = ( USERNAME , PASSWORD )) The first argument to graph is the URI string for the graph. The auth parameter is a tuple of the username (generally 'neo4j') and your password.","title":"2. Connect with py2neo"},{"location":"guides/py2neo/#3-create-nodes-and-relationships","text":"It is quite easy to create nodes and relationships. from py2neo import Node , Relationship a = Node ( \"Person\" , name = \"Alice\" , age = 33 ) b = Node ( \"Person\" , name = \"Bob\" , age = 44 ) KNOWS = Relationship . type ( \"KNOWS\" ) graph . merge ( KNOWS ( a , b ), \"Person\" , \"name\" ) The nodes assigned to a and b and the relationship KNOWS are not actually created until the merge command is called. Let's look at an alternative pattern that uses a transaction to create the nodes and relationships. This is going to be safer in most cases, as a transaction will be rolled back if any part fails. from py2neo import Node , Relationship tx = graph . begin () a = Node ( \"Person\" , name = \"Alice\" ) tx . create ( a ) b = Node ( \"Person\" , name = \"Bob\" ) ab = Relationship ( a , \"KNOWS\" , b ) tx . create ( ab ) tx . commit () Note some commands are transactions, and you shouldn't use both at the same time (e.g., update properties of node with graph.push()). If you have up to 20,000 operations to run you can batch those with one transaction. This is both safe and efficient. Over 20,000 you may run into a memory issue and you'll need to periodically commit as you are running (committing each of significantly more than 20,000 transaction might take a long time). See more on adding nodes .","title":"3. Create nodes and relationships"},{"location":"guides/py2neo/#4-query-graph","text":"You can run any cypher query with the graph.run() command: results = graph . run ( \"MATCH(p:Person {name='Alice'})-[k:KNOWS]-(p2) RETURN p2.name\" )","title":"4. Query graph"},{"location":"guides/py2neo/#usage","text":"","title":"Usage"},{"location":"guides/py2neo/#add-nodes","text":"A node has: a label (e.g., 'Person'), a suite of properties (e.g., name='Alice'), a primary key (the property with which to identify the node, e.g., 'name'), and a primary label (e.g., 'Person') tx = graph . begin () new_node = Node ( '<LABEL>' , < id_property >=<> , primary_key = '<id_property>' , primary_label = '<LABEL>' ) tx . merge ( new_node , '<LABEL' , '<id_property>' ) tx . commit () You can also use a dictionary to add properties. Any kwargs are read as properties, any args are read as labels. This can be helpful when reading from a pandas dataframe (see Import from DataFrame rows below). property_dict = { '<property1>' : '<prop_val1>' , '<property2>' : '<prop_val2>' } new_node = Node ( '<LABEL>' , < id_property >=<> , primary_key = '<id_property>' , primary_label = '<LABEL>' , ** property_dict )","title":"Add nodes"},{"location":"guides/py2neo/#add-relationships","text":"If","title":"Add relationships"},{"location":"guides/py2neo/#match-nodes","text":"To match nodes, first set up a matcher object: matcher = NodeMatcher ( graph )","title":"Match Nodes"},{"location":"guides/py2neo/#update-properties-one-property","text":"This runs as a transaction, so don't wrap in transaction. Note that updates to properties occur only locally until pushed using graph.push . node = matcher . match ( '<LABEL>' , < id_property >=<> ) . first () if node : node [ < property > ] = <> graph . push ( node )","title":"Update properties (one property)"},{"location":"guides/py2neo/#update-properties-multiple-properties","text":"node = matcher . match ( '<LABEL>' , < id_property >=<> ) . first () if node : node . update ( ** properties ) graph . push ( node )","title":"Update properties (multiple properties)"},{"location":"guides/py2neo/#delete-nodes-from-list","text":"tx = graph . begin () for node in bad_node_list : node_matches = matcher . match ( '<LABEL>' , < id_property >= node ) for node in node_matches : graph . delete ( node ) tx . commit ()","title":"Delete nodes from list"},{"location":"guides/py2neo/#query-graph","text":"Cypher Query Cheatsheet To get data from the graph results = graph . run ( \"<CYPHER QUERY>\" ) # returns cursor to stream results for result in results : # do something Instead of streaming results, data can be read to a list of dictionaries results = graph . run ( f \"\"\" MATCH(n:Node)-[]-() WHERE n.name = \" { < name > } \" RETURN n.name, n.prop_1, n.prop_2 \"\"\" ) . data () # returns: [ { 'n.name' : '' , 'n.prop_1' : '' , 'n.prop_2' : '' }, { 'n.name' : '' , 'n.prop_1' : '' , 'n.prop_2' : '' }, ... ] If your graph has spaces in the properties, use indexing: results = graph . run ( f \"\"\" MATCH(n:Node)-[]-() WHERE n.name = \" { < name > } \" RETURN n['my name'], n['my prop_1'], n['my prop_2'] \"\"\" ) . data () If labels have spaces, use backticks results = graph . run ( f \"\"\" MATCH(n:`My Node`)-[]-() WHERE n.name = \" { < name > } \" RETURN n['my name'], n['my prop_1'], n['my prop_2'] \"\"\" ) . data () If you need relationships from one central node to multiple other nodes, use OPTIONAL MATCH: results = graph . run ( f \"\"\" MATCH(n:Node)-[]-(o:other_node) WHERE n.name=\" { < name > } \" OPTIONAL MATCH (o)-[]-(p) WHERE p:<Label1> OR p:<Label2> RETURN n.name, o.name, labels(p), p.name \"\"\" ) . data () This query gives you nodes with labels Label1 or Label2 related to node with label node that is connected through other_node. Note that in the above example the identifying property for all additional nodes must be the same, namely name .","title":"Query Graph"},{"location":"guides/py2neo/#working-with-pandas","text":"Py2neo integrates well with pandas.","title":"Working with pandas"},{"location":"guides/py2neo/#export-to-dataframe","text":"Note that nodes may not conform well to pandas expectations, and unexpected errors can occur. df = graph . run ( \"MATCH (a:Person) RETURN a.name, a.born\" ) . to_data_frame ()","title":"Export to DataFrame"},{"location":"guides/py2neo/#import-nodes-from-dataframe-columns","text":"If lists of nodes of the same type are stored in DataFrame columns, you can create a unique list from each column and create a node using that list with the label as the column header. Labels may contain spaces. tx = graph . begin () for column in df . columns : node_set = df [ column ] . unique () . astype ( str ) # some data types not supported as labels for node in node_set : new_node = Node ( column , < id_property >= node , primary_key = '<id_property>' , primary_label = column ) tx . merge ( new_node , column , '<id_property>' ) tx . commit () https://stackoverflow.com/questions/45738180/neo4j-create-nodes-and-relationships-from-pandas-dataframe-with-py2neo","title":"Import nodes from DataFrame columns"},{"location":"guides/py2neo/#import-nodes-from-dataframe-rows","text":"Alternatively, if the nodes correspond to rows and columns are properties (you should find this with tidy datasets), read the df with the primary label as the index column, convert to a dictionary, and iterate through the dictionary items to add nodes. Note that the index column must be unique. df = pd . read_csv ( 'path/to/data' , index_col = '<id_property>' ) node_dict = df . to_dict ( 'index' ) tx = graph . begin () for node , properties in node_dict . items (): node = Node ( '<LABEL>' , < id_property >= node , primary_key = '<id_property>' , primary_label = '<LABEL>' , ** properties ) tx . merge ( node , '<LABEL>' , '<id_property>' ) tx . commit () # needs to be tested Using **properties passes the dictionary of properties, where each column is a key and the data in the cell is a value, to the Node object.","title":"Import nodes from DataFrame rows"},{"location":"guides/py2neo/#add-relationships-from-dataframe-columns","text":"Where nodes are stored in columns, and nodes have already been imported, you can use either df.iterrows() or convert the DataFrame to a dictionary to relate all nodes in a single row. from py2neo import NodeMatcher matcher = NodeMatcher ( graph ) df = pd . read_csv ( 'path/to/data' , index_col = '<id_property>' ) entity_dict = df . to_dict () tx = graph . begin () for node_label , node_dict in entity_dict . items (): for project_id in entity_dict [ node_label ]: project_node = matcher . match ( 'Project' , project_number = project_id ) . first () entity_node = matcher . match ( node_label , name = node_dict . get ( project_id )) . first () if project_node and entity_node : relationship = Relationship ( project_node , \"IN\" , entity_node ) tx . create ( relationship ) tx . commit () How this works: The entity_dict looks like: { 'Country' : { 'AID-512-A-00-08-00005' : 'Brazil' , 'AID-512-A-00-08-00015' : 'Brazil' , 'AID-512-A-10-00004' : 'Brazil' , 'AID-512-A-11-00004' : 'Brazil' , 'AID-512-A-16-00001' : 'Brazil' }, 'Income Group' : { 'AID-512-A-00-08-00005' : 'Upper Middle Income Country' , 'AID-512-A-00-08-00015' : 'Upper Middle Income Country' , 'AID-512-A-10-00004' : 'Upper Middle Income Country' , 'AID-512-A-11-00004' : 'Upper Middle Income Country' , 'AID-512-A-16-00001' : 'Upper Middle Income Country' } } Each column has the relationship required between the index (in this case a project number) and the node of the type contained in that column. You can create all of the relationships required and then repeat the process by specifying a new index column, if needed. For each column (node_label), we use the dictionary associated to match each project id and each node. If a match is found for both, we create a relationship. Don't forget to commit the transaction. You can use another dictionary to specify the label for the relationship if you want to have different relationship labels for different columns. Simply lookup the relationship name in place of \"IN\".","title":"Add relationships from DataFrame columns"},{"location":"guides/py2neo/#using-objects","text":"from py2neo.ogm import GraphObject , Property class Person ( GraphObject ): name = Property () born = Property () [( a . name , a . born ) for a in Person . match ( graph ) . limit ( 3 )] # returns [( 'Laurence Fishburne' , 1961 ), ( 'Hugo Weaving' , 1960 ), ( 'Lilly Wachowski' , 1967 )]","title":"Using objects"},{"location":"guides/py2neo/#_1","text":"","title":""},{"location":"maintenance/development-guidance/","text":"EI Development Guidance \u00b6 This was produced with mkdocs. For full documentation visit mkdocs.org . Contents \u00b6 Home Portfolio Metrics Services Metrics Design Philosophy Project Planning Git Development Deployment (non-tech: how do we deliver products?) Data Management (non-tech: database overview) Data Science (non-tech: types of data analysis) Data Visualization (non-tech: visualization options) Spatial Analysis (non-tech: types of spatial data analysis) Packages News/Blog How to Maintain this Site Additional Resources Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Installations \u00b6 mkdocs and the material theme must be installed: pip install mkdocs pip install mkdocs-material Contributing \u00b6 This project is available on GitHub here . Fork the project to begin contributing. You can find more information on how to fork a project here , or if you're unfamiliar with Git and GitHub, start here . Here are the technologies used in this project, you'll need to be familiar with these to contribute. MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE for yaml Git - version control Github - code sharing Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc. Use mkdocs serve to visualize the site while working on it. Add the site/ folder to the .gitignore file. Deploying changes \u00b6 To deploy the website: Open the local repository Make sure you're working on the master branch (use git status ) Open the Anaconda Prompt* and run mkdocs gh-deploy Note that in the GitHub repo on the Settings Tab, under project pages, should have the ghp-pages branch selected as the page. *To add the Anaconda Prompt to your right-click context menu on windows, see this gist . Customs Used \u00b6 To identify a user-provided value, wrap it in <> within a code block: git commit <updated-file> Highlight package names, files, code, and cli commands in code blocks: rasterio","title":"Development Guide"},{"location":"maintenance/development-guidance/#ei-development-guidance","text":"This was produced with mkdocs. For full documentation visit mkdocs.org .","title":"EI Development Guidance"},{"location":"maintenance/development-guidance/#contents","text":"Home Portfolio Metrics Services Metrics Design Philosophy Project Planning Git Development Deployment (non-tech: how do we deliver products?) Data Management (non-tech: database overview) Data Science (non-tech: types of data analysis) Data Visualization (non-tech: visualization options) Spatial Analysis (non-tech: types of spatial data analysis) Packages News/Blog How to Maintain this Site Additional Resources","title":"Contents"},{"location":"maintenance/development-guidance/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"maintenance/development-guidance/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"maintenance/development-guidance/#installations","text":"mkdocs and the material theme must be installed: pip install mkdocs pip install mkdocs-material","title":"Installations"},{"location":"maintenance/development-guidance/#contributing","text":"This project is available on GitHub here . Fork the project to begin contributing. You can find more information on how to fork a project here , or if you're unfamiliar with Git and GitHub, start here . Here are the technologies used in this project, you'll need to be familiar with these to contribute. MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE for yaml Git - version control Github - code sharing Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc. Use mkdocs serve to visualize the site while working on it. Add the site/ folder to the .gitignore file.","title":"Contributing"},{"location":"maintenance/development-guidance/#deploying-changes","text":"To deploy the website: Open the local repository Make sure you're working on the master branch (use git status ) Open the Anaconda Prompt* and run mkdocs gh-deploy Note that in the GitHub repo on the Settings Tab, under project pages, should have the ghp-pages branch selected as the page. *To add the Anaconda Prompt to your right-click context menu on windows, see this gist .","title":"Deploying changes"},{"location":"maintenance/development-guidance/#customs-used","text":"To identify a user-provided value, wrap it in <> within a code block: git commit <updated-file> Highlight package names, files, code, and cli commands in code blocks: rasterio","title":"Customs Used"},{"location":"maintenance/tips-and-tricks/","text":"Tips & Tricks \u00b6 Markdown Reference \u00b6 Here is a handy reference for writing in markdown, provided by Typora. Converting Notebooks \u00b6 To add a notebook to the site, us nbconvert to convert the notebook to markdown. Save the output with the desired filename and into a new directory. The new directory will include the markdown file and assets within a *_files directory. Cut and paste the new directory into the ei-dev docs/ folder in the desired location. Open the Anaconda Prompt window within the notebook's root directory Run the following command (a new directory will be created if needed): jupyter nbconvert <notebook.ipynb> --to markdown --output-dir='<rel path to new dirname>' --output <desired-filename> For example jupyter nbconvert my_notebook.ipynb --to markdown --output-dir='../md-notebooks' --output my_markdown Cut and paste the newly created directory into the ei-dev docs/ folder in the desired location. Syntax highlighting is done using the codehilite package (part of standard markdown library) and Pygments (part of standard Python library). However, codehilite must be enabled in the mkdocs.yml file. See documentation for more. Changes made to the notebook will not be reflected in the markdown files, and you'll have to overwrite the folder you created the first time. If you change the name of the outputs, update the mkdocs.yml folder. Linking To and Within Documents \u00b6 To link to another page, simply provide the relative path to the page as a markdown file. You can link to a section if you add a '#' after the filename. Sections should be all lower-case with '-' in place of spaces (make sure all headers are unique as well). Ignore any punctuation in the header. If in the same page, omit the page name. See also documentation . [here's a link to another document](foldername/filename.md#header-name) [here's a link to a header in this document](#header-name) Images and Gifs \u00b6 You can either link to images and gifs online or save locally. If saving locally, store in assets/ within the section directory (e.g., consulting-process/ ). The alternative text (within the square brackets) is read by accessibility services. See also documentation . Use web links whenever possible. ![here's an image](assets/gif.gif) YouTube Videos \u00b6 To include Youtube videos, use the iframe provided under the 'Share' option. Any HTML pasted directly within a markdown file will be rendered in the site. Forms \u00b6 Google forms (and probably many other web forms) can be embedded as HTML within an iframe. Extensions \u00b6 Markdown and Mkdocs both offer extensions for rendering markdown. Note that the extensions may need to be configured for Typora as well as Mkdocs for consistency between editing and serving. Material has a few extensions as well. Check these out in their project documentation pages to see if the functionality would be helpful. The site currently uses the Material theme extension. Some interesting extensions: Admonition \u00b6 Provides callout style boxes, including collapsible boxes, for example: Warning Don't do this really bad thing! For your information... ...I changed the title of this note. See documentation for more. PyMdown \u00b6 A ton of functionality in this package of extensions , including highlighting text, inline editing, math equations, and task lists (though not interactive task lists). Interactive Visualizations \u00b6 Maps, dashboards, and other interactive visualizations should be passed in using the iframe. See Plotly guidance here . HTML Tags \u00b6 Any HTML written in the document will get passed as pure HTML when served. If the standard markdown isn't sufficient, use pure HTML. For example, to get the credit for the comic on the home page to be right adjusted, I typed this line (don't include block quotes if you want the actual text rendered rather than the code) : < div style = \"text-align: right\" > Credit: Randall Munroe (xkcd.com/2054) </ div > A few tags to consider: Details \u00b6 This tag allows for drop-down type hiding of content. < detail > < summary > The text when collapsed goes here </ summary > < p > The text that gets hidden goes here </ p > </ detail > It's best to work within the source code mode ( ctrl+/ ), as Markdown treats this strangely if trying to pass markdown between the detail tags. Click me! This would be the first paragraph of content Here's some more content with a link Note that the details tag is styled by the extension admonition as a call out box when using the Material Theme. I couldn't prevent this behavior by removing the admonition extension from my yaml file. Switching to a different theme will avoid this behavior. Adding custom CSS may also be an option. Details tags are also available as an extension (see PyMdown ). JavaScript \u00b6 You can add custom JavaScript to MkDocs provided you're ok with the script running on every page. In the mkdocs.yml file, add a line that points to the location of the JavaScript file relative to the docs/ folder: extra_javascript : - javascripts/scripts.js The script will automatically be included on every page of the document, no need to add a script tag. Here's an example that I've implemented on the site. I want all external links to open in a new window, and internal links to open in the same window. I could express each link in html, but it would be easier to update all links written in standard markdown (which doesn't have this functionality out-of-the-box). Here's the content of the JavaScript file: var links = document . links ; for ( var i = 0 , linksLength = links . length ; i < linksLength ; i ++ ) { if ( links [ i ]. hostname != window . location . hostname ) { links [ i ]. target = '_blank' ; } } This script gets a list of all links in the document, checks whether the hostname is the same as the hostname of the current window (i.e., the hostname of my website), and then updates the target property of each link to \"_blank\" whenever the link is external. You can also add JavaScript (and CSS) by defining a custom theme . CSS \u00b6 See also JavaScript above, but basically just include in the mkdocs.yml file the following: extra_css : - stylesheets/<my_css.css> - stylesheets/<more_css.css>","title":"Tips & Tricks"},{"location":"maintenance/tips-and-tricks/#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"maintenance/tips-and-tricks/#markdown-reference","text":"Here is a handy reference for writing in markdown, provided by Typora.","title":"Markdown Reference"},{"location":"maintenance/tips-and-tricks/#converting-notebooks","text":"To add a notebook to the site, us nbconvert to convert the notebook to markdown. Save the output with the desired filename and into a new directory. The new directory will include the markdown file and assets within a *_files directory. Cut and paste the new directory into the ei-dev docs/ folder in the desired location. Open the Anaconda Prompt window within the notebook's root directory Run the following command (a new directory will be created if needed): jupyter nbconvert <notebook.ipynb> --to markdown --output-dir='<rel path to new dirname>' --output <desired-filename> For example jupyter nbconvert my_notebook.ipynb --to markdown --output-dir='../md-notebooks' --output my_markdown Cut and paste the newly created directory into the ei-dev docs/ folder in the desired location. Syntax highlighting is done using the codehilite package (part of standard markdown library) and Pygments (part of standard Python library). However, codehilite must be enabled in the mkdocs.yml file. See documentation for more. Changes made to the notebook will not be reflected in the markdown files, and you'll have to overwrite the folder you created the first time. If you change the name of the outputs, update the mkdocs.yml folder.","title":"Converting Notebooks"},{"location":"maintenance/tips-and-tricks/#linking-to-and-within-documents","text":"To link to another page, simply provide the relative path to the page as a markdown file. You can link to a section if you add a '#' after the filename. Sections should be all lower-case with '-' in place of spaces (make sure all headers are unique as well). Ignore any punctuation in the header. If in the same page, omit the page name. See also documentation . [here's a link to another document](foldername/filename.md#header-name) [here's a link to a header in this document](#header-name)","title":"Linking To and Within Documents"},{"location":"maintenance/tips-and-tricks/#images-and-gifs","text":"You can either link to images and gifs online or save locally. If saving locally, store in assets/ within the section directory (e.g., consulting-process/ ). The alternative text (within the square brackets) is read by accessibility services. See also documentation . Use web links whenever possible. ![here's an image](assets/gif.gif)","title":"Images and Gifs"},{"location":"maintenance/tips-and-tricks/#youtube-videos","text":"To include Youtube videos, use the iframe provided under the 'Share' option. Any HTML pasted directly within a markdown file will be rendered in the site.","title":"YouTube Videos"},{"location":"maintenance/tips-and-tricks/#forms","text":"Google forms (and probably many other web forms) can be embedded as HTML within an iframe.","title":"Forms"},{"location":"maintenance/tips-and-tricks/#extensions","text":"Markdown and Mkdocs both offer extensions for rendering markdown. Note that the extensions may need to be configured for Typora as well as Mkdocs for consistency between editing and serving. Material has a few extensions as well. Check these out in their project documentation pages to see if the functionality would be helpful. The site currently uses the Material theme extension. Some interesting extensions:","title":"Extensions"},{"location":"maintenance/tips-and-tricks/#admonition","text":"Provides callout style boxes, including collapsible boxes, for example: Warning Don't do this really bad thing! For your information... ...I changed the title of this note. See documentation for more.","title":"Admonition"},{"location":"maintenance/tips-and-tricks/#pymdown","text":"A ton of functionality in this package of extensions , including highlighting text, inline editing, math equations, and task lists (though not interactive task lists).","title":"PyMdown"},{"location":"maintenance/tips-and-tricks/#interactive-visualizations","text":"Maps, dashboards, and other interactive visualizations should be passed in using the iframe. See Plotly guidance here .","title":"Interactive Visualizations"},{"location":"maintenance/tips-and-tricks/#html-tags","text":"Any HTML written in the document will get passed as pure HTML when served. If the standard markdown isn't sufficient, use pure HTML. For example, to get the credit for the comic on the home page to be right adjusted, I typed this line (don't include block quotes if you want the actual text rendered rather than the code) : < div style = \"text-align: right\" > Credit: Randall Munroe (xkcd.com/2054) </ div > A few tags to consider:","title":"HTML Tags"},{"location":"maintenance/tips-and-tricks/#details","text":"This tag allows for drop-down type hiding of content. < detail > < summary > The text when collapsed goes here </ summary > < p > The text that gets hidden goes here </ p > </ detail > It's best to work within the source code mode ( ctrl+/ ), as Markdown treats this strangely if trying to pass markdown between the detail tags. Click me! This would be the first paragraph of content Here's some more content with a link Note that the details tag is styled by the extension admonition as a call out box when using the Material Theme. I couldn't prevent this behavior by removing the admonition extension from my yaml file. Switching to a different theme will avoid this behavior. Adding custom CSS may also be an option. Details tags are also available as an extension (see PyMdown ).","title":"Details"},{"location":"maintenance/tips-and-tricks/#javascript","text":"You can add custom JavaScript to MkDocs provided you're ok with the script running on every page. In the mkdocs.yml file, add a line that points to the location of the JavaScript file relative to the docs/ folder: extra_javascript : - javascripts/scripts.js The script will automatically be included on every page of the document, no need to add a script tag. Here's an example that I've implemented on the site. I want all external links to open in a new window, and internal links to open in the same window. I could express each link in html, but it would be easier to update all links written in standard markdown (which doesn't have this functionality out-of-the-box). Here's the content of the JavaScript file: var links = document . links ; for ( var i = 0 , linksLength = links . length ; i < linksLength ; i ++ ) { if ( links [ i ]. hostname != window . location . hostname ) { links [ i ]. target = '_blank' ; } } This script gets a list of all links in the document, checks whether the hostname is the same as the hostname of the current window (i.e., the hostname of my website), and then updates the target property of each link to \"_blank\" whenever the link is external. You can also add JavaScript (and CSS) by defining a custom theme .","title":"JavaScript"},{"location":"maintenance/tips-and-tricks/#css","text":"See also JavaScript above, but basically just include in the mkdocs.yml file the following: extra_css : - stylesheets/<my_css.css> - stylesheets/<more_css.css>","title":"CSS"},{"location":"metrics-design/overview/","text":"Metrics Design Philosophy \u00b6 Our metrics design philosophy has been developed and tested through many years building solutions for clients and internal initiatives alike. It is based on a variety of different frameworks, from the Open Standards for the Practice of Conservation to Systems Thinking to Human-Centered Design (see list of inspirations below). To be honest, very little in this philosophy is new. What is unique about it, however, is its synthesis of these various frameworks. Human-centered design typically assumes you already have your solution, and are simply adding a new feature. Open Standards/Systems Thinking are focused on identifying the right place to intervene - but are, in my opinion, lacking in specifics as to how to design and implement a successful solution. Bringing the concepts within these processes together creates a more holistic, and hopefully impactful, approach to conservation. So how is it unique? \u00b6 There's a few key characteristics of this philosophy that make it unique. People-first : This philosophy is unapologetically people-centric. We seek to change people's behavior to achieve our conservation targets. We do that by solving people's problems or creating new opportunities for people. If changing human behavior isn't at the core of your problem or central to the success of your strategy, this isn't the right process for you. Evidence-driven : Prepare to test every assumption! We seek evidence from the very beginning of the process - we do not wait until we've implemented the program to start testing our assumptions. Importantly, we do not even entertain solutions until we have strong evidence that the problem exists and that solving it will help achieve our conservation target. And we never commit to our solution unless we see results in the real world. Inside-out : If you're used to spending years developing the perfect program, creating a monitoring plan, and then seeing what happens, this might be disorienting. Instead, we'll start with the most core functionality - the smallest solution we could build while still meeting the need - and use that to test our assumptions. Once we have some positive evidence that our solution is working for our intended users, we'll build the next piece. We'll greatly reduce the chances of wasting our time (or even making the problem worse) if we fail fast and fail small , as some like to say. Once we've found success, we iterate and scale up. The Process \u00b6 We've codified our philosophy into a seven step process. Each step is described in more detail on the pages linked through here. Also, note that there are templates, additional guidance, and other resources available in Additional Resources , many of which are referenced within these linked pages. Process Steps \u00b6 \u200b Step 1 : Start at the End \u200b Step 2 : Map It \u200b Step 3 : Develop Empathy \u200b Step 4 : Define the Problem \u200b Step 5 : Identify a Solution \u200b Step 6 : Implement \u200b Step 7 : Iterate & Adapt Additional Resources \u00b6 \u200b Scoping Canvas \u200b Product Definition \u200b Persona Development Guide \u200b Interview Guide Inspiration \u00b6 These resources and frameworks were leaned on heavily in the development of this approach. Open Standards for the Practice of Conservation Outcomes Mapping ( 1 ) ( 2 ) ( 3 ) & EI Speed Training Systems Thinking ( Thinking in Systems , Systems Thinking for Social Change , Omidyar Systems Practice Workbook) Design Thinking & Lean UX ( Design Sprint , IDEO , Venture Design , The Lean Startup ) Transformative Scenario Planning (Adam Kahane) Structured Decision Making","title":"Overview"},{"location":"metrics-design/overview/#metrics-design-philosophy","text":"Our metrics design philosophy has been developed and tested through many years building solutions for clients and internal initiatives alike. It is based on a variety of different frameworks, from the Open Standards for the Practice of Conservation to Systems Thinking to Human-Centered Design (see list of inspirations below). To be honest, very little in this philosophy is new. What is unique about it, however, is its synthesis of these various frameworks. Human-centered design typically assumes you already have your solution, and are simply adding a new feature. Open Standards/Systems Thinking are focused on identifying the right place to intervene - but are, in my opinion, lacking in specifics as to how to design and implement a successful solution. Bringing the concepts within these processes together creates a more holistic, and hopefully impactful, approach to conservation.","title":"Metrics Design Philosophy"},{"location":"metrics-design/overview/#so-how-is-it-unique","text":"There's a few key characteristics of this philosophy that make it unique. People-first : This philosophy is unapologetically people-centric. We seek to change people's behavior to achieve our conservation targets. We do that by solving people's problems or creating new opportunities for people. If changing human behavior isn't at the core of your problem or central to the success of your strategy, this isn't the right process for you. Evidence-driven : Prepare to test every assumption! We seek evidence from the very beginning of the process - we do not wait until we've implemented the program to start testing our assumptions. Importantly, we do not even entertain solutions until we have strong evidence that the problem exists and that solving it will help achieve our conservation target. And we never commit to our solution unless we see results in the real world. Inside-out : If you're used to spending years developing the perfect program, creating a monitoring plan, and then seeing what happens, this might be disorienting. Instead, we'll start with the most core functionality - the smallest solution we could build while still meeting the need - and use that to test our assumptions. Once we have some positive evidence that our solution is working for our intended users, we'll build the next piece. We'll greatly reduce the chances of wasting our time (or even making the problem worse) if we fail fast and fail small , as some like to say. Once we've found success, we iterate and scale up.","title":"So how is it unique?"},{"location":"metrics-design/overview/#the-process","text":"We've codified our philosophy into a seven step process. Each step is described in more detail on the pages linked through here. Also, note that there are templates, additional guidance, and other resources available in Additional Resources , many of which are referenced within these linked pages.","title":"The Process"},{"location":"metrics-design/overview/#process-steps","text":"\u200b Step 1 : Start at the End \u200b Step 2 : Map It \u200b Step 3 : Develop Empathy \u200b Step 4 : Define the Problem \u200b Step 5 : Identify a Solution \u200b Step 6 : Implement \u200b Step 7 : Iterate & Adapt","title":"Process Steps"},{"location":"metrics-design/overview/#additional-resources","text":"\u200b Scoping Canvas \u200b Product Definition \u200b Persona Development Guide \u200b Interview Guide","title":"Additional Resources"},{"location":"metrics-design/overview/#inspiration","text":"These resources and frameworks were leaned on heavily in the development of this approach. Open Standards for the Practice of Conservation Outcomes Mapping ( 1 ) ( 2 ) ( 3 ) & EI Speed Training Systems Thinking ( Thinking in Systems , Systems Thinking for Social Change , Omidyar Systems Practice Workbook) Design Thinking & Lean UX ( Design Sprint , IDEO , Venture Design , The Lean Startup ) Transformative Scenario Planning (Adam Kahane) Structured Decision Making","title":"Inspiration"},{"location":"metrics-design/step1-start-at-the-end/","text":"Step 1: Start at the End \u00b6 Describe the world as it would be if the problem were solved. Be agnostic about the solution, just describe your vision for world. A vision statement is a good place to start. A vision statement should meet the following criteria: Idealistic About the future Observable Not about the intervention Don\u2019t wordsmith the vision statement quite yet. It\u2019s just intended to provide a guiding light or goal post throughout this process. You may choose to re-write it later once you better understand the problem. If using the Open Standards, include in your vision the conservation targets . Don\u2019t worry about defining measurable goals yet, unless they are already defined. If desired, write down design principles to guide you through this process. These principles should reflect your values and any constraints on the solution; they will guide decision making throughout the process. See examples here . Other Techniques \u00b6 Ask a question: If you\u2019re not quite sure how you think the world could be improved, but sense that there is room for improvement, just ask yourself the question: how might this be better? Could we do this in a different (e.g., more efficient/equitable) way? Keep asking yourself questions until you have a good sense of what can be changed. Asking the right design question is essential to solving the right problem. Bug list: another approach to visualizing a better world is writing down all of the things that bug you about the current situation. Is one of those \u2018bugs\u2019 sufficiently problematic that it\u2019s worth investing time and resources in addressing? More Info on This Step \u00b6 See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets. They define a vision statement as relatively general, visionary, and brief. See Omidyar Group\u2019s Systems Practice section Set Your Goals.","title":"Step 1: Start at the End"},{"location":"metrics-design/step1-start-at-the-end/#step-1-start-at-the-end","text":"Describe the world as it would be if the problem were solved. Be agnostic about the solution, just describe your vision for world. A vision statement is a good place to start. A vision statement should meet the following criteria: Idealistic About the future Observable Not about the intervention Don\u2019t wordsmith the vision statement quite yet. It\u2019s just intended to provide a guiding light or goal post throughout this process. You may choose to re-write it later once you better understand the problem. If using the Open Standards, include in your vision the conservation targets . Don\u2019t worry about defining measurable goals yet, unless they are already defined. If desired, write down design principles to guide you through this process. These principles should reflect your values and any constraints on the solution; they will guide decision making throughout the process. See examples here .","title":"Step 1: Start at the End"},{"location":"metrics-design/step1-start-at-the-end/#other-techniques","text":"Ask a question: If you\u2019re not quite sure how you think the world could be improved, but sense that there is room for improvement, just ask yourself the question: how might this be better? Could we do this in a different (e.g., more efficient/equitable) way? Keep asking yourself questions until you have a good sense of what can be changed. Asking the right design question is essential to solving the right problem. Bug list: another approach to visualizing a better world is writing down all of the things that bug you about the current situation. Is one of those \u2018bugs\u2019 sufficiently problematic that it\u2019s worth investing time and resources in addressing?","title":"Other Techniques"},{"location":"metrics-design/step1-start-at-the-end/#more-info-on-this-step","text":"See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets. They define a vision statement as relatively general, visionary, and brief. See Omidyar Group\u2019s Systems Practice section Set Your Goals.","title":"More Info on This Step"},{"location":"metrics-design/step2-map-it/","text":"Step 2: Map It \u00b6 Documenting your understanding of the problem space in a visual way is an impactful way of capturing your understanding and can be a good way of sharing it with others. A Situation Model , Systems Map , or Journey Map are all great tools. The Situation Model focuses on relationships between human causes (drivers) of threats to your vision (conservation targets); whereas the Systems Map describes feedback loops between relevant elements of the system. Journey Maps capture the chain of behaviors required to solve a problem or do a job. If you can, start by defining the scope of the system that you will be exploring, such as geographic boundaries or levels (e.g., federal vs. state gov\u2019t). This will help focus your search to just that area of the problem space in which you have a manageable interest . Alternatively, use a framing question to focus your interrogation of the system. Developing these maps typically requires formative research and immersion in the problem space. Start by listing the factors (i.e., threats, drivers and enabling conditions ) that you know and building out the map using a breadth-first search of the problem space. Add to the map anything that seems relevant, but don\u2019t spend too much time understanding how. Group items using affinity mapping until the relationships become clear. Next, focus on what appear to be the most important elements with an in-depth search. Ask an expert: experts can quickly help parse what is important and what is not, but be aware of the bias any expert brings. When interviewing experts, research enough that they don\u2019t feel obligated to spend all of their time educating you on the basics\u2014you want them to really focus on the nuance that they have come to understand by becoming an expert. Write down your findings and document your sources in brief narrative format; don\u2019t waste time developing a long report. Finally, connect the elements based on their relationships. Identify central nodes, leverage points, etc. Explore the feedback loops present between factors, including both positive (reinforcing) and negative (balancing) loops. The map you create can be a good communication tool, but it is often only helpful to those who created it. To really communicate your understanding, you must tell a story for the relevant relationships you have identified. In a Situation Model, describe the key pathways from drivers to threats and impacts on targets. In a Systems Map, describe the major balancing and reinforcing feedback loops. Transformative Scenario Planning relies almost entirely on story-telling to shape understanding. If you can tell a convincing story about what is causing the problem, you will be better able to test and communicate that understanding. More on This Step \u00b6 See Thinking in Systems by Donella Meadows, Chapter 5 for a discussion on the challenges of appropriately defining scope. See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets for a general discussion of scope. Scopes are both \u201cplace-based\u201d and \u201cthematic-based\u201d.See CMP-Open Standards v3.0 section 1C-1D for more on Situation Models. See publication Building Ecosystem Services Conceptual Models (Olander et al) for helpful guidance. See Systems Practice by Omidyar Group, section Gaining Clarity . Context Mapping (Historical, Environmental, Societal/Cultural, Technological, Political) Systems Maps or Conceptual Models? \u00b6 One drawback of simple causal conceptual models typically depicted by Open Standards frameworks is that they imply single and unidirectional causalities between specific pressures and ecosystem conditions (Schwartz et al 2012; Niemeijer and de Groot 2008; Smeets and Weterings 1999). In reality, a specific pressure may affect multiple states and a specific state may be affected by multiple pressures. Instead of simple causal chain frameworks, Niemeijer and de Groot (2008) proposed that complex systems are better represented by causal networks, in which multiple causal chains interact and interconnect.","title":"Step 2: Map It"},{"location":"metrics-design/step2-map-it/#step-2-map-it","text":"Documenting your understanding of the problem space in a visual way is an impactful way of capturing your understanding and can be a good way of sharing it with others. A Situation Model , Systems Map , or Journey Map are all great tools. The Situation Model focuses on relationships between human causes (drivers) of threats to your vision (conservation targets); whereas the Systems Map describes feedback loops between relevant elements of the system. Journey Maps capture the chain of behaviors required to solve a problem or do a job. If you can, start by defining the scope of the system that you will be exploring, such as geographic boundaries or levels (e.g., federal vs. state gov\u2019t). This will help focus your search to just that area of the problem space in which you have a manageable interest . Alternatively, use a framing question to focus your interrogation of the system. Developing these maps typically requires formative research and immersion in the problem space. Start by listing the factors (i.e., threats, drivers and enabling conditions ) that you know and building out the map using a breadth-first search of the problem space. Add to the map anything that seems relevant, but don\u2019t spend too much time understanding how. Group items using affinity mapping until the relationships become clear. Next, focus on what appear to be the most important elements with an in-depth search. Ask an expert: experts can quickly help parse what is important and what is not, but be aware of the bias any expert brings. When interviewing experts, research enough that they don\u2019t feel obligated to spend all of their time educating you on the basics\u2014you want them to really focus on the nuance that they have come to understand by becoming an expert. Write down your findings and document your sources in brief narrative format; don\u2019t waste time developing a long report. Finally, connect the elements based on their relationships. Identify central nodes, leverage points, etc. Explore the feedback loops present between factors, including both positive (reinforcing) and negative (balancing) loops. The map you create can be a good communication tool, but it is often only helpful to those who created it. To really communicate your understanding, you must tell a story for the relevant relationships you have identified. In a Situation Model, describe the key pathways from drivers to threats and impacts on targets. In a Systems Map, describe the major balancing and reinforcing feedback loops. Transformative Scenario Planning relies almost entirely on story-telling to shape understanding. If you can tell a convincing story about what is causing the problem, you will be better able to test and communicate that understanding.","title":"Step 2: Map It"},{"location":"metrics-design/step2-map-it/#more-on-this-step","text":"See Thinking in Systems by Donella Meadows, Chapter 5 for a discussion on the challenges of appropriately defining scope. See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets for a general discussion of scope. Scopes are both \u201cplace-based\u201d and \u201cthematic-based\u201d.See CMP-Open Standards v3.0 section 1C-1D for more on Situation Models. See publication Building Ecosystem Services Conceptual Models (Olander et al) for helpful guidance. See Systems Practice by Omidyar Group, section Gaining Clarity . Context Mapping (Historical, Environmental, Societal/Cultural, Technological, Political)","title":"More on This Step"},{"location":"metrics-design/step2-map-it/#systems-maps-or-conceptual-models","text":"One drawback of simple causal conceptual models typically depicted by Open Standards frameworks is that they imply single and unidirectional causalities between specific pressures and ecosystem conditions (Schwartz et al 2012; Niemeijer and de Groot 2008; Smeets and Weterings 1999). In reality, a specific pressure may affect multiple states and a specific state may be affected by multiple pressures. Instead of simple causal chain frameworks, Niemeijer and de Groot (2008) proposed that complex systems are better represented by causal networks, in which multiple causal chains interact and interconnect.","title":"Systems Maps or Conceptual Models?"},{"location":"metrics-design/step3-develop-empathy/","text":"Step 3: Develop Empathy \u00b6 To really achieve the change that will bring about your vision, you must first change peoples\u2019 behavior. You can\u2019t arm rhinos with bazookas or convince sage-grouse to move to the city. The first step is to identify stakeholders\u2014all of the people that can affect or will be affected by the change you want to see. Then, you will seek to develop empathy for key stakeholders and their needs/problems. This will provide insight into a solution that is more likely to be adopted, supported, persist, and create real change. Create a stakeholder list . Search or ask around to build out your list. Consider government agencies, NGOs, academia, individual businesses, industry groups, segments of the population, etc. Group stakeholders to create a manageable list if possible, but try not to obscure the efforts of individual organizations. For example, if Molson-Coors is very active in your area of interest, keep them separate. If you're more worried about the impact of beverage companies in general, group them together. As you're building your list, capture a sentence or two about why they are relevant so you don\u2019t forget. You can categorize stakeholders based on how directly you can influence them. Stakeholders that you can directly influence are Boundary Partners . Stakeholders that you can work with directly, but can\u2019t influence, are Strategic Partners . Stakeholders that you can influence, but not directly, are your boundary partners\u2019 boundary partners. Stakeholders that are not influenceable and can\u2019t be targeted directly are not relevant (yet). Think of your boundary partners as customers, you need to \u2018sell\u2019 your solution to your boundary partners. Your strategic partners support you and provide components of your solution that you don\u2019t focus on. For each of your boundary partners (potential customers), develop a persona profile . If necessary, you can create multiple personas for a single boundary partner. For example, if you included farmers as a boundary partner, you may need to distinguish between farmers who derive their entire income from farming and hobby farmers. If you are trying to influence a very targeted set of stakeholders, you may simply research the specific people you are targeting, rather than generalized personas. In either case, the process is largely the same. The challenge is drafting personas that contain relevant facts based on truth. While short, personas must be well researched. To do persona research , you can use ethnography and psychographic research methods. Read relevant literature, studies, or surveys that have been conducted, search out their blogs, read their periodicals, imagine a day in the life . Use the framework of thinks/sees/feels/does . Understand their mental models \u2014how do they think about the problem space? Consider their primary interests which most drive their behavior. Develop persona hypotheses \u2014expectations that you have about their character, how they see the world, what their background is, etc. Conduct exploratory interviews (develop an interview guide first) to test your hypotheses (you might combine this with interviews to explore problem hypotheses at the same time, see step 4). Focus on common, not idiosyncratic, characteristics. Refine. Repeat. Don\u2019t stop until you have a clear mental picture of the persona, and you are confident that you have segmented your stakeholders into the right personas. Consider this a process, not a product. Keep updating personas as you continue to understand the problem space and your solution. Good personas are real, exact, actionable, clear, and testable ( REACT ). What do they like to do on the weekend? More on This Step \u00b6 Alex Cowan\u2019s Venture Design process is an excellent resource for persona development. See the persona development guidance for more on personas. EI has also developed an Ideal Client Persona Template for external marketing, which can be adapted. See Systems Thinking for Social Change , for more on mapping mental models into a systems map. From Our Work \u00b6 In designing the Pollinator Scorecard, we developed personas around three groups: \u2018Wingtips\u2019, \u2018Steel Toes\u2019, and \u2018Tevas\u2019. These user groups divided our target industry in a way that best reflected their uses for, and ability to use, the Pollinator Scorecard. It was helpful to both the project team in designing the Scorecard (by putting us in our users\u2019 \u2018shoes\u2019, so to speak) and when presenting the Scorecard to users to help them understand why we made the design decisions we did.","title":"Step 3: Develop Empathy"},{"location":"metrics-design/step3-develop-empathy/#step-3-develop-empathy","text":"To really achieve the change that will bring about your vision, you must first change peoples\u2019 behavior. You can\u2019t arm rhinos with bazookas or convince sage-grouse to move to the city. The first step is to identify stakeholders\u2014all of the people that can affect or will be affected by the change you want to see. Then, you will seek to develop empathy for key stakeholders and their needs/problems. This will provide insight into a solution that is more likely to be adopted, supported, persist, and create real change. Create a stakeholder list . Search or ask around to build out your list. Consider government agencies, NGOs, academia, individual businesses, industry groups, segments of the population, etc. Group stakeholders to create a manageable list if possible, but try not to obscure the efforts of individual organizations. For example, if Molson-Coors is very active in your area of interest, keep them separate. If you're more worried about the impact of beverage companies in general, group them together. As you're building your list, capture a sentence or two about why they are relevant so you don\u2019t forget. You can categorize stakeholders based on how directly you can influence them. Stakeholders that you can directly influence are Boundary Partners . Stakeholders that you can work with directly, but can\u2019t influence, are Strategic Partners . Stakeholders that you can influence, but not directly, are your boundary partners\u2019 boundary partners. Stakeholders that are not influenceable and can\u2019t be targeted directly are not relevant (yet). Think of your boundary partners as customers, you need to \u2018sell\u2019 your solution to your boundary partners. Your strategic partners support you and provide components of your solution that you don\u2019t focus on. For each of your boundary partners (potential customers), develop a persona profile . If necessary, you can create multiple personas for a single boundary partner. For example, if you included farmers as a boundary partner, you may need to distinguish between farmers who derive their entire income from farming and hobby farmers. If you are trying to influence a very targeted set of stakeholders, you may simply research the specific people you are targeting, rather than generalized personas. In either case, the process is largely the same. The challenge is drafting personas that contain relevant facts based on truth. While short, personas must be well researched. To do persona research , you can use ethnography and psychographic research methods. Read relevant literature, studies, or surveys that have been conducted, search out their blogs, read their periodicals, imagine a day in the life . Use the framework of thinks/sees/feels/does . Understand their mental models \u2014how do they think about the problem space? Consider their primary interests which most drive their behavior. Develop persona hypotheses \u2014expectations that you have about their character, how they see the world, what their background is, etc. Conduct exploratory interviews (develop an interview guide first) to test your hypotheses (you might combine this with interviews to explore problem hypotheses at the same time, see step 4). Focus on common, not idiosyncratic, characteristics. Refine. Repeat. Don\u2019t stop until you have a clear mental picture of the persona, and you are confident that you have segmented your stakeholders into the right personas. Consider this a process, not a product. Keep updating personas as you continue to understand the problem space and your solution. Good personas are real, exact, actionable, clear, and testable ( REACT ). What do they like to do on the weekend?","title":"Step 3: Develop Empathy"},{"location":"metrics-design/step3-develop-empathy/#more-on-this-step","text":"Alex Cowan\u2019s Venture Design process is an excellent resource for persona development. See the persona development guidance for more on personas. EI has also developed an Ideal Client Persona Template for external marketing, which can be adapted. See Systems Thinking for Social Change , for more on mapping mental models into a systems map.","title":"More on This Step"},{"location":"metrics-design/step3-develop-empathy/#from-our-work","text":"In designing the Pollinator Scorecard, we developed personas around three groups: \u2018Wingtips\u2019, \u2018Steel Toes\u2019, and \u2018Tevas\u2019. These user groups divided our target industry in a way that best reflected their uses for, and ability to use, the Pollinator Scorecard. It was helpful to both the project team in designing the Scorecard (by putting us in our users\u2019 \u2018shoes\u2019, so to speak) and when presenting the Scorecard to users to help them understand why we made the design decisions we did.","title":"From Our Work"},{"location":"metrics-design/step4-define-the-problem/","text":"Step 4: Define the Problem \u00b6 \u201cGiven one hour to save the planet, I would spend 59 minutes understanding the problem and one minute resolving it.\u201d - Attributed to Albert Einstein (probably incorrectly) You may have started this process thinking you know what the problem is, or what strategy you\u2019ll employ. Do you still? It will do no good to solve the wrong problem; it might actually do more harm! There are many examples of well-meaning groups attempting to solve a problem with their \u2018cure-all\u2019 and seen it fall flat--or worse, exacerbate existing problems. To a hammer, the whole world's a nail \u00b6 Many of the frameworks that inspired this approach suggest defining your group's mission at the outset. We prefer to be fully agnostic as to the solution until the problem is clearly defined and evidence supports your understanding. We recognize that your group has unique strengths and experience with some solutions more than others. If the problem doesn't call for your solution, that's fine, you can go solve a different one. The temptation is simply too great to frame the problem in terms of your solution if you come with preconceptions as to what your strategy will be. So how do we define a problem and develop our solution? Let\u2019s go back to our map. If you\u2019ve developed a systems map, look for leverage points . If you\u2019ve developed a situation model, identify key intervention points --which factor will you focus on? If you haven\u2019t already, map other stakeholders\u2019 interventions onto your map. Are there factors that aren\u2019t receiving enough attention? Where could other interventions be leveraged? Don\u2019t worry about the how (i.e., your strategy) yet. Once you\u2019ve identified the factor you\u2019d like to address, review your stakeholder list. Which of your boundary partners are relevant to this factor? Refer back to their personas and describe the problem scenarios that they face relevant to your understanding of the situation scope. These can be either problems or simply jobs to be done: Farmers aren\u2019t sure where to find information on conservation programs. Agency staff need to develop restoration project specifications but aren\u2019t experts in restoration. CSR managers need to meet biodiversity targets but don\u2019t have time to develop habitat projects. You get the picture. Depending on how broad your scope, this task may seem nebulous. Here are a few options to help clarify: For each persona, draft an outcome challenge --the ideal behavior you want each persona to adopt in a world in which your vision is achieved. You can break this down into progress markers by defining what you would expect to see, like to see, and love to see from each persona. Create a journey map . A journey map illustrates the steps the persona takes to complete a relevant task or solve a relevant problem. For example, if you envision restaurants sustainably sourcing ingredients, map out a typical process for creating procurement policies. Where do they begin? Who else is involved? Within the journey map, highlight pain points --areas where the process is difficult. Those pain points may just turn into the problems you choose to tackle. A behavior chain is similar to a journey map, and outlines the unique, self-contained behaviors necessary to complete a more complex behavior. To identify problems that are actually solvable, consider organizing them as parent, anchor and child scenarios. The parent problem is too big; the child scenario to small. The anchor problem , just right. For each anchor problem, consider alternatives --what is the persona doing now instead of what you want them to do; what other options currently exist that they\u2019re not using? Understanding the alternatives is necessary to understand the baseline and to specify conditions of satisfaction --in other words, how you\u2019ll know your solution is sufficiently better than the alternatives that the user will consider it. Craft problem hypotheses for the problems you most want to address or think are most important. These testable propositions will be evaluated through persona interviews or focus groups (you may want to combine this step with the persona interviews in step 3). Before moving on to the next step, where you will identify potential strategies, it is critical that you have a clear understanding of the problem space and have evidence supporting your hypotheses. By now you should have talked to experts and interviewed at least 5 relevant people. For more substantial endeavors, shoot for 30 people. There is no guarantee that this process will deliver a solution that will help you reach your vision for the world, but the chances of having an impact in a system you don\u2019t understand with people you can\u2019t relate to are slim. Research all at once? \u00b6 To maximize the efficiency of your research process, and avoid having to interview people twice, you may want to hold off on interviews until you've developed both persona hypotheses and problem hypotheses. If you have been working in this space for some time and have accumulated sufficient evidence that you are comfortable proposing a solution at this time, you can even hold off on interviews until the next step, once you have a proposal for a solution.","title":"Step 4: Define the Problem"},{"location":"metrics-design/step4-define-the-problem/#step-4-define-the-problem","text":"\u201cGiven one hour to save the planet, I would spend 59 minutes understanding the problem and one minute resolving it.\u201d - Attributed to Albert Einstein (probably incorrectly) You may have started this process thinking you know what the problem is, or what strategy you\u2019ll employ. Do you still? It will do no good to solve the wrong problem; it might actually do more harm! There are many examples of well-meaning groups attempting to solve a problem with their \u2018cure-all\u2019 and seen it fall flat--or worse, exacerbate existing problems.","title":"Step 4: Define the Problem"},{"location":"metrics-design/step4-define-the-problem/#to-a-hammer-the-whole-worlds-a-nail","text":"Many of the frameworks that inspired this approach suggest defining your group's mission at the outset. We prefer to be fully agnostic as to the solution until the problem is clearly defined and evidence supports your understanding. We recognize that your group has unique strengths and experience with some solutions more than others. If the problem doesn't call for your solution, that's fine, you can go solve a different one. The temptation is simply too great to frame the problem in terms of your solution if you come with preconceptions as to what your strategy will be. So how do we define a problem and develop our solution? Let\u2019s go back to our map. If you\u2019ve developed a systems map, look for leverage points . If you\u2019ve developed a situation model, identify key intervention points --which factor will you focus on? If you haven\u2019t already, map other stakeholders\u2019 interventions onto your map. Are there factors that aren\u2019t receiving enough attention? Where could other interventions be leveraged? Don\u2019t worry about the how (i.e., your strategy) yet. Once you\u2019ve identified the factor you\u2019d like to address, review your stakeholder list. Which of your boundary partners are relevant to this factor? Refer back to their personas and describe the problem scenarios that they face relevant to your understanding of the situation scope. These can be either problems or simply jobs to be done: Farmers aren\u2019t sure where to find information on conservation programs. Agency staff need to develop restoration project specifications but aren\u2019t experts in restoration. CSR managers need to meet biodiversity targets but don\u2019t have time to develop habitat projects. You get the picture. Depending on how broad your scope, this task may seem nebulous. Here are a few options to help clarify: For each persona, draft an outcome challenge --the ideal behavior you want each persona to adopt in a world in which your vision is achieved. You can break this down into progress markers by defining what you would expect to see, like to see, and love to see from each persona. Create a journey map . A journey map illustrates the steps the persona takes to complete a relevant task or solve a relevant problem. For example, if you envision restaurants sustainably sourcing ingredients, map out a typical process for creating procurement policies. Where do they begin? Who else is involved? Within the journey map, highlight pain points --areas where the process is difficult. Those pain points may just turn into the problems you choose to tackle. A behavior chain is similar to a journey map, and outlines the unique, self-contained behaviors necessary to complete a more complex behavior. To identify problems that are actually solvable, consider organizing them as parent, anchor and child scenarios. The parent problem is too big; the child scenario to small. The anchor problem , just right. For each anchor problem, consider alternatives --what is the persona doing now instead of what you want them to do; what other options currently exist that they\u2019re not using? Understanding the alternatives is necessary to understand the baseline and to specify conditions of satisfaction --in other words, how you\u2019ll know your solution is sufficiently better than the alternatives that the user will consider it. Craft problem hypotheses for the problems you most want to address or think are most important. These testable propositions will be evaluated through persona interviews or focus groups (you may want to combine this step with the persona interviews in step 3). Before moving on to the next step, where you will identify potential strategies, it is critical that you have a clear understanding of the problem space and have evidence supporting your hypotheses. By now you should have talked to experts and interviewed at least 5 relevant people. For more substantial endeavors, shoot for 30 people. There is no guarantee that this process will deliver a solution that will help you reach your vision for the world, but the chances of having an impact in a system you don\u2019t understand with people you can\u2019t relate to are slim.","title":"To a hammer, the whole world's a nail"},{"location":"metrics-design/step4-define-the-problem/#research-all-at-once","text":"To maximize the efficiency of your research process, and avoid having to interview people twice, you may want to hold off on interviews until you've developed both persona hypotheses and problem hypotheses. If you have been working in this space for some time and have accumulated sufficient evidence that you are comfortable proposing a solution at this time, you can even hold off on interviews until the next step, once you have a proposal for a solution.","title":"Research all at once?"},{"location":"metrics-design/step5-identify-a-solution/","text":"Step 5: Identify a Solution \u00b6 This is the fun part! In terms of the design \u2018 double diamond ,\u2019 we are at the central convergence point. Starting from our vision, we\u2019ve diverged to explore the problem space and its stakeholders, and have since converged around a problem worth solving. Now, we get to come up with a solution. Call it brainstorming or ideation or, my personal favorite, spitballing . Just don\u2019t take it too seriously and don\u2019t do it alone. There are a plethora of techniques for working in groups to create solutions. Can you invest in a weeklong design sprint ? If not, try this one day version. Use sketching to help you visualize the solution. Timed idea generation , mind mapping , storyboarding , journey maps are all great options to get a creative buzz on. Reframe your problem scenarios into opportunities by asking \u2018 how might we? \u2019. Think about the problem from multiple angles. Create frameworks to help you see the problem with a different lens. Use many model thinking . Consider both extremes and mainstreams . Try coming up with your worst idea . Look at similar projects or other people\u2019s interventions and describe their challenges, insights, and opportunities . Consider the problem and define success --what would a good solution look like? You\u2019re looking for a solution that is all three: desirable, feasible, viable . Ideally one that also gets people in your group excited and overlaps with your unique strengths . Once you\u2019ve arrived at a potential conceptual solution, it\u2019s time to put it to the test. For the people for whom you\u2019re trying to solve this problem, craft a value proposition for each of the problem scenarios you\u2019ve already defined. Also include the current alternative so you know what you\u2019re competing against. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Does the solution appear to be better than the alternative? How do you know? Do you need to test the value proposition to know for sure? Craft value hypotheses for each value proposition that should be tested. You can state a value hypothesis as \u201cIf we do X for Y, they will Z\u201d. Make sure the behavior you want to see is observable. Some value hypotheses can be tested without building any of the proposed solution, others will require a minimum viable product to test--we\u2019ll work on that in the next step. Finally, it\u2019s good practice to summarize the value of your solution in a positioning statement . Fill in the blanks: For [persona A] who need to [problem scenario], our [solution name] is a [product category] that [value proposition]. Unlike [current alternative], our product [key differentiation]. Draft one for each of your primary personas. Now is also the time to develop a Results Chain to clarify assumptions as to why solving this problem will improve the conservation target and identify metrics for evaluating progress.","title":"Step 5: Identify a Solution"},{"location":"metrics-design/step5-identify-a-solution/#step-5-identify-a-solution","text":"This is the fun part! In terms of the design \u2018 double diamond ,\u2019 we are at the central convergence point. Starting from our vision, we\u2019ve diverged to explore the problem space and its stakeholders, and have since converged around a problem worth solving. Now, we get to come up with a solution. Call it brainstorming or ideation or, my personal favorite, spitballing . Just don\u2019t take it too seriously and don\u2019t do it alone. There are a plethora of techniques for working in groups to create solutions. Can you invest in a weeklong design sprint ? If not, try this one day version. Use sketching to help you visualize the solution. Timed idea generation , mind mapping , storyboarding , journey maps are all great options to get a creative buzz on. Reframe your problem scenarios into opportunities by asking \u2018 how might we? \u2019. Think about the problem from multiple angles. Create frameworks to help you see the problem with a different lens. Use many model thinking . Consider both extremes and mainstreams . Try coming up with your worst idea . Look at similar projects or other people\u2019s interventions and describe their challenges, insights, and opportunities . Consider the problem and define success --what would a good solution look like? You\u2019re looking for a solution that is all three: desirable, feasible, viable . Ideally one that also gets people in your group excited and overlaps with your unique strengths . Once you\u2019ve arrived at a potential conceptual solution, it\u2019s time to put it to the test. For the people for whom you\u2019re trying to solve this problem, craft a value proposition for each of the problem scenarios you\u2019ve already defined. Also include the current alternative so you know what you\u2019re competing against. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Does the solution appear to be better than the alternative? How do you know? Do you need to test the value proposition to know for sure? Craft value hypotheses for each value proposition that should be tested. You can state a value hypothesis as \u201cIf we do X for Y, they will Z\u201d. Make sure the behavior you want to see is observable. Some value hypotheses can be tested without building any of the proposed solution, others will require a minimum viable product to test--we\u2019ll work on that in the next step. Finally, it\u2019s good practice to summarize the value of your solution in a positioning statement . Fill in the blanks: For [persona A] who need to [problem scenario], our [solution name] is a [product category] that [value proposition]. Unlike [current alternative], our product [key differentiation]. Draft one for each of your primary personas. Now is also the time to develop a Results Chain to clarify assumptions as to why solving this problem will improve the conservation target and identify metrics for evaluating progress.","title":"Step 5: Identify a Solution"},{"location":"metrics-design/step6-implement/","text":"Coming soon... \u00b6","title":"Step 6: Implement"},{"location":"metrics-design/step6-implement/#coming-soon","text":"","title":"Coming soon..."},{"location":"metrics-design/step7-adapt/","text":"Coming later... \u00b6","title":"Step 7: Iterate & Adapt"},{"location":"metrics-design/step7-adapt/#coming-later","text":"","title":"Coming later..."},{"location":"metrics-services/how-we-work/","text":"We've refined our metrics design philosophy to a simple, scalable process to support you in the design and development of your tools and programs. Whether you've identified a new need within an existing project, are setting expectations with a client for a new tool, or simply replacing an old tool with a new one, we can help. Expect to meet at least three times as we work to understand the problem, define a solution, and develop a prototype. Our role can be minimal or more involved, from simply helping you think through the important details to building and supporting the product over the long term. Our process can be scaled to fit your needs large or small, supporting projects including: Building an internal tool Conducting a data analysis Creating a map, data visualization or dashboard Evaluating options for a technology solution Scoping a tool as a product/service for you practice area Developing a prototype prior to working with a technology partner Interfacing with technology partners Building a client-facing tool as a deliverable We'll work with you through the process of scoping , product definition , specification , implementation and adaptation . What's described below is optimized for developing a medium- to large-effort tool, but will be similar, though less involved, for lesser engagements. Scoping \u00b6 Scoping is all about understanding the situation and exploring the problem. We often start by asking you to describe your vision. If its our first time working on this program, we'd like to know your vision for the program, your conservation targets, etc. If we've already engaged, then we'll focus on your vision for the world in which this solution exists. Understanding your vision helps us know what a successful solution will look like. If that's too optimistic for you, we can instead focus on the problems you face, decisions you need to make, or jobs to be done. We'll have you list the problems and needs so we can zero in on the key problems, who is involved, and the overall situation. While you're describing the vision or problem, we'll be listening for three things. First, the people involved. Who are the users and what are their roles? Who are the stakeholders (those affected by the tool)? What are the users' capabilities? The list of people will be refined to a list of users and persona descriptions that describe their motivations and constraints. Second, their problems or needs. Which can be addressed? What are people doing instead? How will our solution improve upon the alternatives? Problems will be catalogued and we'll ultimately seek to identify the problem, or set of problems, that our solution will address first. Finally, context. How will this solution integrate with existing tools or processes? What are the requirements, constraints, and design considerations we must keep in mind? If we come up with questions you don't have an answer for, its helpful to have an expert (whether a true expert or just another member of the project team) available to get clarity before we move on. While we're not committing to a particular solution yet, we will want to work with you to identify priority problems to address by asking 'How Might We?'. This question allows us to transform problems into opportunities and get creative about potential solutions. After this meeting, we'll refine our understanding of what you told us, develop testable hypotheses for people and problems, and craft a research plan. We'll do background research to fill in gaps. We may even interview potential users to confirm our persona hypotheses and problem hypotheses. Product Definition \u00b6 Utilizing our research and what you've told us, we'll develop options for solutions and develop a recommendation. We'll return with a Product Definition and, potentially, a prototype - some tangible way for you to understand the recommended solution. The prototype is the minimum build sufficient to test key hypotheses we've developed. It may not even function at all (e.g., a 'wizard of oz' approach), but will simulate the experience of using the tool. It might be as simple as a drawing, a wireframe, or even a small version of what we'll eventually develop. We find it's better to have something tangible for you to react to so that you are fully aware of the look/feel and limitations of any solution. Also, we may not be able to solve all of the problems (or meet all of the needs) all at once, if not, we'll pick an 'anchor problem' and start there. We'll keep track of other problems to be solved in future features. This is your chance to give us direction on the scope and solution. Did we overestimate the users capabilities? Forget a key requirement? It's our job to make sure you fully understand the implications of the proposed solution - who will manage it, what is its expected lifetime, what will it cost, etc. We'll also review roles, timeline, and necessary resources. Specification \u00b6 We'll document the specifications for the solution in our template Specifications Document , create our project plan and roadmap, and develop the solution or support you to get the product built with a technology provider. Before working with a technology provider, we may develop a simple prototype that will allow you to test and interact with the product so you can be very clear about what you want and how you want it to work. Implementation \u00b6 Implementation - the development and deployment of the solution- is simply iteration. We strongly recommend implementing the smallest workable subset of a solution possible. Don't wait for the perfect build out before putting it in the hands of your users. Also, don't overlook the importance of outreach to and education of both users and stakeholders. We can help you navigate the cycle of building and testing that is central to solution implementation. Adaptation \u00b6 Almost every tool requires some revision in the first year. We can provide support for the first year and recommend a structured process for adapting the product as needed after a full year of use. The adaptation process should, of course, be scaled to the size of the tool. Why have we developed this process? \u00b6 A common component of the delivery of metrics services is the design of tools. We have developed over the past few years a unique perspective and approach to developing tools to support metrics projects. Our tools can include a technology component (such as a scripting language like R or Python), but many do not. The design and development of tools, whether technological or not, is a unique skillset developed within the Metrics service line over the past few years. We have built out processes and products to facilitate the efficient development of tools that can be used both internally and externally to improve EI services and internal processes. In addition, some EI staff not explicitly included in the Metrics service line have also been developing skills and experience in overlapping areas (e.g., human-centered design). Our aim is to build EI's capacity for effective tool development by promoting the processes and products we have while incorporating the learning of other EI staff in those products and processes. We are not pretending to be a tech company when we are not, nor develop a competency in the development of software or hardware. EI will continue to work with technology providers to deliver robust technology solutions. Instead, this is an effort to describe and promote, internally and externally, a subset of the Metrics service line's skillset that is unique to Metrics service line experts. Tool design is central to this, but it can also include data analytics and visualization as stand-alone services. We also explore the opportunity and requirements to deliver Metrics services outside the typical program design and implementation package to new clients/markets, to existing clients, and to internal initiatives. The Metrics service line can support an effort as small as designing a new budget template to as large as developing an entire information management system for a large agency. We leverage our skills in consulting, design, development, and deployment created through past internal and external work. We work to improve EI's internal practices, expand our staff's understanding of what is possible, and deliver robust, user-friendly tools to clients. Additionally, we seek to increase the capacity of all EI staff to develop lasting solutions.","title":"Metrics Services"},{"location":"metrics-services/how-we-work/#scoping","text":"Scoping is all about understanding the situation and exploring the problem. We often start by asking you to describe your vision. If its our first time working on this program, we'd like to know your vision for the program, your conservation targets, etc. If we've already engaged, then we'll focus on your vision for the world in which this solution exists. Understanding your vision helps us know what a successful solution will look like. If that's too optimistic for you, we can instead focus on the problems you face, decisions you need to make, or jobs to be done. We'll have you list the problems and needs so we can zero in on the key problems, who is involved, and the overall situation. While you're describing the vision or problem, we'll be listening for three things. First, the people involved. Who are the users and what are their roles? Who are the stakeholders (those affected by the tool)? What are the users' capabilities? The list of people will be refined to a list of users and persona descriptions that describe their motivations and constraints. Second, their problems or needs. Which can be addressed? What are people doing instead? How will our solution improve upon the alternatives? Problems will be catalogued and we'll ultimately seek to identify the problem, or set of problems, that our solution will address first. Finally, context. How will this solution integrate with existing tools or processes? What are the requirements, constraints, and design considerations we must keep in mind? If we come up with questions you don't have an answer for, its helpful to have an expert (whether a true expert or just another member of the project team) available to get clarity before we move on. While we're not committing to a particular solution yet, we will want to work with you to identify priority problems to address by asking 'How Might We?'. This question allows us to transform problems into opportunities and get creative about potential solutions. After this meeting, we'll refine our understanding of what you told us, develop testable hypotheses for people and problems, and craft a research plan. We'll do background research to fill in gaps. We may even interview potential users to confirm our persona hypotheses and problem hypotheses.","title":"Scoping"},{"location":"metrics-services/how-we-work/#product-definition","text":"Utilizing our research and what you've told us, we'll develop options for solutions and develop a recommendation. We'll return with a Product Definition and, potentially, a prototype - some tangible way for you to understand the recommended solution. The prototype is the minimum build sufficient to test key hypotheses we've developed. It may not even function at all (e.g., a 'wizard of oz' approach), but will simulate the experience of using the tool. It might be as simple as a drawing, a wireframe, or even a small version of what we'll eventually develop. We find it's better to have something tangible for you to react to so that you are fully aware of the look/feel and limitations of any solution. Also, we may not be able to solve all of the problems (or meet all of the needs) all at once, if not, we'll pick an 'anchor problem' and start there. We'll keep track of other problems to be solved in future features. This is your chance to give us direction on the scope and solution. Did we overestimate the users capabilities? Forget a key requirement? It's our job to make sure you fully understand the implications of the proposed solution - who will manage it, what is its expected lifetime, what will it cost, etc. We'll also review roles, timeline, and necessary resources.","title":"Product Definition"},{"location":"metrics-services/how-we-work/#specification","text":"We'll document the specifications for the solution in our template Specifications Document , create our project plan and roadmap, and develop the solution or support you to get the product built with a technology provider. Before working with a technology provider, we may develop a simple prototype that will allow you to test and interact with the product so you can be very clear about what you want and how you want it to work.","title":"Specification"},{"location":"metrics-services/how-we-work/#implementation","text":"Implementation - the development and deployment of the solution- is simply iteration. We strongly recommend implementing the smallest workable subset of a solution possible. Don't wait for the perfect build out before putting it in the hands of your users. Also, don't overlook the importance of outreach to and education of both users and stakeholders. We can help you navigate the cycle of building and testing that is central to solution implementation.","title":"Implementation"},{"location":"metrics-services/how-we-work/#adaptation","text":"Almost every tool requires some revision in the first year. We can provide support for the first year and recommend a structured process for adapting the product as needed after a full year of use. The adaptation process should, of course, be scaled to the size of the tool.","title":"Adaptation"},{"location":"metrics-services/how-we-work/#why-have-we-developed-this-process","text":"A common component of the delivery of metrics services is the design of tools. We have developed over the past few years a unique perspective and approach to developing tools to support metrics projects. Our tools can include a technology component (such as a scripting language like R or Python), but many do not. The design and development of tools, whether technological or not, is a unique skillset developed within the Metrics service line over the past few years. We have built out processes and products to facilitate the efficient development of tools that can be used both internally and externally to improve EI services and internal processes. In addition, some EI staff not explicitly included in the Metrics service line have also been developing skills and experience in overlapping areas (e.g., human-centered design). Our aim is to build EI's capacity for effective tool development by promoting the processes and products we have while incorporating the learning of other EI staff in those products and processes. We are not pretending to be a tech company when we are not, nor develop a competency in the development of software or hardware. EI will continue to work with technology providers to deliver robust technology solutions. Instead, this is an effort to describe and promote, internally and externally, a subset of the Metrics service line's skillset that is unique to Metrics service line experts. Tool design is central to this, but it can also include data analytics and visualization as stand-alone services. We also explore the opportunity and requirements to deliver Metrics services outside the typical program design and implementation package to new clients/markets, to existing clients, and to internal initiatives. The Metrics service line can support an effort as small as designing a new budget template to as large as developing an entire information management system for a large agency. We leverage our skills in consulting, design, development, and deployment created through past internal and external work. We work to improve EI's internal practices, expand our staff's understanding of what is possible, and deliver robust, user-friendly tools to clients. Additionally, we seek to increase the capacity of all EI staff to develop lasting solutions.","title":"Why have we developed this process?"},{"location":"portfolio/project-portfolio/","text":"Project Portfolio \u00b6 These are a few of our projects. Peruse at your leisure. Sustainable WASH Systems Monitoring Plan Visuals \u00b6 We worked with the Sustainable WASH Systems team to explore, analyze and create data visualizations for Performance Indicators included in their Monitoring, Evaluation and Learning (MEL) Plan report. Monarch Registry \u00b6 We worked with a web design firm to develop the Monarch Registry for the Monarch Habitat Exchange. Our role was to design the database, identify reporting metrics, and provide input on visual aesthetic. # custom web app, registry, database Nevada Credit System Site Screening Tool \u00b6 The Nevada Site Screening Tool allows landowners to quickly screen their private lands for potential to participate in the Nevada Credit System based on a number of factors relevant to sage-grouse habitat quality. We partnered with Sitka to develop the web app, its algorithms, and outputs. # custom web app, spatial analysis Roadside Managers Monarch Tool \u00b6 We partnered with UX design students to design an interface for a mobile field data collection app for roadside managers to quickly assess the quality of monarch habitat on roadsides. The design was used to spur interest from funders and as the basis for the final mobile field data collection app design, which was build on ESRI's Survey123. # UX design, Survey123, mobile app Pollinator Habitat Quantification Tool \u00b6 The Pollinator Habitat Quantification Tool evaluates habitat quality in agricultural landscapes for a user-specified list of native pollinators. The tool implements an algorithm described here . The tool is presented in this ESRI StoryMap , which can be viewed below (scroll). # ESRI StoryMap, ArcGIS Pollinator Scorecard \u00b6 The Pollinator Scorecard is designed for electric utilities and other rights-of-way managers to rapidly assess pollinator and monarch habitat on their rights-of-way. It is the required assessment method for the USFWS Monarch Candidate Conservation Agreement with Assurances. Data may be collected through a fillable PDF form or ESRI's Survey123 app. # Low-tech, PDF, Survey123, field protocol, datasheets San Diego Incentives Demographics \u00b6 We created a mapping application to inform the design of rebates and incentives program in San Diego County by evaluating basic demographic information. # ESRI StoryMap, Spatial Analysis, Mapping Monarch Habitat Quantification Tool \u00b6 The Monarch Habitat Quantification Tool is designed to assess monarch habitat quality in agricultural landscapes. We led a team of monarch experts to develop an assessment protocol and habitat quantification approach for EDF. The tool includes an Excel-based spreadsheet calculator, field datasheets, and field data collection protocol. # Excel, Monitoring Protocol, Datasheets Sage-Steppe Habitat Quantification Tool \u00b6 The Sage-Steppe Habitat Quantification Tool facilitates habitat assessment for greater sage-grouse and mule deer in multiple states around the West. Multiple state mitigation programs use a regionalized version of the tool. The tool has been used to inform conservation spending and mitigation of over $1 million (USD). The tool consists of a set of custom script tools using ESRI's ArcGIS API, arcpy and Excel spreadsheet calculators. # ArcGIS, arcpy, python, spatial analysis, Excel, VBA, Excel forms Idaho Greater-Sage Grouse Project Calculator \u00b6 As part of the Sage-Steppe Habitat Quantification Tool, we developed a custom Excel form with VBA scripting to facilitate data entry into the Project Calculator. # Excel, VBA, Excel Forms Aliso Creek Results Chain Primer \u00b6 The Water Team developed a great introduction to Results Chains that can be tailored to any audience. Check it out below, or download here . # Open Standards, Results Chain, Performance Measures Performance Measure Design \u00b6 This slide deck introduces a five step process to performance measure design. It's a useful resource when introducing performance measures to new clients or developing proposals. # Open Standards, Results Chains, Performance Measures Qualitative Monitoring Dashboard \u00b6 The Qualitative Monitoring Dashboard reports progress markers for sustainable WASH (water, sanitation and hygiene) systems in Africa. The interactive dashboard allows users to select their municipality and view qualitative indicators and a sustainability scorecard. The dashboard was developed using InDesign before converting to HTML. It is regularly updated and hosted on the EI website (Note it contains sensitive information and is thus password protected). # dashboard, PDF, InDesign, qualitative indicators Central Valley Multi-Species HQT \u00b6 The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes. The HQT scoring for giant garter-snake is broken down in the diagram below. # Excel, field protocol","title":"Portfolio"},{"location":"portfolio/project-portfolio/#project-portfolio","text":"These are a few of our projects. Peruse at your leisure.","title":"Project Portfolio"},{"location":"portfolio/project-portfolio/#sustainable-wash-systems-monitoring-plan-visuals","text":"We worked with the Sustainable WASH Systems team to explore, analyze and create data visualizations for Performance Indicators included in their Monitoring, Evaluation and Learning (MEL) Plan report.","title":"Sustainable WASH Systems Monitoring Plan Visuals"},{"location":"portfolio/project-portfolio/#monarch-registry","text":"We worked with a web design firm to develop the Monarch Registry for the Monarch Habitat Exchange. Our role was to design the database, identify reporting metrics, and provide input on visual aesthetic. # custom web app, registry, database","title":"Monarch Registry"},{"location":"portfolio/project-portfolio/#nevada-credit-system-site-screening-tool","text":"The Nevada Site Screening Tool allows landowners to quickly screen their private lands for potential to participate in the Nevada Credit System based on a number of factors relevant to sage-grouse habitat quality. We partnered with Sitka to develop the web app, its algorithms, and outputs. # custom web app, spatial analysis","title":"Nevada Credit System Site Screening Tool"},{"location":"portfolio/project-portfolio/#roadside-managers-monarch-tool","text":"We partnered with UX design students to design an interface for a mobile field data collection app for roadside managers to quickly assess the quality of monarch habitat on roadsides. The design was used to spur interest from funders and as the basis for the final mobile field data collection app design, which was build on ESRI's Survey123. # UX design, Survey123, mobile app","title":"Roadside Managers Monarch Tool"},{"location":"portfolio/project-portfolio/#pollinator-habitat-quantification-tool","text":"The Pollinator Habitat Quantification Tool evaluates habitat quality in agricultural landscapes for a user-specified list of native pollinators. The tool implements an algorithm described here . The tool is presented in this ESRI StoryMap , which can be viewed below (scroll). # ESRI StoryMap, ArcGIS","title":"Pollinator Habitat Quantification Tool"},{"location":"portfolio/project-portfolio/#pollinator-scorecard","text":"The Pollinator Scorecard is designed for electric utilities and other rights-of-way managers to rapidly assess pollinator and monarch habitat on their rights-of-way. It is the required assessment method for the USFWS Monarch Candidate Conservation Agreement with Assurances. Data may be collected through a fillable PDF form or ESRI's Survey123 app. # Low-tech, PDF, Survey123, field protocol, datasheets","title":"Pollinator Scorecard"},{"location":"portfolio/project-portfolio/#san-diego-incentives-demographics","text":"We created a mapping application to inform the design of rebates and incentives program in San Diego County by evaluating basic demographic information. # ESRI StoryMap, Spatial Analysis, Mapping","title":"San Diego Incentives Demographics"},{"location":"portfolio/project-portfolio/#monarch-habitat-quantification-tool","text":"The Monarch Habitat Quantification Tool is designed to assess monarch habitat quality in agricultural landscapes. We led a team of monarch experts to develop an assessment protocol and habitat quantification approach for EDF. The tool includes an Excel-based spreadsheet calculator, field datasheets, and field data collection protocol. # Excel, Monitoring Protocol, Datasheets","title":"Monarch Habitat Quantification Tool"},{"location":"portfolio/project-portfolio/#sage-steppe-habitat-quantification-tool","text":"The Sage-Steppe Habitat Quantification Tool facilitates habitat assessment for greater sage-grouse and mule deer in multiple states around the West. Multiple state mitigation programs use a regionalized version of the tool. The tool has been used to inform conservation spending and mitigation of over $1 million (USD). The tool consists of a set of custom script tools using ESRI's ArcGIS API, arcpy and Excel spreadsheet calculators. # ArcGIS, arcpy, python, spatial analysis, Excel, VBA, Excel forms","title":"Sage-Steppe Habitat Quantification Tool"},{"location":"portfolio/project-portfolio/#idaho-greater-sage-grouse-project-calculator","text":"As part of the Sage-Steppe Habitat Quantification Tool, we developed a custom Excel form with VBA scripting to facilitate data entry into the Project Calculator. # Excel, VBA, Excel Forms","title":"Idaho Greater-Sage Grouse Project Calculator"},{"location":"portfolio/project-portfolio/#aliso-creek-results-chain-primer","text":"The Water Team developed a great introduction to Results Chains that can be tailored to any audience. Check it out below, or download here . # Open Standards, Results Chain, Performance Measures","title":"Aliso Creek Results Chain Primer"},{"location":"portfolio/project-portfolio/#performance-measure-design","text":"This slide deck introduces a five step process to performance measure design. It's a useful resource when introducing performance measures to new clients or developing proposals. # Open Standards, Results Chains, Performance Measures","title":"Performance Measure Design"},{"location":"portfolio/project-portfolio/#qualitative-monitoring-dashboard","text":"The Qualitative Monitoring Dashboard reports progress markers for sustainable WASH (water, sanitation and hygiene) systems in Africa. The interactive dashboard allows users to select their municipality and view qualitative indicators and a sustainability scorecard. The dashboard was developed using InDesign before converting to HTML. It is regularly updated and hosted on the EI website (Note it contains sensitive information and is thus password protected). # dashboard, PDF, InDesign, qualitative indicators","title":"Qualitative Monitoring Dashboard"},{"location":"portfolio/project-portfolio/#central-valley-multi-species-hqt","text":"The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes. The HQT scoring for giant garter-snake is broken down in the diagram below. # Excel, field protocol","title":"Central Valley Multi-Species HQT"},{"location":"portfolio/ggs-hqt/ggs-hqt/","text":"Giant Garter Snake Habitat Quantification Tool \u00b6 Excel-based Tool for Understanding Habitat Functionality \u00b6 The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes. Giant Garter Snakes \u00b6 Giant garter snakes were included in the original tool. This species, once common in the seasonal wetlands of the Central Valley, has seen drastic habitat reductions and is now listed as endangered under the Endangered Species Act and the California Endangered Species Act. These snakes now live in agricultural canals and rice fields, and are often impacted by standard maintenance activities, like dredging canals or grading levees. Because this species relies on human-created ecosystems and is heavily regulated by federal and state wildlife agencies, there is a great need to understand their habitat, be able to compare habitat sites, and demonstrate improvement of habitat for the snake. Benefits of the HQT \u00b6 The giant garter snake HQT was developed by Environmental Defense Fund and a Technical Advisory Committee of giant garter snake experts. EI helped to ensure the tool was specifically designed to be used by conservation organizations, agencies, and technically savvy farmers and landowners. As such, the tool is * Excel-based, with 13 attributes needed * Minor spatial analysis can be completed in ArcGIS or Google Earth * Includes management strategies that will likely lead to improved habitat * Combines landscape, regional, and site-specific attributes into one clear metric (% functionality) * Can be used on working lands and protected wetlands and everything in between We believe tools are only useful if they are used. Given the identified users of this tool, having a relatively simple excel sheet that completes the complicating weighting of attributes behind-the-scenes was critical for tool success. Adaptive Management & Proliferation \u00b6 We at EI were using the tool to help the California Department of Water Resources, who manages hundreds of acres of GGS habitat, demonstrate habitat improvements. Through this process, we identified several modification that would make the tool more usable and useful to meet its goals. Our team 1. Identified consequences of specific attribute weights in the tool that were leading to misaligned incentives 2. Developed a scenario analysis to understand the true impact of identified attributes 3. Underwent adaptive management process with the TAC to improve the tool so it properly weighted specific attributed 4. Trained DWR staff on use of the tool, including in-field data collection and desktop analyses 5. Worked with DWR to integrate the HQT into their standard practices","title":"Giant Garter Snake Habitat Quantification Tool"},{"location":"portfolio/ggs-hqt/ggs-hqt/#giant-garter-snake-habitat-quantification-tool","text":"","title":"Giant Garter Snake Habitat Quantification Tool"},{"location":"portfolio/ggs-hqt/ggs-hqt/#excel-based-tool-for-understanding-habitat-functionality","text":"The Central Valley Habitat Exchange developed a multi-species habitat quanitification tool (HQT) to understand not only the quantity (e.g. acres) but also the quality of habitat across the Central Valley of California. This tool was focused on species that co-exist with agricultural landscapes.","title":"Excel-based Tool for Understanding Habitat Functionality"},{"location":"portfolio/ggs-hqt/ggs-hqt/#giant-garter-snakes","text":"Giant garter snakes were included in the original tool. This species, once common in the seasonal wetlands of the Central Valley, has seen drastic habitat reductions and is now listed as endangered under the Endangered Species Act and the California Endangered Species Act. These snakes now live in agricultural canals and rice fields, and are often impacted by standard maintenance activities, like dredging canals or grading levees. Because this species relies on human-created ecosystems and is heavily regulated by federal and state wildlife agencies, there is a great need to understand their habitat, be able to compare habitat sites, and demonstrate improvement of habitat for the snake.","title":"Giant Garter Snakes"},{"location":"portfolio/ggs-hqt/ggs-hqt/#benefits-of-the-hqt","text":"The giant garter snake HQT was developed by Environmental Defense Fund and a Technical Advisory Committee of giant garter snake experts. EI helped to ensure the tool was specifically designed to be used by conservation organizations, agencies, and technically savvy farmers and landowners. As such, the tool is * Excel-based, with 13 attributes needed * Minor spatial analysis can be completed in ArcGIS or Google Earth * Includes management strategies that will likely lead to improved habitat * Combines landscape, regional, and site-specific attributes into one clear metric (% functionality) * Can be used on working lands and protected wetlands and everything in between We believe tools are only useful if they are used. Given the identified users of this tool, having a relatively simple excel sheet that completes the complicating weighting of attributes behind-the-scenes was critical for tool success.","title":"Benefits of the HQT"},{"location":"portfolio/ggs-hqt/ggs-hqt/#adaptive-management-proliferation","text":"We at EI were using the tool to help the California Department of Water Resources, who manages hundreds of acres of GGS habitat, demonstrate habitat improvements. Through this process, we identified several modification that would make the tool more usable and useful to meet its goals. Our team 1. Identified consequences of specific attribute weights in the tool that were leading to misaligned incentives 2. Developed a scenario analysis to understand the true impact of identified attributes 3. Underwent adaptive management process with the TAC to improve the tool so it properly weighted specific attributed 4. Trained DWR staff on use of the tool, including in-field data collection and desktop analyses 5. Worked with DWR to integrate the HQT into their standard practices","title":"Adaptive Management &amp; Proliferation"},{"location":"portfolio/utilization-report/utilization-report/","text":"Utilization Report \u00b6 The EI Utilization Report helps inform weekly planning by providing up-to-date utilization data for all EI staff. # python, heroku, streamlit, pandas, matplotlib, google api, google sheets, data viz","title":"Utilization Report"},{"location":"portfolio/utilization-report/utilization-report/#utilization-report","text":"The EI Utilization Report helps inform weekly planning by providing up-to-date utilization data for all EI staff. # python, heroku, streamlit, pandas, matplotlib, google api, google sheets, data viz","title":"Utilization Report"},{"location":"project-organization/project-planning/","text":"Getting Started \u00b6 Before beginning any project, start by thinking it through. What technologies are required? What existing frameworks or packages are available? How will the project be deployed? Who are the users? Is the project worth the effort? How long will it take? How will the project be supported? These are the high-level questions that will shape the direction and scope of the project. Use the EI Data Driven Product - Product Definition to get started. Next, think about the general approach. What will the architecture of the project be? Which specific packages will be used? Should you set up a virtual environment? What will the user interface look like? How will the backend be managed? What testing approach will be used? What conventions will be used for file and folder naming, code style, etc.? Draft a Tool Specifications document if warranted (i.e., for large, billable projects). See the draft Tool Specifications outline in the EI Data Driven Product - Product Definition. You should have a clear plan in writing before starting with the first line of code. Example Project Plan \u00b6 Here's an example project plan for this project: Solution Description \u00b6 A web-based wiki for capturing and organizing information important to developing metrics products with a focus on performance-driven conservation programs. Goal & Objectives \u00b6 Goal \u00b6 Create a single source for documenting and sharing EI's approach to metrics product development and best practices for current and future staff. Objectives \u00b6 Compile all existing resources for data product development and create single platform for accumulating new resources. Present information in user-friendly format that balances instructional content with requirements for illustrating in-line code. Require staff focused on product development to work with technologies that will be used in deploying EI data products. Users \u00b6 Primary \u00b6 EI's metrics staff and other technical staff (e.g., Erik, Kristen, Maso) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Metrics staff have developed standard processes and must learn new technologies frequently to deliver metrics products. Until now, this information has been scattered in multiple locations or lost entirely. New information that is learned or \u200b Value Propositions: This product will provide a common location for storing information and allow metrics staff to quickly access said information and re-familiarize themselves with important information quickly. Secondary \u00b6 Non-technical staff working with metrics staff to develop a data product (e.g., Kelsey) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Non-technical staff are unaware of the capabilities available to them from metrics service line staff or are uncertain as to how to utilize those capabilities for their projects, both internal and external. \u200b Value Propositions: This product will include overviews of available services and technologies, as well as project examples, that non-technical staff can peruse at their leisure. Metrics staff can also use the wiki to introduce options and explain our engagement process. User Stories \u00b6 As the metrics service line lead, I want to ______ so I can ______: Direct staff to a common resource so I can limit the time needed to train new staff on commonly used technologies and best practices. Encourage metrics staff to work with technologies like git and the command line so I can better integrate them into our workflows. As a metrics/technical staff, I want to ______ so I can ______: Quickly relearn previously used technologies so I can employ them in new projects with minimal spin up time. Communicate to non-metrics staff the options they have for a specific product, visualization or analysis so I can more quickly scope their projects. Capture and share new technologies, ideas, and other information as I discover it so I can reference it later and promote its use. As a non-metrics EI staff, I want to ______ so I can ______: Better understand the capabilities of EI staff to support on technology-based products so I can pitch them to clients or develop them for internal uses. Better understand the process for working with metrics service line staff so I can assess the feasibility of the product I'm considering. Product Sketch \u00b6 [Story Board, what did you learn from storyboarding?] Design Principles & Constraints \u00b6 (list 'em. Consider integration, hosting, support, adaptive management, deployment but don't get into details yet) Conditions of Satisfaction \u00b6 (How will we know this tool is successful?) Approach \u00b6 (Roles, Timeline, Resources required) As you can see, lots to be done! The folder order is roughly the prioritization for these pages. Thus, my approach will be to work through these pages in roughly this order. One objective for this project is to provide a place to store new information as it becomes available, so I'll create the above folders initially as markdown files of the same name within a 'tbd' folder where I can store links and other references as I come across them. I've also built some of the pages above in other formats (Evernote, Google Docs, Jupyter Notebooks, etc.) so I can now pull everything together into one place. Optional: Situation Model \u00b6 Optional: Results Chain \u00b6 \u00b6 (Move to Specs) \u00b6 Technologies: \u00b6 MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE Git - version control Github - Repository Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc. Architecture \u00b6 The MkDocs package will create the basic architecture when creating the project . After creating the project, a mkdocs.yml file will be created. A docs folder will also be created with an index.md file within it. The index.md file manages the site outline; the mkdocs.yml file manages the settings. I'll add a README.md file in the root folder that will show up on the Github repo page. Files and folders can be created within the docs folder to create the project pages. Here's the file structure proposed within the root folder; the folder structure will mirror the site outline: mkdocs.yml # describes how the site is organized, enables features README.md # instructions for accessing and publishing docs/ index.md # home page how-we-work/ consulting-process.md program-requirements.md conservation-design/ project-planning/ file-organization-and-naming.md project-planning.md specifications-outline.md skills-and-training.md git/ installing-git.md initializing-git.md using-git.md development/ virtual-environments.md IDEs.md data-science-workflow.md deployment/ deployment-overview.md jupyter.md heroku.md linux.md aws.md docker.md data-management/ database-overview.md data-science/ workflow-overview.md data-exploration.md data-analysis.md data-visualization.md (e.g., inline exploratory) spatial-analysis/ earth-observation.md google-earth-engine.md gdal.md arcpy.md land-use-land-cover.md dashboards/ visualization/ (e.g. report quality) packages/ dash pandas seaborn folium sqlite rasterio Other Considerations \u00b6 Keep track of concerns and other considerations as you go and revisit the specifications periodically to ensure the best approach has been taken. How will this tech stack allow for illustrating using code? Can code be run within the deployment environment, or will static code blocks and outputs be needed? How often should links to a Jupyter Notebook, for example, be used as opposed to illustrating static code? Is there a good way to surface content for non-technical staff that are interested in these services or are asked by the metrics staff to, for example, complete a product definition? Or should the users be limited to technical staff only?","title":"Project planning"},{"location":"project-organization/project-planning/#getting-started","text":"Before beginning any project, start by thinking it through. What technologies are required? What existing frameworks or packages are available? How will the project be deployed? Who are the users? Is the project worth the effort? How long will it take? How will the project be supported? These are the high-level questions that will shape the direction and scope of the project. Use the EI Data Driven Product - Product Definition to get started. Next, think about the general approach. What will the architecture of the project be? Which specific packages will be used? Should you set up a virtual environment? What will the user interface look like? How will the backend be managed? What testing approach will be used? What conventions will be used for file and folder naming, code style, etc.? Draft a Tool Specifications document if warranted (i.e., for large, billable projects). See the draft Tool Specifications outline in the EI Data Driven Product - Product Definition. You should have a clear plan in writing before starting with the first line of code.","title":"Getting Started"},{"location":"project-organization/project-planning/#example-project-plan","text":"Here's an example project plan for this project:","title":"Example Project Plan"},{"location":"project-organization/project-planning/#solution-description","text":"A web-based wiki for capturing and organizing information important to developing metrics products with a focus on performance-driven conservation programs.","title":"Solution Description"},{"location":"project-organization/project-planning/#goal-objectives","text":"","title":"Goal &amp; Objectives"},{"location":"project-organization/project-planning/#goal","text":"Create a single source for documenting and sharing EI's approach to metrics product development and best practices for current and future staff.","title":"Goal"},{"location":"project-organization/project-planning/#objectives","text":"Compile all existing resources for data product development and create single platform for accumulating new resources. Present information in user-friendly format that balances instructional content with requirements for illustrating in-line code. Require staff focused on product development to work with technologies that will be used in deploying EI data products.","title":"Objectives"},{"location":"project-organization/project-planning/#users","text":"","title":"Users"},{"location":"project-organization/project-planning/#primary","text":"EI's metrics staff and other technical staff (e.g., Erik, Kristen, Maso) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Metrics staff have developed standard processes and must learn new technologies frequently to deliver metrics products. Until now, this information has been scattered in multiple locations or lost entirely. New information that is learned or \u200b Value Propositions: This product will provide a common location for storing information and allow metrics staff to quickly access said information and re-familiarize themselves with important information quickly.","title":"Primary"},{"location":"project-organization/project-planning/#secondary","text":"Non-technical staff working with metrics staff to develop a data product (e.g., Kelsey) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Non-technical staff are unaware of the capabilities available to them from metrics service line staff or are uncertain as to how to utilize those capabilities for their projects, both internal and external. \u200b Value Propositions: This product will include overviews of available services and technologies, as well as project examples, that non-technical staff can peruse at their leisure. Metrics staff can also use the wiki to introduce options and explain our engagement process.","title":"Secondary"},{"location":"project-organization/project-planning/#user-stories","text":"As the metrics service line lead, I want to ______ so I can ______: Direct staff to a common resource so I can limit the time needed to train new staff on commonly used technologies and best practices. Encourage metrics staff to work with technologies like git and the command line so I can better integrate them into our workflows. As a metrics/technical staff, I want to ______ so I can ______: Quickly relearn previously used technologies so I can employ them in new projects with minimal spin up time. Communicate to non-metrics staff the options they have for a specific product, visualization or analysis so I can more quickly scope their projects. Capture and share new technologies, ideas, and other information as I discover it so I can reference it later and promote its use. As a non-metrics EI staff, I want to ______ so I can ______: Better understand the capabilities of EI staff to support on technology-based products so I can pitch them to clients or develop them for internal uses. Better understand the process for working with metrics service line staff so I can assess the feasibility of the product I'm considering.","title":"User Stories"},{"location":"project-organization/project-planning/#product-sketch","text":"[Story Board, what did you learn from storyboarding?]","title":"Product Sketch"},{"location":"project-organization/project-planning/#design-principles-constraints","text":"(list 'em. Consider integration, hosting, support, adaptive management, deployment but don't get into details yet)","title":"Design Principles &amp; Constraints"},{"location":"project-organization/project-planning/#conditions-of-satisfaction","text":"(How will we know this tool is successful?)","title":"Conditions of Satisfaction"},{"location":"project-organization/project-planning/#approach","text":"(Roles, Timeline, Resources required) As you can see, lots to be done! The folder order is roughly the prioritization for these pages. Thus, my approach will be to work through these pages in roughly this order. One objective for this project is to provide a place to store new information as it becomes available, so I'll create the above folders initially as markdown files of the same name within a 'tbd' folder where I can store links and other references as I come across them. I've also built some of the pages above in other formats (Evernote, Google Docs, Jupyter Notebooks, etc.) so I can now pull everything together into one place.","title":"Approach"},{"location":"project-organization/project-planning/#optional-situation-model","text":"","title":"Optional: Situation Model"},{"location":"project-organization/project-planning/#optional-results-chain","text":"","title":"Optional: Results Chain"},{"location":"project-organization/project-planning/#_1","text":"","title":""},{"location":"project-organization/project-planning/#move-to-specs","text":"","title":"(Move to Specs)"},{"location":"project-organization/project-planning/#technologies","text":"MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE Git - version control Github - Repository Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc.","title":"Technologies:"},{"location":"project-organization/project-planning/#architecture","text":"The MkDocs package will create the basic architecture when creating the project . After creating the project, a mkdocs.yml file will be created. A docs folder will also be created with an index.md file within it. The index.md file manages the site outline; the mkdocs.yml file manages the settings. I'll add a README.md file in the root folder that will show up on the Github repo page. Files and folders can be created within the docs folder to create the project pages. Here's the file structure proposed within the root folder; the folder structure will mirror the site outline: mkdocs.yml # describes how the site is organized, enables features README.md # instructions for accessing and publishing docs/ index.md # home page how-we-work/ consulting-process.md program-requirements.md conservation-design/ project-planning/ file-organization-and-naming.md project-planning.md specifications-outline.md skills-and-training.md git/ installing-git.md initializing-git.md using-git.md development/ virtual-environments.md IDEs.md data-science-workflow.md deployment/ deployment-overview.md jupyter.md heroku.md linux.md aws.md docker.md data-management/ database-overview.md data-science/ workflow-overview.md data-exploration.md data-analysis.md data-visualization.md (e.g., inline exploratory) spatial-analysis/ earth-observation.md google-earth-engine.md gdal.md arcpy.md land-use-land-cover.md dashboards/ visualization/ (e.g. report quality) packages/ dash pandas seaborn folium sqlite rasterio","title":"Architecture"},{"location":"project-organization/project-planning/#other-considerations","text":"Keep track of concerns and other considerations as you go and revisit the specifications periodically to ensure the best approach has been taken. How will this tech stack allow for illustrating using code? Can code be run within the deployment environment, or will static code blocks and outputs be needed? How often should links to a Jupyter Notebook, for example, be used as opposed to illustrating static code? Is there a good way to surface content for non-technical staff that are interested in these services or are asked by the metrics staff to, for example, complete a product definition? Or should the users be limited to technical staff only?","title":"Other Considerations"},{"location":"the-basics/getting-started/","text":"Getting Started \u00b6 Install python and basic packages \u00b6 Install miniconda Add to context menu (https://gist.github.com/jiewpeng/8ba446acf329b1801bf91db767d179ea) Run regedit.exe Navigate to HKEY_CLASSES_ROOT > Directory > Background > shell Add a key named AnacondaPrompt and set its value to \"Anaconda Prompt Here\" (or anything you'd like it to appear as in the right click context menu). Right click on the shell folder and select 'New Key'. Add a key under this key, called command , and set its value to cmd.exe /K C:Users\\Erik\\miniconda3\\Scripts\\activate.bat (may have to change the activate.bat file to where ever Anaconda is installed) Add icon Create a new String Value in the AnacondaPrompt key (right click the key created in step 2.3) and set its value to the location of the .ico file you'd like to use, like C:\\Users\\Erik\\miniconda3\\Menu\\Iconleak-Atrous-Console.ico Install jupter lab Right click anywhere Open Anaconda Prompt Type conda install -c conda-forge jupterlab When prompted, type y to begin download (You may need to upgrade some packages to get jupyter lab to run. Use conda update <PACKAGE NAME> , which will get you the latest version) If you like, you can install a few additional often used packages into the base environment, but we recommend using environments for most projects, rather than installing everything into the base environment: pandas Why install packages into the base environment? \u00b6 In general, we recommend creating virtual environments for most projects. However, especially as you are getting started with coding, you'll want to be able to quickly explore different concepts. Having these commonly used packages installed in the base environment will allow you to explore without a lot of set up or tear down. Working in Jupyter Notebooks \u00b6 Jupyter notebooks are a literate coding environment which allow you to mix code with your own text in Markdown. Outputs are printed directly to the screen. Notebooks are great for getting started, and may become an important part of your process even as you advance. I use notebooks for data exploration and when developing new processes. As I test different strategies and packages, I can capture both what works (which eventually finds its way into a .py file) and what didn't work. Using Jupyter Notebooks with environments \u00b6 As you are exploring a new project, you may want to use jupyter notebooks in a virtual environment. This will allow you to avoid polluting your base environment, or your project's main environment, with packages you won't end up using, and the jupyter notebook package itself. While you could install jupyter notebook directly in the environment (but make sure to complete step 6 below), you might not if you plan to deploy your app as you will end up installing jupyter notebook on the production server when you really don't need to. I like to set up a temporary environment for this exploration phase. Follow the process below in your command prompt. conda create -n <temp_env_name> (I like to append -nb to indicate a notebook environement) conda activate <temp_env_name> conda install -c conda-forge <jupyterlab> (install any other packages needed) python -m ipykernel install --user : this step installs the kernel into the environment so that your system can find the environment. See here for more on the relationship between a jupyter notebook kernel and the environment. Note that you will need to do this step anytime you change environments and wish to use jupyter notebooks (even in the base environment) jupyter notebook will launch the notebook in your browser (it will open automatically)","title":"Getting Started"},{"location":"the-basics/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"the-basics/getting-started/#install-python-and-basic-packages","text":"Install miniconda Add to context menu (https://gist.github.com/jiewpeng/8ba446acf329b1801bf91db767d179ea) Run regedit.exe Navigate to HKEY_CLASSES_ROOT > Directory > Background > shell Add a key named AnacondaPrompt and set its value to \"Anaconda Prompt Here\" (or anything you'd like it to appear as in the right click context menu). Right click on the shell folder and select 'New Key'. Add a key under this key, called command , and set its value to cmd.exe /K C:Users\\Erik\\miniconda3\\Scripts\\activate.bat (may have to change the activate.bat file to where ever Anaconda is installed) Add icon Create a new String Value in the AnacondaPrompt key (right click the key created in step 2.3) and set its value to the location of the .ico file you'd like to use, like C:\\Users\\Erik\\miniconda3\\Menu\\Iconleak-Atrous-Console.ico Install jupter lab Right click anywhere Open Anaconda Prompt Type conda install -c conda-forge jupterlab When prompted, type y to begin download (You may need to upgrade some packages to get jupyter lab to run. Use conda update <PACKAGE NAME> , which will get you the latest version) If you like, you can install a few additional often used packages into the base environment, but we recommend using environments for most projects, rather than installing everything into the base environment: pandas","title":"Install python and basic packages"},{"location":"the-basics/getting-started/#why-install-packages-into-the-base-environment","text":"In general, we recommend creating virtual environments for most projects. However, especially as you are getting started with coding, you'll want to be able to quickly explore different concepts. Having these commonly used packages installed in the base environment will allow you to explore without a lot of set up or tear down.","title":"Why install packages into the base environment?"},{"location":"the-basics/getting-started/#working-in-jupyter-notebooks","text":"Jupyter notebooks are a literate coding environment which allow you to mix code with your own text in Markdown. Outputs are printed directly to the screen. Notebooks are great for getting started, and may become an important part of your process even as you advance. I use notebooks for data exploration and when developing new processes. As I test different strategies and packages, I can capture both what works (which eventually finds its way into a .py file) and what didn't work.","title":"Working in Jupyter Notebooks"},{"location":"the-basics/getting-started/#using-jupyter-notebooks-with-environments","text":"As you are exploring a new project, you may want to use jupyter notebooks in a virtual environment. This will allow you to avoid polluting your base environment, or your project's main environment, with packages you won't end up using, and the jupyter notebook package itself. While you could install jupyter notebook directly in the environment (but make sure to complete step 6 below), you might not if you plan to deploy your app as you will end up installing jupyter notebook on the production server when you really don't need to. I like to set up a temporary environment for this exploration phase. Follow the process below in your command prompt. conda create -n <temp_env_name> (I like to append -nb to indicate a notebook environement) conda activate <temp_env_name> conda install -c conda-forge <jupyterlab> (install any other packages needed) python -m ipykernel install --user : this step installs the kernel into the environment so that your system can find the environment. See here for more on the relationship between a jupyter notebook kernel and the environment. Note that you will need to do this step anytime you change environments and wish to use jupyter notebooks (even in the base environment) jupyter notebook will launch the notebook in your browser (it will open automatically)","title":"Using Jupyter Notebooks with environments"}]}