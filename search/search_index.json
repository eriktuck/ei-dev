{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Credit: Randall Munroe (xkcd.com/2054) Welcome to Environmental Incentives' Metrics Service Line site! This site is intended for all EI staff to better understand the services of the Metrics service line and for Metrics service line experts as an important standard reference. Don't build a giant house of cards! We can help you develop solutions that last. All EI Staff \u00b6 If you're here to better understand how the Metrics service line can support your project or internal initiative, you might want to check out our portfolio of projects , better understand how we can work with you , or review one of these helpful introductions to technologies and services available to you: Data Analysis Tool Development # (i.e., deployment) Databases Visualization Mapping and Spatial Analysis Metrics Service Line Experts \u00b6 Metrics service line experts will want to familiarize themselves with all of the content here. This site provides helpful training resources, captures best practices for developing metrics products, and describes our standard consulting process for internal and external clients. If you will be contributing to the development of this site, please reference this guidance .","title":"Home"},{"location":"#all-ei-staff","text":"If you're here to better understand how the Metrics service line can support your project or internal initiative, you might want to check out our portfolio of projects , better understand how we can work with you , or review one of these helpful introductions to technologies and services available to you: Data Analysis Tool Development # (i.e., deployment) Databases Visualization Mapping and Spatial Analysis","title":"All EI Staff"},{"location":"#metrics-service-line-experts","text":"Metrics service line experts will want to familiarize themselves with all of the content here. This site provides helpful training resources, captures best practices for developing metrics products, and describes our standard consulting process for internal and external clients. If you will be contributing to the development of this site, please reference this guidance .","title":"Metrics Service Line Experts"},{"location":"development-guidance/","text":"EI Development Guidance \u00b6 This was produced with mkdocs. For full documentation visit mkdocs.org . Contents \u00b6 Home Portfolio Metrics Services Metrics Design Philosophy Project Planning Git Development Deployment (non-tech: how do we deliver products?) Data Management (non-tech: database overview) Data Science (non-tech: types of data analysis) Spatial Analysis (non-tech: types of spatial data analysis) Visualization (non-tech: visualization options) Packages News/Blog How to Maintain this Site Additional Resources Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 1 2 3 4 mkdocs . yml # The configuration file . docs / index . md # The documentation homepage . ... # Other markdown pages , images and other files .","title":"How to Maintain This Site"},{"location":"development-guidance/#ei-development-guidance","text":"This was produced with mkdocs. For full documentation visit mkdocs.org .","title":"EI Development Guidance"},{"location":"development-guidance/#contents","text":"Home Portfolio Metrics Services Metrics Design Philosophy Project Planning Git Development Deployment (non-tech: how do we deliver products?) Data Management (non-tech: database overview) Data Science (non-tech: types of data analysis) Spatial Analysis (non-tech: types of spatial data analysis) Visualization (non-tech: visualization options) Packages News/Blog How to Maintain this Site Additional Resources","title":"Contents"},{"location":"development-guidance/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"development-guidance/#project-layout","text":"1 2 3 4 mkdocs . yml # The configuration file . docs / index . md # The documentation homepage . ... # Other markdown pages , images and other files .","title":"Project layout"},{"location":"notes/","text":"This document serves as a quick notepad for storing helpful information. Customs Used \u00b6 To identify a user-provided value, wrap it in <> within a code block: git commit <updated-file> Highlight package names in code blocks: rasterio Markdown Reference \u00b6 Here is a handy reference for writing in markdown, provided by Typora. Converting Notebooks \u00b6 To add a notebook to the site, us nbconvert to convert the notebook to markdown. Save the output with the desired filename and into a new directory. The new directory will include the markdown file and assets within a *_files directory. Cut and paste the new directory into the ei-dev docs/ folder in the desired location. Open the Anaconda Prompt window within the notebook's root directory Run the following command (a new directory will be created if needed): jupyter nbconvert <notebook.ipynb> --to markdown --output-dir='<rel path to new dirname>' --output <desired-filename> For example jupyter nbconvert my_notebook.ipynb --to markdown --output-dir='../md-notebooks' --output my_markdown Cut and paste the newly created directory into the ei-dev docs/ folder in the desired location. Syntax highlighting is done using the codehilite package (part of standard markdown library) and Pygments (part of standard Python library). However, codehilite must be enabled in the mkdocs.yml file. See documentation for more. Changes made to the notebook will not be reflected in the markdown files, and you'll have to overwrite the folder you created the first time. If you change the name of the outputs, update the mkdocs.yml folder. Linking Documents \u00b6 To link to another page, simply provide the relative path to the page as a markdown file. You can link to a section if you add a '#' after the filename. Sections should be all lower-case and '-' delimitted. Ignore any punctuation in the header. See also documentation . [here's a link](foldername/filename.md#header-name) Images and Gifs \u00b6 You can either link to images and gifs online or save locally. If saving locally, store in assets/ within the section directory (e.g., consulting-process/ ). The alternative text (within the square brackets) is read by accessibility services. See also documentation ![here's an image](assets/gif.gif) YouTube Videos \u00b6 To include Youtube videos, use the iframe provided under the 'Share' option. Any HTML pasted directly within a markdown file will be rendered in the site. Forms \u00b6 Google forms (and probably many other web forms) can be embedded as HTML within an iframe. Extensions \u00b6 Markdown and Mkdcos both offer extensions for rendering markdown. Note that the extensions may need to be configured for Typora as well as Mkdocs for consistency between editing and serving. Material has a few extensions as well. Check these out in their project documentation pages to see if the functionality would be helpful. Interactive Visualizations \u00b6 Maps, dashboards, and other interactive visualizations should be passed in using the iframe. See Plotly guidance here .","title":"Notes"},{"location":"notes/#customs-used","text":"To identify a user-provided value, wrap it in <> within a code block: git commit <updated-file> Highlight package names in code blocks: rasterio","title":"Customs Used"},{"location":"notes/#markdown-reference","text":"Here is a handy reference for writing in markdown, provided by Typora.","title":"Markdown Reference"},{"location":"notes/#converting-notebooks","text":"To add a notebook to the site, us nbconvert to convert the notebook to markdown. Save the output with the desired filename and into a new directory. The new directory will include the markdown file and assets within a *_files directory. Cut and paste the new directory into the ei-dev docs/ folder in the desired location. Open the Anaconda Prompt window within the notebook's root directory Run the following command (a new directory will be created if needed): jupyter nbconvert <notebook.ipynb> --to markdown --output-dir='<rel path to new dirname>' --output <desired-filename> For example jupyter nbconvert my_notebook.ipynb --to markdown --output-dir='../md-notebooks' --output my_markdown Cut and paste the newly created directory into the ei-dev docs/ folder in the desired location. Syntax highlighting is done using the codehilite package (part of standard markdown library) and Pygments (part of standard Python library). However, codehilite must be enabled in the mkdocs.yml file. See documentation for more. Changes made to the notebook will not be reflected in the markdown files, and you'll have to overwrite the folder you created the first time. If you change the name of the outputs, update the mkdocs.yml folder.","title":"Converting Notebooks"},{"location":"notes/#linking-documents","text":"To link to another page, simply provide the relative path to the page as a markdown file. You can link to a section if you add a '#' after the filename. Sections should be all lower-case and '-' delimitted. Ignore any punctuation in the header. See also documentation . [here's a link](foldername/filename.md#header-name)","title":"Linking Documents"},{"location":"notes/#images-and-gifs","text":"You can either link to images and gifs online or save locally. If saving locally, store in assets/ within the section directory (e.g., consulting-process/ ). The alternative text (within the square brackets) is read by accessibility services. See also documentation ![here's an image](assets/gif.gif)","title":"Images and Gifs"},{"location":"notes/#youtube-videos","text":"To include Youtube videos, use the iframe provided under the 'Share' option. Any HTML pasted directly within a markdown file will be rendered in the site.","title":"YouTube Videos"},{"location":"notes/#forms","text":"Google forms (and probably many other web forms) can be embedded as HTML within an iframe.","title":"Forms"},{"location":"notes/#extensions","text":"Markdown and Mkdcos both offer extensions for rendering markdown. Note that the extensions may need to be configured for Typora as well as Mkdocs for consistency between editing and serving. Material has a few extensions as well. Check these out in their project documentation pages to see if the functionality would be helpful.","title":"Extensions"},{"location":"notes/#interactive-visualizations","text":"Maps, dashboards, and other interactive visualizations should be passed in using the iframe. See Plotly guidance here .","title":"Interactive Visualizations"},{"location":"under-construction/","text":"We're getting to it... \u00b6","title":"Spatial Analysis Overview"},{"location":"under-construction/#were-getting-to-it","text":"","title":"We're getting to it..."},{"location":"additional-resources/interview-guide/","text":"Interview Guide \u00b6 An editable interview guide is available here ; download a version of this before proceeding. Talking to real people will help us better understand our potential users and the problems they face in their efforts to meet our conservation targets. This guide is intended to help you plan and facilitate the interview. There are two sections to the interview: (1) understanding the persona and (2) understanding their problems. Within each section, a table with \u2018question topics\u2019 and \u2018example questions\u2019 is provided. Edit the example questions within each question topic prior to the interview. Use these questions as a guide, but feel free to improvise or skip questions as you see fit. Discussion during the interview should relate to our area of interest (conservation targets), but should not be leading. People are generally agreeable\u2014if you ask whether they think pollinators are important, they will probably say yes. Better to ask what their priorities are and see if they mention the problems or solutions you're testing. Towards the end of the interview, you may ask more leading questions to see why they did or did not mention our area of interest or value proposition in their earlier responses. Before conducting the interview, you should (1) develop a persona based on available data sources and (2) brainstorm the types of problems they would face in relation to your program scope, vision or conservation targets. To ensure your personas are well-developed, make sure they meet the following criteria: Real \u2013 based on actual evidence (secondary research, interviews, observations, collected data) Exact \u2013 includes sufficient detail that the persona \u2018feels\u2019 like a real person Actionable \u2013 represents people that would actually use the MBHE Clear \u2013 described well enough to be understood by others on the team Testable \u2013 evidence can be collected to substantiate and improve the persona over time Fill out the table in the interview guide for the persona to be interviewed to ensure your persona is ready and to brainstorm specific questions to ask.","title":"Interview Guide"},{"location":"additional-resources/interview-guide/#interview-guide","text":"An editable interview guide is available here ; download a version of this before proceeding. Talking to real people will help us better understand our potential users and the problems they face in their efforts to meet our conservation targets. This guide is intended to help you plan and facilitate the interview. There are two sections to the interview: (1) understanding the persona and (2) understanding their problems. Within each section, a table with \u2018question topics\u2019 and \u2018example questions\u2019 is provided. Edit the example questions within each question topic prior to the interview. Use these questions as a guide, but feel free to improvise or skip questions as you see fit. Discussion during the interview should relate to our area of interest (conservation targets), but should not be leading. People are generally agreeable\u2014if you ask whether they think pollinators are important, they will probably say yes. Better to ask what their priorities are and see if they mention the problems or solutions you're testing. Towards the end of the interview, you may ask more leading questions to see why they did or did not mention our area of interest or value proposition in their earlier responses. Before conducting the interview, you should (1) develop a persona based on available data sources and (2) brainstorm the types of problems they would face in relation to your program scope, vision or conservation targets. To ensure your personas are well-developed, make sure they meet the following criteria: Real \u2013 based on actual evidence (secondary research, interviews, observations, collected data) Exact \u2013 includes sufficient detail that the persona \u2018feels\u2019 like a real person Actionable \u2013 represents people that would actually use the MBHE Clear \u2013 described well enough to be understood by others on the team Testable \u2013 evidence can be collected to substantiate and improve the persona over time Fill out the table in the interview guide for the persona to be interviewed to ensure your persona is ready and to brainstorm specific questions to ask.","title":"Interview Guide"},{"location":"additional-resources/persona-guide/","text":"Persona Development Guide \u00b6 Personas include (1) a name, (2) a photo, (3) a screening question that will help distinguish those that fit the persona and those that don\u2019t and (4) a description of relevant motivations and constraints. You can use a template or simply write down one or two paragraphs. The persona profile should fit on a single page or PowerPoint slide. The goal is to personalize the stakeholder so you understand their perspective and allow you to advocate on their behalf while developing solutions. Here is a quick and simple process: Start by dumping a list of all the personas you think might be relevant. List them by name, such as \u2018Fred the Farmer\u2019. Ultimately, you\u2019ll need to focus on the ones that are most relevant (you can\u2019t make everyone happy), but for now more is more*. Next, rank the personas and group redundant ones. Who do you need to appeal to first to achieve your vision? Who is most likely to help or hurt your cause? Draft screening questions for each priority persona. This is one sentence that \u2018defines\u2019 who the persona is and allows you to determine if someone fits the bill (e.g., do you depend entirely on farm income for your livelihood?) Write down at least 5 people that fit the persona. You don\u2019t have to know these people, but they must exist. Use one of their photos for the persona profile. Now spend less than 30 minutes per persona to start a draft. You will find that you may be very ignorant and stereotypical at this point\u2014that\u2019s fine, overcoming that is what this exercise is all about! Next you will research and refine your personas. * Sometimes, it's best to use a different pattern for categorizing personas instead of people, which can carry connotations that you don't mean to include. For example, categorize them by their shoes (wingtips, steel toes, and tevas), where they go on vacation (snow birds, summer in france, timeshares), or something else relevant to your project. Example \u00b6 This example was used in our effort to encourage farmers to participate in a cost-share conservation project (the Monarch Habitat Exchange). Acknowledgements \u00b6 This guide was adapted from Alex Cowan's excellent guide, which you can find here .","title":"Persona Development Guide"},{"location":"additional-resources/persona-guide/#persona-development-guide","text":"Personas include (1) a name, (2) a photo, (3) a screening question that will help distinguish those that fit the persona and those that don\u2019t and (4) a description of relevant motivations and constraints. You can use a template or simply write down one or two paragraphs. The persona profile should fit on a single page or PowerPoint slide. The goal is to personalize the stakeholder so you understand their perspective and allow you to advocate on their behalf while developing solutions. Here is a quick and simple process: Start by dumping a list of all the personas you think might be relevant. List them by name, such as \u2018Fred the Farmer\u2019. Ultimately, you\u2019ll need to focus on the ones that are most relevant (you can\u2019t make everyone happy), but for now more is more*. Next, rank the personas and group redundant ones. Who do you need to appeal to first to achieve your vision? Who is most likely to help or hurt your cause? Draft screening questions for each priority persona. This is one sentence that \u2018defines\u2019 who the persona is and allows you to determine if someone fits the bill (e.g., do you depend entirely on farm income for your livelihood?) Write down at least 5 people that fit the persona. You don\u2019t have to know these people, but they must exist. Use one of their photos for the persona profile. Now spend less than 30 minutes per persona to start a draft. You will find that you may be very ignorant and stereotypical at this point\u2014that\u2019s fine, overcoming that is what this exercise is all about! Next you will research and refine your personas. * Sometimes, it's best to use a different pattern for categorizing personas instead of people, which can carry connotations that you don't mean to include. For example, categorize them by their shoes (wingtips, steel toes, and tevas), where they go on vacation (snow birds, summer in france, timeshares), or something else relevant to your project.","title":"Persona Development Guide"},{"location":"additional-resources/persona-guide/#example","text":"This example was used in our effort to encourage farmers to participate in a cost-share conservation project (the Monarch Habitat Exchange).","title":"Example"},{"location":"additional-resources/persona-guide/#acknowledgements","text":"This guide was adapted from Alex Cowan's excellent guide, which you can find here .","title":"Acknowledgements"},{"location":"data-management/database-overview/","text":"Do you (or your client) have data stored across so many files you can't remember where they are? Are you scrambling to pull together information when requested, constantly months behind in reporting, or finding yourself dumping data from multiple sources into a single spreadsheet for analysis? If so, it might be time for a database. However, there are complexities to consider when setting up and managing a database. This tutorial will introduce you to databases , describe available database solutions , and help you select the right database for your needs . Introduction to Databases \u00b6 A database is simply an organized store for information. Data are stored as rows and columns in tables; relationships are defined between tables (hence the name, \"relational database\"). You might have tables for customers, products, and orders. Customers place orders which include products. The database helps you get the right products to the right customers. How the data are stored is often less important than how the data are accessed. The primary reason you're developing a database is probably to make data easier to access, analyze and report. When choosing and designing a database, you'll need to pay as much attention to how the user will interface with the database as the database solution itself. If you're simply looking for data analysis and visualization (think: Tableau), check out our Introduction to Data Visualization . Interacting with a database \u00b6 Most users will interact with a database through a navigation form. The navigation form surfaces the important functionality of the database, including entering data, querying data, and generating reports. Here's an example navigation form for a bakery's order database created in Microsoft Access. The navigation form is set up after the database is configured by the database administrator (DBA; this may be you, someone on the Metrics Service Line, or an outside contractor). The navigation form--as well as other forms, queries, and reports that are made available to the user by the DBA--make data accessible to users while protecting the integrity of the database Forms \u00b6 Data is entered into the database primarily through forms (it can be imported as well). Forms can be made available through the Navigation Form or through individual forms designed and made available by the DBA. Queries \u00b6 Queries allow you to search for and update records in the database. Want to know how many orders were placed yesterday? How many blueberry muffins you've sold this year? You'll need a query for that. Reports \u00b6 Reports are queries or collections of queries that can be batched together and run periodically. You might have a standard report that is produced each quarter or a ready-made report to provide real time information to your boss when requested. Reports must also be set up and made available by the DBA. A database solution involves multiple components \u00b6 A database solution (aka database management system; DBMS) will include these key components Database Engine and Hosting Platform : The database engine is the software that runs the database. The database and the database engine will be hosted either on a local server or on the cloud. Database Management Client : The database management client is the interface that the DBA will use to configure the database and manage access for users. The database client helps reduce, but doesn't necessarily eliminate, the need to use SQL (Structured Query Language) to interact with the database. User Interface and Analysis Platform : The user interface allows users to access the data while the analysis platform allows the user to gain insights from the data. Often, these are combined. To illustrate, below are three options for a database solution that you might consider. Example 1: Microsoft Access \u00b6 Microsoft Access is an all-in-one database solution from the '90s. It lacks many features that some would deem critical, but for lightweight and desktop-based database solutions, it's hard to beat the out-of-the-box functionality for low-tech users. More modern options are available as cloud-based, subscription services. Example 2: Microsoft Azure SQL Database + SQL Server Management Studio + Microsoft PowerApps \u00b6 A cloud-based solution that offers great customization, if you're tech-savvy enough to configure and maintain it. You don't have to be a software engineer, but don't expect it to be easy. Amazon and Oracle offer similar solutions. Example 3: MySQL + MySQL Workbench + Custom Web Application \u00b6 This open source solution may be your go-to if you're building a website and working with a web developer. Many open source solutions are available. Project Spotlight: The Registry for the Monarch Butterfly Habitat Exchange was built on this stack. Database Options \u00b6 There are many choices when it comes to databases, each with its own advantages and disadvantages. All-in-one, low tech solutions \u00b6 These are your options if you don't have a tech guru on the team and don't have enough budget to hire one. Free(ish) and Mostly Desktop-Based \u00b6 Microsoft Access and OpenOffice Base are good entry-level options. OpenOffice Base is free but has fewer features and less support than Microsoft Access. Microsoft Excel , especially if paired with custom VBA Forms and Modules, can even serve as a database but has limitations (it pains me to even suggest this). If you'd prefer a cloud-based solution, Google Sheets paired with Google Forms might even work. Cheap(ish) Cloud-based Subscription Services \u00b6 The need for data storage and analytics solutions to serve low-tech users has led to a boom in cloud-based solutions like AirTable , QuickBase , TeamDesk , Knack , and Sonadier . The price scales with increasing storage capacity and number of users, from cheap ($5/month) to not-so-cheap ($500/month or more). The data visualization powerhouse Tableau offers a data management add-on for $5/mo that might meet your needs if you're mostly focused on visualization and are already a Tableau subscriber. Salesforce and other CRMs (Customer Relationship Management software) also combine data management and analytics in a way that could serve as a solution for your needs. Finally, one notable desktop application is Notion , which is a do-everything app with a cult following that provides some database-like functionality. Solutions for the more technical crowd \u00b6 Venture into this territory and you'll want to make sure you have the technical chops on board to configure and maintain these options. Open Source Database Software \u00b6 PostgreSQL , MySQL , and MariaDB are examples of open source (free) database solutions. While these are robust, scalable database solutions used by many large corporations, they are not designed for your everyday desk jockey. These open source solutions are often paired with a web-based interface. If you are working with a web developer and have the budget for them to design the forms, queries and reports you need you'll likely choose one of these open source options. Don't forget you'll probably need to hire them again when your version of the software is no longer supported by its open source community. Cloud-based Scalable Services \u00b6 Cloud-based options are offered by Amazon RCD , Microsoft Azure and Oracle Cloud Services . Your data now lives on the cloud. Price scales with use, and entry-level tiers can be very cheap, but you will have to provide a credit card and make sure the charges don't get out of hand. To get to your data, you'll need an application--either a custom web app or a desktop app you build using a service like Microsoft PowerApps , FileMaker , Oracle Forms , or Appian . These \"low code\" application platforms allow you to develop custom solutions without a team of software engineers, but do require some serious training. Enterprise Database Solutions \u00b6 Microsoft, IBM, and Oracle all sell enterprise database solutions. Expect to pay over $1,000 per month. You can get Microsoft SQL Server Express for free but make sure its limited set of features will work for you. Database Clients \u00b6 For these more technical solutions, you'll need a database management client. Some database solutions come with a preferred client, such as Microsoft's SQL Server Management Studio and Azure Data Studio , Oracle's SQL Developer , PostreSQL's pgAdmin , and MySQL's MySQL Workbench . However, you're not stuck with these options, some of these can be used for other databases, while a number of popular free alternative options exist including SQuirrel SLQ , DataGrip , or even VSCode with mssql extension (for MySQL databases; however see Azure Data Studio as a better solution built from the VSCode platform). How to Choose? \u00b6 With so many options, how do you choose? This section will help you pick the right one. Budget \u00b6 Consider both the cost of the software itself and the cost of configuring and maintaining the database. Scalability \u00b6 Unless you're dealing with *Big Data*, you won't need to worry about the database growing too large to fit on a single server. The only likely exception would be if attempting to store geospatial data within a relational database outside of ArcGIS. If you go with a cloud-based solution you can scale as much as you want, provided you can pay for it. Hosting & Integration \u00b6 If the database will be hosted and managed by the client or their website manager, your options will likely be limited to those that integrate with their existing systems. Work with them when selecting a database and during database design. In most other cases, the database will live on the client's local server (unless you picked a cloud-based alternative). Maintenance & Support \u00b6 Who will maintain the database over time? If it's the client, make sure they are comfortable with the solution. Who will provide support if something crashes or the client needs a new type of report developed? If it's you, make sure you are comfortable with the solution. Keep in mind that paying a contractor to set up a cool web-based database will likely require paying a contractor to maintain that database periodically over time. Final Thoughts \u00b6 Setting up a database is a big step in the maturation of any data-driven program. The right database solution will keep your data safe and secure while also providing access to the data for analysis and reporting. When choosing a database, the best strategy is to keep it as simple as possible. Important! If you've decided that you're not the right person to decide which database to use, that's ok. While someone with more technical experience will be better able to select the right database solution, they may not know enough about your program to design the database and the forms, queries and reports you need. You're the expert on your program, stay involved in the process to make sure the final solution meets your needs!","title":"Database Overview"},{"location":"data-management/database-overview/#introduction-to-databases","text":"A database is simply an organized store for information. Data are stored as rows and columns in tables; relationships are defined between tables (hence the name, \"relational database\"). You might have tables for customers, products, and orders. Customers place orders which include products. The database helps you get the right products to the right customers. How the data are stored is often less important than how the data are accessed. The primary reason you're developing a database is probably to make data easier to access, analyze and report. When choosing and designing a database, you'll need to pay as much attention to how the user will interface with the database as the database solution itself. If you're simply looking for data analysis and visualization (think: Tableau), check out our Introduction to Data Visualization .","title":"Introduction to Databases"},{"location":"data-management/database-overview/#interacting-with-a-database","text":"Most users will interact with a database through a navigation form. The navigation form surfaces the important functionality of the database, including entering data, querying data, and generating reports. Here's an example navigation form for a bakery's order database created in Microsoft Access. The navigation form is set up after the database is configured by the database administrator (DBA; this may be you, someone on the Metrics Service Line, or an outside contractor). The navigation form--as well as other forms, queries, and reports that are made available to the user by the DBA--make data accessible to users while protecting the integrity of the database","title":"Interacting with a database"},{"location":"data-management/database-overview/#forms","text":"Data is entered into the database primarily through forms (it can be imported as well). Forms can be made available through the Navigation Form or through individual forms designed and made available by the DBA.","title":"Forms"},{"location":"data-management/database-overview/#queries","text":"Queries allow you to search for and update records in the database. Want to know how many orders were placed yesterday? How many blueberry muffins you've sold this year? You'll need a query for that.","title":"Queries"},{"location":"data-management/database-overview/#reports","text":"Reports are queries or collections of queries that can be batched together and run periodically. You might have a standard report that is produced each quarter or a ready-made report to provide real time information to your boss when requested. Reports must also be set up and made available by the DBA.","title":"Reports"},{"location":"data-management/database-overview/#a-database-solution-involves-multiple-components","text":"A database solution (aka database management system; DBMS) will include these key components Database Engine and Hosting Platform : The database engine is the software that runs the database. The database and the database engine will be hosted either on a local server or on the cloud. Database Management Client : The database management client is the interface that the DBA will use to configure the database and manage access for users. The database client helps reduce, but doesn't necessarily eliminate, the need to use SQL (Structured Query Language) to interact with the database. User Interface and Analysis Platform : The user interface allows users to access the data while the analysis platform allows the user to gain insights from the data. Often, these are combined. To illustrate, below are three options for a database solution that you might consider.","title":"A database solution involves multiple components"},{"location":"data-management/database-overview/#example-1-microsoft-access","text":"Microsoft Access is an all-in-one database solution from the '90s. It lacks many features that some would deem critical, but for lightweight and desktop-based database solutions, it's hard to beat the out-of-the-box functionality for low-tech users. More modern options are available as cloud-based, subscription services.","title":"Example 1: Microsoft Access"},{"location":"data-management/database-overview/#example-2-microsoft-azure-sql-database-sql-server-management-studio-microsoft-powerapps","text":"A cloud-based solution that offers great customization, if you're tech-savvy enough to configure and maintain it. You don't have to be a software engineer, but don't expect it to be easy. Amazon and Oracle offer similar solutions.","title":"Example 2: Microsoft Azure SQL Database + SQL Server Management Studio + Microsoft PowerApps"},{"location":"data-management/database-overview/#example-3-mysql-mysql-workbench-custom-web-application","text":"This open source solution may be your go-to if you're building a website and working with a web developer. Many open source solutions are available. Project Spotlight: The Registry for the Monarch Butterfly Habitat Exchange was built on this stack.","title":"Example 3: MySQL + MySQL Workbench + Custom Web Application"},{"location":"data-management/database-overview/#database-options","text":"There are many choices when it comes to databases, each with its own advantages and disadvantages.","title":"Database Options"},{"location":"data-management/database-overview/#all-in-one-low-tech-solutions","text":"These are your options if you don't have a tech guru on the team and don't have enough budget to hire one.","title":"All-in-one, low tech solutions"},{"location":"data-management/database-overview/#freeish-and-mostly-desktop-based","text":"Microsoft Access and OpenOffice Base are good entry-level options. OpenOffice Base is free but has fewer features and less support than Microsoft Access. Microsoft Excel , especially if paired with custom VBA Forms and Modules, can even serve as a database but has limitations (it pains me to even suggest this). If you'd prefer a cloud-based solution, Google Sheets paired with Google Forms might even work.","title":"Free(ish) and Mostly Desktop-Based"},{"location":"data-management/database-overview/#cheapish-cloud-based-subscription-services","text":"The need for data storage and analytics solutions to serve low-tech users has led to a boom in cloud-based solutions like AirTable , QuickBase , TeamDesk , Knack , and Sonadier . The price scales with increasing storage capacity and number of users, from cheap ($5/month) to not-so-cheap ($500/month or more). The data visualization powerhouse Tableau offers a data management add-on for $5/mo that might meet your needs if you're mostly focused on visualization and are already a Tableau subscriber. Salesforce and other CRMs (Customer Relationship Management software) also combine data management and analytics in a way that could serve as a solution for your needs. Finally, one notable desktop application is Notion , which is a do-everything app with a cult following that provides some database-like functionality.","title":"Cheap(ish) Cloud-based Subscription Services"},{"location":"data-management/database-overview/#solutions-for-the-more-technical-crowd","text":"Venture into this territory and you'll want to make sure you have the technical chops on board to configure and maintain these options.","title":"Solutions for the more technical crowd"},{"location":"data-management/database-overview/#open-source-database-software","text":"PostgreSQL , MySQL , and MariaDB are examples of open source (free) database solutions. While these are robust, scalable database solutions used by many large corporations, they are not designed for your everyday desk jockey. These open source solutions are often paired with a web-based interface. If you are working with a web developer and have the budget for them to design the forms, queries and reports you need you'll likely choose one of these open source options. Don't forget you'll probably need to hire them again when your version of the software is no longer supported by its open source community.","title":"Open Source Database Software"},{"location":"data-management/database-overview/#cloud-based-scalable-services","text":"Cloud-based options are offered by Amazon RCD , Microsoft Azure and Oracle Cloud Services . Your data now lives on the cloud. Price scales with use, and entry-level tiers can be very cheap, but you will have to provide a credit card and make sure the charges don't get out of hand. To get to your data, you'll need an application--either a custom web app or a desktop app you build using a service like Microsoft PowerApps , FileMaker , Oracle Forms , or Appian . These \"low code\" application platforms allow you to develop custom solutions without a team of software engineers, but do require some serious training.","title":"Cloud-based Scalable Services"},{"location":"data-management/database-overview/#enterprise-database-solutions","text":"Microsoft, IBM, and Oracle all sell enterprise database solutions. Expect to pay over $1,000 per month. You can get Microsoft SQL Server Express for free but make sure its limited set of features will work for you.","title":"Enterprise Database Solutions"},{"location":"data-management/database-overview/#database-clients","text":"For these more technical solutions, you'll need a database management client. Some database solutions come with a preferred client, such as Microsoft's SQL Server Management Studio and Azure Data Studio , Oracle's SQL Developer , PostreSQL's pgAdmin , and MySQL's MySQL Workbench . However, you're not stuck with these options, some of these can be used for other databases, while a number of popular free alternative options exist including SQuirrel SLQ , DataGrip , or even VSCode with mssql extension (for MySQL databases; however see Azure Data Studio as a better solution built from the VSCode platform).","title":"Database Clients"},{"location":"data-management/database-overview/#how-to-choose","text":"With so many options, how do you choose? This section will help you pick the right one.","title":"How to Choose?"},{"location":"data-management/database-overview/#budget","text":"Consider both the cost of the software itself and the cost of configuring and maintaining the database.","title":"Budget"},{"location":"data-management/database-overview/#scalability","text":"Unless you're dealing with *Big Data*, you won't need to worry about the database growing too large to fit on a single server. The only likely exception would be if attempting to store geospatial data within a relational database outside of ArcGIS. If you go with a cloud-based solution you can scale as much as you want, provided you can pay for it.","title":"Scalability"},{"location":"data-management/database-overview/#hosting-integration","text":"If the database will be hosted and managed by the client or their website manager, your options will likely be limited to those that integrate with their existing systems. Work with them when selecting a database and during database design. In most other cases, the database will live on the client's local server (unless you picked a cloud-based alternative).","title":"Hosting &amp; Integration"},{"location":"data-management/database-overview/#maintenance-support","text":"Who will maintain the database over time? If it's the client, make sure they are comfortable with the solution. Who will provide support if something crashes or the client needs a new type of report developed? If it's you, make sure you are comfortable with the solution. Keep in mind that paying a contractor to set up a cool web-based database will likely require paying a contractor to maintain that database periodically over time.","title":"Maintenance &amp; Support"},{"location":"data-management/database-overview/#final-thoughts","text":"Setting up a database is a big step in the maturation of any data-driven program. The right database solution will keep your data safe and secure while also providing access to the data for analysis and reporting. When choosing a database, the best strategy is to keep it as simple as possible. Important! If you've decided that you're not the right person to decide which database to use, that's ok. While someone with more technical experience will be better able to select the right database solution, they may not know enough about your program to design the database and the forms, queries and reports you need. You're the expert on your program, stay involved in the process to make sure the final solution meets your needs!","title":"Final Thoughts"},{"location":"data-management/notes/","text":"Database Design Conceptual Data Model: must reflect the actual and possible states of the outside world. For example, if people can have two phone numbers, this must be possible. ER-Model Unified Modelling Language Schema/Logical Data Model: actual implementation of the conceptual data model in the chosen database technology. Normalization: According to wikipeda, \"each elementary 'fact' is recorded in one place so that insertions, updates, and deletions automatically maintain consistency\". Physical database design: affects performance, scalability, recovery, security, etc. Security Views","title":"Notes"},{"location":"git/initializing-git/","text":"This section describes how to begin a new project and track it on Github. We have two options: Create the project locally and push it up to Github* Create the project on Github first and clone it down to your computer *If you already have a project started, you've already started locally. Starting Locally \u00b6 If you have already started a project or prefer to work from your local machine first, follow these steps: 1. Create the project directory \u00b6 In Windows Explorer, navigate to the folder in which you will create your project directory (or the root of an existing project directory). Right-click and select 'GitBash Here' to open an instance of Bash. Skip the following paragraph if you already have a project started . Use the command mkdir <project-name> (replace <project-name> with the name of the directory for the project) to create a folder. Next, use the command cd <project-name> to move into that folder. 2. Initialize git \u00b6 Use the command git init to initialize git. A new repository named .git will be created within the directory which stores all of the git version control. You don't need to have GitBash open during your editing sessions, this folder will take care of all of your version control. If for some reason you want to stop tracking with git and lose all of your previous versions, just delete this folder. 3. Create a .gitignore and README.md file \u00b6 Use the command touch .gitignore to create a new file named .gitignore. This file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. Use the command touch README.md to create a README file. This file will automatically be displayed on the repository page on Github. Using the Markdown language, populate this README file with important info as necessary. 4. Commit your initial file(s) \u00b6 First, we'll need to add all files to be tracked by git. Use the command: 1 git add -A Next we'll use a commit to get the first batch of files/folders into our git repository. Use the following command: 1 git commit -m \"Initial Commit\" You will learn more about commits and other features of git in the Using Git section. 5. Create a Github repository \u00b6 Navigate to your Github.com page and login. Click the green Create New Repository button. Name it with the same name you used in step 1 for the project directory. Provide a description. The repository will be public. DO NOT create a README or .gitignore file. After you click 'Ok', Github will provide instructions for importing your project files into the repository. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. You'll use two commands in GitBash to accomplish this: 1 2 git remote add origin <url> git push -u origin master --tags The first line establishes a remote connection to the repository. Replace the url with the url provided by github to your project repository. Note that you can change the name 'origin' to anything you'd like, but origin is used by convention. You will be prompted to sign into your Github account. The second line 'pushes' your files/folders up to Github. The -u flag tells Github to track these files/folders along with master, while the --tags flag will move any messages from previous commits. If this fails, it may be because you already have a remote established named 'origin'. Try git remove rm origin to remove any existing connections, or change the name origin to something unique. To confirm your remote connection was established, use the command git remote -v . 6. Create your project structure and begin coding! \u00b6 At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work. Starting Remote \u00b6 In this option, we'll start by creating a repository on Github and then clone that down to our computer. You can copy files/folders into this newly created repository if you'd like. 1. Create a Gihub repository \u00b6 Navigate to your Github.com page and login. Click the green Create New Repository button. Provide a description. The repository will be public. Create a README and .gitignore file. Optionally add a license. 2. Clone the remote repository \u00b6 Within the project repository on Github, click the 'Clone or Download' button. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. In Windows Explorer, navigate to the folder in which you will create your project directory. Right-click and select 'GitBash Here' to open an instance of Bash. Clone the repository into this folder with the command: 1 git clone <url> where you replace <url> with the copied url (use Shift+Insert to paste into Bash). You may provide a name for the project following the url if you'd like the folder on your local drive to have a different name than the repository on Github. Run ls to confirm that the repository was created. Type the command cd <repository-name> to move into the repository folder (replace <repository name> with the correct name). If you need to copy files/folders into the repository, use the command cp -R <~/path/> . . Replace <~/path/> with the correct path of the source folder/file. The -R flag signifies recursive and copies everything from within the folders as well. The . simply means the current directory, so your copying everything from the path provided to the current directory. 3. Populate the .gitignore file \u00b6 The .gitignore file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. 4. Create your project structure and begin coding! \u00b6 At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work. Additional Resources \u00b6 Corey Shafer's excellent series on Git","title":"Create Project"},{"location":"git/initializing-git/#starting-locally","text":"If you have already started a project or prefer to work from your local machine first, follow these steps:","title":"Starting Locally"},{"location":"git/initializing-git/#1-create-the-project-directory","text":"In Windows Explorer, navigate to the folder in which you will create your project directory (or the root of an existing project directory). Right-click and select 'GitBash Here' to open an instance of Bash. Skip the following paragraph if you already have a project started . Use the command mkdir <project-name> (replace <project-name> with the name of the directory for the project) to create a folder. Next, use the command cd <project-name> to move into that folder.","title":"1. Create the project directory"},{"location":"git/initializing-git/#2-initialize-git","text":"Use the command git init to initialize git. A new repository named .git will be created within the directory which stores all of the git version control. You don't need to have GitBash open during your editing sessions, this folder will take care of all of your version control. If for some reason you want to stop tracking with git and lose all of your previous versions, just delete this folder.","title":"2. Initialize git"},{"location":"git/initializing-git/#3-create-a-gitignore-and-readmemd-file","text":"Use the command touch .gitignore to create a new file named .gitignore. This file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore. Use the command touch README.md to create a README file. This file will automatically be displayed on the repository page on Github. Using the Markdown language, populate this README file with important info as necessary.","title":"3. Create a .gitignore and README.md file"},{"location":"git/initializing-git/#4-commit-your-initial-files","text":"First, we'll need to add all files to be tracked by git. Use the command: 1 git add -A Next we'll use a commit to get the first batch of files/folders into our git repository. Use the following command: 1 git commit -m \"Initial Commit\" You will learn more about commits and other features of git in the Using Git section.","title":"4. Commit your initial file(s)"},{"location":"git/initializing-git/#5-create-a-github-repository","text":"Navigate to your Github.com page and login. Click the green Create New Repository button. Name it with the same name you used in step 1 for the project directory. Provide a description. The repository will be public. DO NOT create a README or .gitignore file. After you click 'Ok', Github will provide instructions for importing your project files into the repository. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. You'll use two commands in GitBash to accomplish this: 1 2 git remote add origin <url> git push -u origin master --tags The first line establishes a remote connection to the repository. Replace the url with the url provided by github to your project repository. Note that you can change the name 'origin' to anything you'd like, but origin is used by convention. You will be prompted to sign into your Github account. The second line 'pushes' your files/folders up to Github. The -u flag tells Github to track these files/folders along with master, while the --tags flag will move any messages from previous commits. If this fails, it may be because you already have a remote established named 'origin'. Try git remove rm origin to remove any existing connections, or change the name origin to something unique. To confirm your remote connection was established, use the command git remote -v .","title":"5. Create a Github repository"},{"location":"git/initializing-git/#6-create-your-project-structure-and-begin-coding","text":"At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work.","title":"6. Create your project structure and begin coding!"},{"location":"git/initializing-git/#starting-remote","text":"In this option, we'll start by creating a repository on Github and then clone that down to our computer. You can copy files/folders into this newly created repository if you'd like.","title":"Starting Remote"},{"location":"git/initializing-git/#1-create-a-gihub-repository","text":"Navigate to your Github.com page and login. Click the green Create New Repository button. Provide a description. The repository will be public. Create a README and .gitignore file. Optionally add a license.","title":"1. Create a Gihub repository"},{"location":"git/initializing-git/#2-clone-the-remote-repository","text":"Within the project repository on Github, click the 'Clone or Download' button. Use HTTP or SSH if you have it set up. Copy the url provided to your clipboard. In Windows Explorer, navigate to the folder in which you will create your project directory. Right-click and select 'GitBash Here' to open an instance of Bash. Clone the repository into this folder with the command: 1 git clone <url> where you replace <url> with the copied url (use Shift+Insert to paste into Bash). You may provide a name for the project following the url if you'd like the folder on your local drive to have a different name than the repository on Github. Run ls to confirm that the repository was created. Type the command cd <repository-name> to move into the repository folder (replace <repository name> with the correct name). If you need to copy files/folders into the repository, use the command cp -R <~/path/> . . Replace <~/path/> with the correct path of the source folder/file. The -R flag signifies recursive and copies everything from within the folders as well. The . simply means the current directory, so your copying everything from the path provided to the current directory.","title":"2. Clone the remote repository"},{"location":"git/initializing-git/#3-populate-the-gitignore-file","text":"The .gitignore file will be used by git to ignore files in the project that should not be tracked. Use the command code .gitignore to open VSCode to edit the file (or however you would open and edit files). Using one line per folder or file, list all folders or files to be ignored. Wildcards (i.e., globbing patterns) are honored. Each project will be different but consider including *.pyc , venv , idea . See this GitHub help page for more on .gitignore files and this repo for suggested files to ignore.","title":"3. Populate the .gitignore file"},{"location":"git/initializing-git/#4-create-your-project-structure-and-begin-coding","text":"At this point, you may wish to switch into your IDE and open the project folder to build your project architecture and begin coding. Follow the workflow described in the Using Git section as you work.","title":"4. Create your project structure and begin coding!"},{"location":"git/initializing-git/#additional-resources","text":"Corey Shafer's excellent series on Git","title":"Additional Resources"},{"location":"git/notes/","text":"HELP ME PLEASE! Setting Up Git & GitHub \u00b6 Download Git \u00b6 git-scm.com Download & run installer git --version on cmd to make sure its successfully installed Configure Git \u00b6 git config --global user.name <name> git config --global user.email <email> git config --list to see all settings Git Log git log <- all commits git show <- last commit git ls-files <- lists all files that git is tracking git log --oneline --graph --decorate --all <-more detailed history of commits To create an alias for the git history command \u00b6 git config --global alias.hist \"log --oneline --graph --decorate --all\" now use git hist to see the same command, note it still accepts additional arguments (for example, provide --filename to see history for one file) History will be served line by line, type q to quit at any time Setup editor (VSCode should be already available as 'code') Add full folder path that includes executable to system path environment variable, separate with semi-colon Restart Bash Create alias for editor notepad++ ~/.bash_profile alias npp='notepad++ -mulitInst - nosession' git config --global core.editor \"notepad++ -multiInst -nosession\" git config --global -e Setup Diff & Merge Tool This section describes how to use p4merge. Use VS Code instead git config --global diff.tool p4merge git config --gloabl difftool.p4merge.path \"C:/... /p4merge.exe git config --global difftool.prompt false git config --global merge.tool p4merge git config --global mergetool.p4merge.path \"C:/.../p4merge.exe git config --global mergetool.prompt false Git Workflow \u00b6 Common workflow on master \u00b6 Navigate to root project folder in Windows Explorer Right-click and select 'Git Bash Here' from context menu git status commit and push any changes if found, but usually wouldn't be Work on a feature You can open, add, and remove files using the Windows Explorer GUI or using unix commands in Git Bash Commit changes locally If new files were added, git add -A then git commit -m \"Message\" Otherwise, express commit git commit -am \"Message\" Push to GitHub git pull origin master in case others are working on it git push origin master Common workflow on branch \u00b6 Creating and committing a branch \u00b6 git branch <name> <- creates branch git checkout <branch name> <- checks out make changes git add -A git commit -m \"Message\" git push -u origin <branch name> (first time only) git branch -a (to confirm) Merging to master \u00b6 git checkout master git pull origin master git branch --merged (which branches have been merged?) git merge <branch name> git push origin master git branch -d <branch name> (to delete branch locally) git push origin --delete <branch name> (to delete the remote branch) Workflow Commands \u00b6 Adding files to staging area Use git add <filename> to add file Use git add -A to add everything in the working directory (Use git reset <filename> to remove from the staging area or git reset to remove everything) Regular commit git commit -m \"Message\" If -m \"Message\" is excluded, the default editor will be opened and a message should be inputted. No need to add quotes or anything else. Line breaks will be ignored. Express commit git commit -am \"Message\" For any files already tracked (use git ls-files if unsure or git status to see what's in staging area) Renaming files git mv <file1> <file2> git commit -m \"message\" also can use git add -A if changes (renames, deletions) made outside of git Delete files (from git tracking) git rm filename git commit -m \"message\" also can use git add -u if files deleted outside of git Cloning remote repositories git clone <url> git remote -v <- shows remote connection git branch -a <- shows branches in repository Pushing git diff <- shows changes Example workflow Other Common Commands \u00b6 Help git <action> --help Remove file after adding to .gitignore 1 2 3 git rm -r --cached <file or folder name> git commit -m \"Removed files message\" git push origin master List files git ls-files Open file with default editor start <file name> Open file with VSCode code <file name> (set up default editors with aliases or use start and ensure preferred editor is system default) Diffs & Merges \u00b6 If you encounter a merge issue after pulling from the repo, and it can't be automatically merged, open the file in VSCode and accept/reject changes (will be highlighted in VSCode). code <mergefile> Then save and close the editor. You can now add the file to the staging area and push all changes to the repo ( git commit -m \"Fixed merge conflict\" > git push ). Working Together \u00b6 There are two common ways of collaborating: Fork the repo and submit pull requests to the repo owner Add Collaborators to your repo to give others push authority Forking the repo and submitting pull requests is the safest, as the repo owner is in charge of reviewing all proposed changes before integrating them into the repo. However, that can create a lot of work for the repo owner depending on the frequency of commits. Adding Collaborators can be done in the Settings tab of a repo. This allows anyone listed as a collaborator to work on the repo as if it was their own. This will streamline the workflow, but you risk missing simple mistakes, severe mistakes, and malicious intent. https://kbroman.org/github_tutorial/pages/fork.html","title":"Notes"},{"location":"git/notes/#setting-up-git-github","text":"","title":"Setting Up Git &amp; GitHub"},{"location":"git/notes/#download-git","text":"git-scm.com Download & run installer git --version on cmd to make sure its successfully installed","title":"Download Git"},{"location":"git/notes/#configure-git","text":"git config --global user.name <name> git config --global user.email <email> git config --list to see all settings Git Log git log <- all commits git show <- last commit git ls-files <- lists all files that git is tracking git log --oneline --graph --decorate --all <-more detailed history of commits","title":"Configure Git"},{"location":"git/notes/#to-create-an-alias-for-the-git-history-command","text":"git config --global alias.hist \"log --oneline --graph --decorate --all\" now use git hist to see the same command, note it still accepts additional arguments (for example, provide --filename to see history for one file) History will be served line by line, type q to quit at any time Setup editor (VSCode should be already available as 'code') Add full folder path that includes executable to system path environment variable, separate with semi-colon Restart Bash Create alias for editor notepad++ ~/.bash_profile alias npp='notepad++ -mulitInst - nosession' git config --global core.editor \"notepad++ -multiInst -nosession\" git config --global -e Setup Diff & Merge Tool This section describes how to use p4merge. Use VS Code instead git config --global diff.tool p4merge git config --gloabl difftool.p4merge.path \"C:/... /p4merge.exe git config --global difftool.prompt false git config --global merge.tool p4merge git config --global mergetool.p4merge.path \"C:/.../p4merge.exe git config --global mergetool.prompt false","title":"To create an alias for the git history command"},{"location":"git/notes/#git-workflow","text":"","title":"Git Workflow"},{"location":"git/notes/#common-workflow-on-master","text":"Navigate to root project folder in Windows Explorer Right-click and select 'Git Bash Here' from context menu git status commit and push any changes if found, but usually wouldn't be Work on a feature You can open, add, and remove files using the Windows Explorer GUI or using unix commands in Git Bash Commit changes locally If new files were added, git add -A then git commit -m \"Message\" Otherwise, express commit git commit -am \"Message\" Push to GitHub git pull origin master in case others are working on it git push origin master","title":"Common workflow on master"},{"location":"git/notes/#common-workflow-on-branch","text":"","title":"Common workflow on branch"},{"location":"git/notes/#creating-and-committing-a-branch","text":"git branch <name> <- creates branch git checkout <branch name> <- checks out make changes git add -A git commit -m \"Message\" git push -u origin <branch name> (first time only) git branch -a (to confirm)","title":"Creating and committing a branch"},{"location":"git/notes/#merging-to-master","text":"git checkout master git pull origin master git branch --merged (which branches have been merged?) git merge <branch name> git push origin master git branch -d <branch name> (to delete branch locally) git push origin --delete <branch name> (to delete the remote branch)","title":"Merging to master"},{"location":"git/notes/#workflow-commands","text":"Adding files to staging area Use git add <filename> to add file Use git add -A to add everything in the working directory (Use git reset <filename> to remove from the staging area or git reset to remove everything) Regular commit git commit -m \"Message\" If -m \"Message\" is excluded, the default editor will be opened and a message should be inputted. No need to add quotes or anything else. Line breaks will be ignored. Express commit git commit -am \"Message\" For any files already tracked (use git ls-files if unsure or git status to see what's in staging area) Renaming files git mv <file1> <file2> git commit -m \"message\" also can use git add -A if changes (renames, deletions) made outside of git Delete files (from git tracking) git rm filename git commit -m \"message\" also can use git add -u if files deleted outside of git Cloning remote repositories git clone <url> git remote -v <- shows remote connection git branch -a <- shows branches in repository Pushing git diff <- shows changes Example workflow","title":"Workflow Commands"},{"location":"git/notes/#other-common-commands","text":"Help git <action> --help Remove file after adding to .gitignore 1 2 3 git rm -r --cached <file or folder name> git commit -m \"Removed files message\" git push origin master List files git ls-files Open file with default editor start <file name> Open file with VSCode code <file name> (set up default editors with aliases or use start and ensure preferred editor is system default)","title":"Other Common Commands"},{"location":"git/notes/#diffs-merges","text":"If you encounter a merge issue after pulling from the repo, and it can't be automatically merged, open the file in VSCode and accept/reject changes (will be highlighted in VSCode). code <mergefile> Then save and close the editor. You can now add the file to the staging area and push all changes to the repo ( git commit -m \"Fixed merge conflict\" > git push ).","title":"Diffs &amp; Merges"},{"location":"git/notes/#working-together","text":"There are two common ways of collaborating: Fork the repo and submit pull requests to the repo owner Add Collaborators to your repo to give others push authority Forking the repo and submitting pull requests is the safest, as the repo owner is in charge of reviewing all proposed changes before integrating them into the repo. However, that can create a lot of work for the repo owner depending on the frequency of commits. Adding Collaborators can be done in the Settings tab of a repo. This allows anyone listed as a collaborator to work on the repo as if it was their own. This will streamline the workflow, but you risk missing simple mistakes, severe mistakes, and malicious intent. https://kbroman.org/github_tutorial/pages/fork.html","title":"Working Together"},{"location":"metrics-design/overview/","text":"Metrics Design Philosophy \u00b6 Our metrics design philosophy has been developed and tested through many years building solutions for clients and internal initiatives alike. It is based on a variety of different frameworks, from the Open Standards for the Practice of Conservation to Systems Thinking to Human-Centered Design (see list of inspirations below). To be honest, very little in this philosophy is new. What is unique about it, however, is its synthesis of these various frameworks. Human-centered design typically assumes you already have your solution, and are simply adding a new feature. Open Standards/Systems Thinking are focused on identifying the right place to intervene - but are, in my opinion, lacking in specifics as to how to design and implement a successful solution. Bringing the concepts within these processes together creates a more wholistic, and hopefully impactful, approach to conservation. So how is it unique? \u00b6 There's a few key characteristics of this philosophy that make it unique. People-first : This philosophy is unapologetically people-centric. We seek to change people's behavior to achieve our conservation targets. We do that by solving people's problems or creating new opportunities for people. If changing human behavior isn't at the core of your problem or central to the success of your strategy, this isn't the right process for you. Evidence-based : Prepare to test every assumption! We seek evidence from the very beginning of the process - we do not wait until we've implemented the program to start testing our assumptions. Importantly, we do not even entertain solutions until we have strong evidence that the problem exists and that solving it will help achieve our conservation target. And we never commit to our solution unless we see results in the real world. Inverted : If you're used to spending years developing the perfect program, creating a monitoring plan, and then seeing what happens, this might be disorienting. Instead, you will build the minimum solution required to begin testing assumptions. This might actually look a lot different than the full solution you're trying to build, that's ok. You'll greatly reduce the chances of wasting your time - or even making the problem worse - if you fail fast and fail small , as some like to say. Once you've found success, iterate and scale up. The Process \u00b6 We've codified our philosophy into a seven step process. Each step is described in more detail on the pages linked through here. Also, note that there are templates, additional guidance, and other resources available in Additional Resources , many of which are referenced within these linked pages. Process Steps \u00b6 \u200b Step 1 : Start at the End \u200b Step 2 : Map It \u200b Step 3 : Develop Empathy \u200b Step 4 : Define the Problem \u200b Step 5 : Identify a Solution \u200b Step 6 : Implement \u200b Step 7 : Iterate & Adapt Additional Resources \u00b6 \u200b Persona Development Guide \u200b Interview Guide Inspiration \u00b6 These resources and frameworks were leaned on heavily in the development of this approach. Open Standards for the Practice of Conservation Outcomes Mapping ( 1 ) ( 2 ) ( 3 ) & EI Speed Training Systems Thinking ( Thinking in Systems , Systems Thinking for Social Change , Omidyar Systems Practice Workbook) Design Thinking & Lean UX ( Design Sprint , IDEO , Venture Design , The Lean Startup ) Transformative Scenario Planning (Adam Kahane) Structured Decision Making","title":"Overview"},{"location":"metrics-design/overview/#metrics-design-philosophy","text":"Our metrics design philosophy has been developed and tested through many years building solutions for clients and internal initiatives alike. It is based on a variety of different frameworks, from the Open Standards for the Practice of Conservation to Systems Thinking to Human-Centered Design (see list of inspirations below). To be honest, very little in this philosophy is new. What is unique about it, however, is its synthesis of these various frameworks. Human-centered design typically assumes you already have your solution, and are simply adding a new feature. Open Standards/Systems Thinking are focused on identifying the right place to intervene - but are, in my opinion, lacking in specifics as to how to design and implement a successful solution. Bringing the concepts within these processes together creates a more wholistic, and hopefully impactful, approach to conservation.","title":"Metrics Design Philosophy"},{"location":"metrics-design/overview/#so-how-is-it-unique","text":"There's a few key characteristics of this philosophy that make it unique. People-first : This philosophy is unapologetically people-centric. We seek to change people's behavior to achieve our conservation targets. We do that by solving people's problems or creating new opportunities for people. If changing human behavior isn't at the core of your problem or central to the success of your strategy, this isn't the right process for you. Evidence-based : Prepare to test every assumption! We seek evidence from the very beginning of the process - we do not wait until we've implemented the program to start testing our assumptions. Importantly, we do not even entertain solutions until we have strong evidence that the problem exists and that solving it will help achieve our conservation target. And we never commit to our solution unless we see results in the real world. Inverted : If you're used to spending years developing the perfect program, creating a monitoring plan, and then seeing what happens, this might be disorienting. Instead, you will build the minimum solution required to begin testing assumptions. This might actually look a lot different than the full solution you're trying to build, that's ok. You'll greatly reduce the chances of wasting your time - or even making the problem worse - if you fail fast and fail small , as some like to say. Once you've found success, iterate and scale up.","title":"So how is it unique?"},{"location":"metrics-design/overview/#the-process","text":"We've codified our philosophy into a seven step process. Each step is described in more detail on the pages linked through here. Also, note that there are templates, additional guidance, and other resources available in Additional Resources , many of which are referenced within these linked pages.","title":"The Process"},{"location":"metrics-design/overview/#process-steps","text":"\u200b Step 1 : Start at the End \u200b Step 2 : Map It \u200b Step 3 : Develop Empathy \u200b Step 4 : Define the Problem \u200b Step 5 : Identify a Solution \u200b Step 6 : Implement \u200b Step 7 : Iterate & Adapt","title":"Process Steps"},{"location":"metrics-design/overview/#additional-resources","text":"\u200b Persona Development Guide \u200b Interview Guide","title":"Additional Resources"},{"location":"metrics-design/overview/#inspiration","text":"These resources and frameworks were leaned on heavily in the development of this approach. Open Standards for the Practice of Conservation Outcomes Mapping ( 1 ) ( 2 ) ( 3 ) & EI Speed Training Systems Thinking ( Thinking in Systems , Systems Thinking for Social Change , Omidyar Systems Practice Workbook) Design Thinking & Lean UX ( Design Sprint , IDEO , Venture Design , The Lean Startup ) Transformative Scenario Planning (Adam Kahane) Structured Decision Making","title":"Inspiration"},{"location":"metrics-design/step1-start-at-the-end/","text":"Step 1: Start at the End \u00b6 Describe the world as it would be if the problem were solved. Be agnostic about the solution, just describe your vision for world. A vision statement is a good place to start. A vision statement should meet the following criteria: Idealistic About the future Observable Not about the intervention Don\u2019t wordsmith the vision statement quite yet. It\u2019s just intended to provide a guiding light or goal post throughout this process. You may choose to re-write it later once you better understand the problem. If using the Open Standards, include in your vision the conservation targets . Don\u2019t worry about defining measurable goals yet, unless they are already defined. If desired, write down design principles to guide you through this process. These principles should reflect your values and any constraints on the solution; they will guide decision making throughout the process. See examples here . Other Techniques \u00b6 Ask a question: If you\u2019re not quite sure how you think the world could be improved, but sense that there is room for improvement, just ask yourself the question: how might this be better? Could we do this in a different (e.g., more efficient/equitable) way? Keep asking yourself questions until you have a good sense of what can be changed. Asking the right design question is essential to solving the right problem. Bug list: another approach to visualizing a better world is writing down all of the things that bug you about the current situation. Is one of those \u2018bugs\u2019 sufficiently problematic that it\u2019s worth investing time and resources in addressing? More Info on This Step \u00b6 See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets. They define a vision statement as relatively general, visionary, and brief. See Omidyar Group\u2019s Systems Practice section Set Your Goals.","title":"Step 1: Start at the End"},{"location":"metrics-design/step1-start-at-the-end/#step-1-start-at-the-end","text":"Describe the world as it would be if the problem were solved. Be agnostic about the solution, just describe your vision for world. A vision statement is a good place to start. A vision statement should meet the following criteria: Idealistic About the future Observable Not about the intervention Don\u2019t wordsmith the vision statement quite yet. It\u2019s just intended to provide a guiding light or goal post throughout this process. You may choose to re-write it later once you better understand the problem. If using the Open Standards, include in your vision the conservation targets . Don\u2019t worry about defining measurable goals yet, unless they are already defined. If desired, write down design principles to guide you through this process. These principles should reflect your values and any constraints on the solution; they will guide decision making throughout the process. See examples here .","title":"Step 1: Start at the End"},{"location":"metrics-design/step1-start-at-the-end/#other-techniques","text":"Ask a question: If you\u2019re not quite sure how you think the world could be improved, but sense that there is room for improvement, just ask yourself the question: how might this be better? Could we do this in a different (e.g., more efficient/equitable) way? Keep asking yourself questions until you have a good sense of what can be changed. Asking the right design question is essential to solving the right problem. Bug list: another approach to visualizing a better world is writing down all of the things that bug you about the current situation. Is one of those \u2018bugs\u2019 sufficiently problematic that it\u2019s worth investing time and resources in addressing?","title":"Other Techniques"},{"location":"metrics-design/step1-start-at-the-end/#more-info-on-this-step","text":"See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets. They define a vision statement as relatively general, visionary, and brief. See Omidyar Group\u2019s Systems Practice section Set Your Goals.","title":"More Info on This Step"},{"location":"metrics-design/step2-map-it/","text":"Step 2: Map It \u00b6 Documenting your understanding of the problem space in a visual way is an impactful way of capturing your understanding and can be a good way of sharing it with others. A Situation Model , Systems Map , or Journey Map are all great tools. The Situation Model focuses on relationships between human causes (drivers) of threats to your vision (conservation targets); whereas the Systems Map describes feedback loops between relevant elements of the system. Journey Maps capture the chain of behaviors required to solve a problem or do a job. If you can, start by defining the scope of the system that you will be exploring, such as geographic boundaries or levels (e.g., federal vs. state gov\u2019t). This will help focus your search to just that area of the problem space in which you have a manageable interest . Alternatively, use a framing question to focus your interrogation of the system. Developing these maps typically requires formative research and immersion in the problem space. Start by listing the factors (i.e., threats, drivers and enabling conditions ) that you know and building out the map using a breadth-first search of the problem space. Add to the map anything that seems relevant, but don\u2019t spend too much time understanding how. Group items using affinity mapping until the relationships become clear. Next, focus on what appear to be the most important elements with an in-depth search. Ask an expert: experts can quickly help parse what is important and what is not, but be aware of the bias any expert brings. When interviewing experts, research enough that they don\u2019t feel obligated to spend all of their time educating you on the basics\u2014you want them to really focus on the nuance that they have come to understand by becoming an expert. Write down your findings and document your sources in brief narrative format; don\u2019t waste time developing a long report. Finally, connect the elements based on their relationships. Identify central nodes, leverage points, etc. Explore the feedback loops present between factors, including both positive (reinforcing) and negative (balancing) loops. The map you create can be a good communication tool, but it is often only helpful to those who created it. To really communicate your understanding, you must tell a story for the relevant relationships you have identified. In a Situation Model, describe the key pathways from drivers to threats and impacts on targets. In a Systems Map, describe the major balancing and reinforcing feedback loops. Transformative Scenario Planning relies almost entirely on story-telling to shape understanding. If you can tell a convincing story about what is causing the problem, you will be better able to test and communicate that understanding. More on This Step \u00b6 See Thinking in Systems by Donella Meadows, Chapter 5 for a discussion on the challenges of appropriately defining scope. See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets for a general discussion of scope. Scopes are both \u201cplace-based\u201d and \u201cthematic-based\u201d.See CMP-Open Standards v3.0 section 1C-1D for more on Situation Models. See publication Building Ecosystem Services Conceptual Models (Olander et al) for helpful guidance. See Systems Practice by Omidyar Group, section Gaining Clarity . Context Mapping (Historical, Environmental, Societal/Cultural, Technological, Political) Systems Maps or Conceptual Models? \u00b6 One drawback of simple causal conceptual models typically depicted by Open Standards frameworks is that they imply single and unidirectional causalities between specific pressures and ecosystem conditions (Schwartz et al 2012; Niemeijer and de Groot 2008; Smeets and Weterings 1999). In reality, a specific pressure may affect multiple states and a specific state may be affected by multiple pressures. Instead of simple causal chain frameworks, Niemeijer and de Groot (2008) proposed that complex systems are better represented by causal networks, in which multiple causal chains interact and interconnect.","title":"Step 2: Map It"},{"location":"metrics-design/step2-map-it/#step-2-map-it","text":"Documenting your understanding of the problem space in a visual way is an impactful way of capturing your understanding and can be a good way of sharing it with others. A Situation Model , Systems Map , or Journey Map are all great tools. The Situation Model focuses on relationships between human causes (drivers) of threats to your vision (conservation targets); whereas the Systems Map describes feedback loops between relevant elements of the system. Journey Maps capture the chain of behaviors required to solve a problem or do a job. If you can, start by defining the scope of the system that you will be exploring, such as geographic boundaries or levels (e.g., federal vs. state gov\u2019t). This will help focus your search to just that area of the problem space in which you have a manageable interest . Alternatively, use a framing question to focus your interrogation of the system. Developing these maps typically requires formative research and immersion in the problem space. Start by listing the factors (i.e., threats, drivers and enabling conditions ) that you know and building out the map using a breadth-first search of the problem space. Add to the map anything that seems relevant, but don\u2019t spend too much time understanding how. Group items using affinity mapping until the relationships become clear. Next, focus on what appear to be the most important elements with an in-depth search. Ask an expert: experts can quickly help parse what is important and what is not, but be aware of the bias any expert brings. When interviewing experts, research enough that they don\u2019t feel obligated to spend all of their time educating you on the basics\u2014you want them to really focus on the nuance that they have come to understand by becoming an expert. Write down your findings and document your sources in brief narrative format; don\u2019t waste time developing a long report. Finally, connect the elements based on their relationships. Identify central nodes, leverage points, etc. Explore the feedback loops present between factors, including both positive (reinforcing) and negative (balancing) loops. The map you create can be a good communication tool, but it is often only helpful to those who created it. To really communicate your understanding, you must tell a story for the relevant relationships you have identified. In a Situation Model, describe the key pathways from drivers to threats and impacts on targets. In a Systems Map, describe the major balancing and reinforcing feedback loops. Transformative Scenario Planning relies almost entirely on story-telling to shape understanding. If you can tell a convincing story about what is causing the problem, you will be better able to test and communicate that understanding.","title":"Step 2: Map It"},{"location":"metrics-design/step2-map-it/#more-on-this-step","text":"See Thinking in Systems by Donella Meadows, Chapter 5 for a discussion on the challenges of appropriately defining scope. See CMP-Open Standards v3.0 section 1B Define Scope, Vision, and Conservation Targets for a general discussion of scope. Scopes are both \u201cplace-based\u201d and \u201cthematic-based\u201d.See CMP-Open Standards v3.0 section 1C-1D for more on Situation Models. See publication Building Ecosystem Services Conceptual Models (Olander et al) for helpful guidance. See Systems Practice by Omidyar Group, section Gaining Clarity . Context Mapping (Historical, Environmental, Societal/Cultural, Technological, Political)","title":"More on This Step"},{"location":"metrics-design/step2-map-it/#systems-maps-or-conceptual-models","text":"One drawback of simple causal conceptual models typically depicted by Open Standards frameworks is that they imply single and unidirectional causalities between specific pressures and ecosystem conditions (Schwartz et al 2012; Niemeijer and de Groot 2008; Smeets and Weterings 1999). In reality, a specific pressure may affect multiple states and a specific state may be affected by multiple pressures. Instead of simple causal chain frameworks, Niemeijer and de Groot (2008) proposed that complex systems are better represented by causal networks, in which multiple causal chains interact and interconnect.","title":"Systems Maps or Conceptual Models?"},{"location":"metrics-design/step3-develop-empathy/","text":"Step 3: Develop Empathy \u00b6 To really achieve the change that will bring about your vision, you must first change peoples\u2019 behavior. You can\u2019t arm rhinos with bazookas or convince sage-grouse to move to the city. The first step is to identify stakeholders\u2014all of the people that can affect or will be affected by the change you want to see. Then, you will seek to develop empathy for key stakeholders and their needs/problems. This will provide insight into a solution that is more likely to be adopted, supported, persist, and create real change. Create a stakeholder list . Search or ask around to build out your list. Consider government agencies, NGOs, academia, individual businesses, industry groups, segments of the population, etc. Group stakeholders to create a manageable list if possible, but try not to obscure the efforts of individual organizations. For example, if Molson-Coors is very active in your area of interest, keep them separate. If your more worried about the impact of beverage companies in general, group them together. As your building your list, capture a sentence or two about why they are relevant so you don\u2019t forget. You can categorize stakeholders based on how directly you can influence them. Stakeholders that you can directly influence are Boundary Partners . Stakeholders that you can work with directly, but can\u2019t influence, are Strategic Partners . Stakeholders that you can influence, but not directly, are your boundary partners\u2019 boundary partners. Stakeholders that are not influenceable and can\u2019t be targeted directly are not relevant (yet). Think of your boundary partners as customers, you need to \u2018sell\u2019 your solution to your boundary partners. Your strategic partners support you and provide components of your solution that you don\u2019t focus on. For each of your boundary partners (potential customers), develop a persona profile . If necessary, you can create multiple personas for a single boundary partner. For example, if you included farmers as a boundary partner, you may need to distinguish between farmers who derive their entire income from farming and hobby farmers. If you are trying to influence a very targeted set of stakeholders, you may simply research the specific people you are targeting, rather than generalized personas. In either case, the process is largely the same. The challenge is drafting personas that contain relevant facts based on truth. While short, personas must be well researched. To do persona research , you can use techniques ethnography and psychographic research methods. Read relevant literature, studies, or surveys that have been conducted, search out their blogs, read their periodicals, imagine a day in the life . Use the framework of thinks/sees/feels/does . Understand their mental models \u2014how do they think about the problem space? Consider their primary interests which most drive their behavior. Develop persona hypotheses \u2014expectations that you have about their character, how they see the world, what their background is, etc. Conduct exploratory interviews (develop an interview guide first) to test your hypotheses (you might combine this with interviews to explore problem hypotheses at the same time, see step 4). Focus on common, not idiosyncratic, characteristics. Refine. Repeat. Don\u2019t stop until you have a clear mental picture of the persona, and you are confident that you have segmented your stakeholders into the right personas. Consider this a process, not a product. Keep updating personas as you continue to understand the problem space and your solution. Good personas are real, exact, actionable, clear, and testable ( REACT ). What do they like to do on the weekend? More on This Step \u00b6 Alex Cowan\u2019s Venture Design process is an excellent resource for persona development. See Appendix II for guidance on developing personas. EI has also developed an Ideal Client Persona Template for external marketing, which can be adapted. See Systems Thinking for Social Change , chapter \u2026, for more on mapping mental models into a systems map. From Our Work \u00b6 In designing the Pollinator Scorecard, we developed personas around three groups: \u2018Wingtips\u2019, \u2018Steel Toes\u2019, and \u2018Tevas\u2019. These user groups divided our target industry in a way that best reflected their uses for, and ability to use, the Pollinator Scorecard. It was helpful to both the project team in designing the Scorecard (by putting us in our users\u2019 \u2018shoes\u2019, so to speak) and when presenting the Scorecard to users to help them understand why we made the design decisions we did.","title":"Step 3: Develop Empathy"},{"location":"metrics-design/step3-develop-empathy/#step-3-develop-empathy","text":"To really achieve the change that will bring about your vision, you must first change peoples\u2019 behavior. You can\u2019t arm rhinos with bazookas or convince sage-grouse to move to the city. The first step is to identify stakeholders\u2014all of the people that can affect or will be affected by the change you want to see. Then, you will seek to develop empathy for key stakeholders and their needs/problems. This will provide insight into a solution that is more likely to be adopted, supported, persist, and create real change. Create a stakeholder list . Search or ask around to build out your list. Consider government agencies, NGOs, academia, individual businesses, industry groups, segments of the population, etc. Group stakeholders to create a manageable list if possible, but try not to obscure the efforts of individual organizations. For example, if Molson-Coors is very active in your area of interest, keep them separate. If your more worried about the impact of beverage companies in general, group them together. As your building your list, capture a sentence or two about why they are relevant so you don\u2019t forget. You can categorize stakeholders based on how directly you can influence them. Stakeholders that you can directly influence are Boundary Partners . Stakeholders that you can work with directly, but can\u2019t influence, are Strategic Partners . Stakeholders that you can influence, but not directly, are your boundary partners\u2019 boundary partners. Stakeholders that are not influenceable and can\u2019t be targeted directly are not relevant (yet). Think of your boundary partners as customers, you need to \u2018sell\u2019 your solution to your boundary partners. Your strategic partners support you and provide components of your solution that you don\u2019t focus on. For each of your boundary partners (potential customers), develop a persona profile . If necessary, you can create multiple personas for a single boundary partner. For example, if you included farmers as a boundary partner, you may need to distinguish between farmers who derive their entire income from farming and hobby farmers. If you are trying to influence a very targeted set of stakeholders, you may simply research the specific people you are targeting, rather than generalized personas. In either case, the process is largely the same. The challenge is drafting personas that contain relevant facts based on truth. While short, personas must be well researched. To do persona research , you can use techniques ethnography and psychographic research methods. Read relevant literature, studies, or surveys that have been conducted, search out their blogs, read their periodicals, imagine a day in the life . Use the framework of thinks/sees/feels/does . Understand their mental models \u2014how do they think about the problem space? Consider their primary interests which most drive their behavior. Develop persona hypotheses \u2014expectations that you have about their character, how they see the world, what their background is, etc. Conduct exploratory interviews (develop an interview guide first) to test your hypotheses (you might combine this with interviews to explore problem hypotheses at the same time, see step 4). Focus on common, not idiosyncratic, characteristics. Refine. Repeat. Don\u2019t stop until you have a clear mental picture of the persona, and you are confident that you have segmented your stakeholders into the right personas. Consider this a process, not a product. Keep updating personas as you continue to understand the problem space and your solution. Good personas are real, exact, actionable, clear, and testable ( REACT ). What do they like to do on the weekend?","title":"Step 3: Develop Empathy"},{"location":"metrics-design/step3-develop-empathy/#more-on-this-step","text":"Alex Cowan\u2019s Venture Design process is an excellent resource for persona development. See Appendix II for guidance on developing personas. EI has also developed an Ideal Client Persona Template for external marketing, which can be adapted. See Systems Thinking for Social Change , chapter \u2026, for more on mapping mental models into a systems map.","title":"More on This Step"},{"location":"metrics-design/step3-develop-empathy/#from-our-work","text":"In designing the Pollinator Scorecard, we developed personas around three groups: \u2018Wingtips\u2019, \u2018Steel Toes\u2019, and \u2018Tevas\u2019. These user groups divided our target industry in a way that best reflected their uses for, and ability to use, the Pollinator Scorecard. It was helpful to both the project team in designing the Scorecard (by putting us in our users\u2019 \u2018shoes\u2019, so to speak) and when presenting the Scorecard to users to help them understand why we made the design decisions we did.","title":"From Our Work"},{"location":"metrics-design/step4-define-the-problem/","text":"Step 4: Define the Problem \u00b6 \u201cGiven one hour to save the planet, I would spend 59 minutes understanding the problem and one minute resolving it.\u201d - Attributed to Albert Einstein (probably incorrectly) You may have started this process thinking you know what the problem is, or what strategy you\u2019ll employ. Do you still? It will do no good to solve the wrong problem; it might actually do more harm! There are many examples of well-meaning groups attempting to solve a problem with their \u2018cure-all\u2019 and seen it fall flat--or worse, exacerbate existing problems. To a hammer, the whole world's a nail \u00b6 Many of the frameworks that inspired this approach suggest defining your group's mission at the outset. We prefer to be fully agnostic as to the solution until the problem is clearly defined and evidence supports your understanding. We recognize that your group has unique strengths and experience with some solutions more than others. If the problem doesn't call for your solution, that's fine, you can go solve a different one. The temptation is simply too great to frame the problem in terms of your solution if you come with preconceptions as to what your strategy will be. So how do we define a problem and develop our solution? Let\u2019s go back to our map. If you\u2019ve developed a systems map, look for leverage points . If you\u2019ve developed a situation model, identify key intervention points --which factor will you focus on? If you haven\u2019t already, map other stakeholders\u2019 interventions onto your map. Are there factors that aren\u2019t receiving enough attention? Where could other interventions be leveraged? Don\u2019t worry about the how (i.e., your strategy) yet. Once you\u2019ve identified the factor you\u2019d like to address, review your stakeholder list. Which of your boundary partners are relevant to this factor? Refer back to their personas and describe the problem scenarios that they face relevant to your understanding of the situation scope. These can be either problems or simply jobs to be done: Farmers aren\u2019t sure where to find information on conservation programs. Agency staff need to develop restoration project specifications but aren\u2019t experts in restoration. CSR managers need to meet biodiversity targets but don\u2019t have time to develop habitat projects. You get the picture. Depending on how broad your scope, this task may seem nebulous. Here are a few options to help clarify: For each persona, draft an outcome challenge --the ideal behavior you want each persona to adopt in a world in which your vision is achieved. You can break this down into progress markers by defining what you would expect to see, like to see, and love to see from each persona. Create a journey map . A journey map illustrates the steps the persona takes to complete a relevant task or solve a relevant problem. For example, if you envision restaurants sustainably sourcing ingredients, map out a typical process for creating procurement policies. Where do they begin? Who else is involved? Within the journey map, highlight pain points --areas where the process is difficult. Those pain points may just turn into the problems you choose to tackle. A behavior chain is similar to a journey map, and outlines the unique, self-contained behaviors necessary to complete a more complex behavior. To identify problems that are actually solvable, consider organizing them as parent, anchor and child scenarios. The parent problem is too big; the child scenario to small. The anchor problem , just right. For each anchor problem, consider alternatives --what is the persona doing now instead of what you want them to do; what other options currently exist that they\u2019re not using? Understanding the alternatives is necessary to understand the baseline and to specify conditions of satisfaction --in other words, how you\u2019ll know your solution is sufficiently better than the alternatives that the user will consider it. Craft problem hypotheses for the problems you most want to address or think are most important. These testable propositions will be evaluated through persona interviews or focus groups (you may want to combine this step with the persona interviews in step 3). Before moving on to the next step, where you will identify potential strategies, it is critical that you have a clear understanding of the problem space and have evidence supporting your hypotheses. By now you should have talked to experts and interviewed at least 5 relevant people. For more substantial endeavors, shoot for 30 people. There is no guarantee that this process will deliver a solution that will help you reach your vision for the world, but the chances of having an impact in a system you don\u2019t understand with people you can\u2019t relate to are slim. Research all at once? \u00b6 To maximize the efficiency of your research process, and avoid having to interview people twice, you may want to hold off on interviews until you've developed both persona hypotheses and problem hypotheses. If you have been working in this space for some time and have accumulated sufficient evidence that you are comfortable proposing a solution at this time, you can even hold off on interviews until the next step, once you have a proposal for a solution.","title":"Step 4: Define the Problem"},{"location":"metrics-design/step4-define-the-problem/#step-4-define-the-problem","text":"\u201cGiven one hour to save the planet, I would spend 59 minutes understanding the problem and one minute resolving it.\u201d - Attributed to Albert Einstein (probably incorrectly) You may have started this process thinking you know what the problem is, or what strategy you\u2019ll employ. Do you still? It will do no good to solve the wrong problem; it might actually do more harm! There are many examples of well-meaning groups attempting to solve a problem with their \u2018cure-all\u2019 and seen it fall flat--or worse, exacerbate existing problems.","title":"Step 4: Define the Problem"},{"location":"metrics-design/step4-define-the-problem/#to-a-hammer-the-whole-worlds-a-nail","text":"Many of the frameworks that inspired this approach suggest defining your group's mission at the outset. We prefer to be fully agnostic as to the solution until the problem is clearly defined and evidence supports your understanding. We recognize that your group has unique strengths and experience with some solutions more than others. If the problem doesn't call for your solution, that's fine, you can go solve a different one. The temptation is simply too great to frame the problem in terms of your solution if you come with preconceptions as to what your strategy will be. So how do we define a problem and develop our solution? Let\u2019s go back to our map. If you\u2019ve developed a systems map, look for leverage points . If you\u2019ve developed a situation model, identify key intervention points --which factor will you focus on? If you haven\u2019t already, map other stakeholders\u2019 interventions onto your map. Are there factors that aren\u2019t receiving enough attention? Where could other interventions be leveraged? Don\u2019t worry about the how (i.e., your strategy) yet. Once you\u2019ve identified the factor you\u2019d like to address, review your stakeholder list. Which of your boundary partners are relevant to this factor? Refer back to their personas and describe the problem scenarios that they face relevant to your understanding of the situation scope. These can be either problems or simply jobs to be done: Farmers aren\u2019t sure where to find information on conservation programs. Agency staff need to develop restoration project specifications but aren\u2019t experts in restoration. CSR managers need to meet biodiversity targets but don\u2019t have time to develop habitat projects. You get the picture. Depending on how broad your scope, this task may seem nebulous. Here are a few options to help clarify: For each persona, draft an outcome challenge --the ideal behavior you want each persona to adopt in a world in which your vision is achieved. You can break this down into progress markers by defining what you would expect to see, like to see, and love to see from each persona. Create a journey map . A journey map illustrates the steps the persona takes to complete a relevant task or solve a relevant problem. For example, if you envision restaurants sustainably sourcing ingredients, map out a typical process for creating procurement policies. Where do they begin? Who else is involved? Within the journey map, highlight pain points --areas where the process is difficult. Those pain points may just turn into the problems you choose to tackle. A behavior chain is similar to a journey map, and outlines the unique, self-contained behaviors necessary to complete a more complex behavior. To identify problems that are actually solvable, consider organizing them as parent, anchor and child scenarios. The parent problem is too big; the child scenario to small. The anchor problem , just right. For each anchor problem, consider alternatives --what is the persona doing now instead of what you want them to do; what other options currently exist that they\u2019re not using? Understanding the alternatives is necessary to understand the baseline and to specify conditions of satisfaction --in other words, how you\u2019ll know your solution is sufficiently better than the alternatives that the user will consider it. Craft problem hypotheses for the problems you most want to address or think are most important. These testable propositions will be evaluated through persona interviews or focus groups (you may want to combine this step with the persona interviews in step 3). Before moving on to the next step, where you will identify potential strategies, it is critical that you have a clear understanding of the problem space and have evidence supporting your hypotheses. By now you should have talked to experts and interviewed at least 5 relevant people. For more substantial endeavors, shoot for 30 people. There is no guarantee that this process will deliver a solution that will help you reach your vision for the world, but the chances of having an impact in a system you don\u2019t understand with people you can\u2019t relate to are slim.","title":"To a hammer, the whole world's a nail"},{"location":"metrics-design/step4-define-the-problem/#research-all-at-once","text":"To maximize the efficiency of your research process, and avoid having to interview people twice, you may want to hold off on interviews until you've developed both persona hypotheses and problem hypotheses. If you have been working in this space for some time and have accumulated sufficient evidence that you are comfortable proposing a solution at this time, you can even hold off on interviews until the next step, once you have a proposal for a solution.","title":"Research all at once?"},{"location":"metrics-design/step5-identify-a-solution/","text":"Step 5: Identify a Solution \u00b6 This is the fun part! In terms of the design \u2018 double diamond ,\u2019 we are at the central convergence point. Starting from our vision, we\u2019ve diverged to explore the problem space and its stakeholders, and have since converged around a problem worth solving. Now, we get to come up with a solution. Call it brainstorming or ideation or, my personal favorite, spitballing . Just don\u2019t take it too seriously and don\u2019t do it alone. There are a plethora of techniques for working in groups to create solutions. Can you invest in a weeklong design sprint ? If not, try this one day version. Use sketching to help you visualize the solution. Timed idea generation , mind mapping , storyboarding , journey maps are all great options to get a creative buzz on. Reframe your problem scenarios into opportunities by asking \u2018 how might we? \u2019. Think about the problem from multiple angles. Create frameworks to help you see the problem with a different lens. Use many model thinking . Consider both extremes and mainstreams . Try coming up with your worst idea . Look at similar projects or other people\u2019s interventions and describe their challenges, insights, and opportunities . Consider the problem and define success --what would a good solution look like? You\u2019re looking for a solution that is all three: desirable, feasible, viable . Ideally one that also gets people in your group excited and overlaps with your unique strengths . Once you\u2019ve arrived at a potential conceptual solution, it\u2019s time to put it to the test. For the people for whom you\u2019re trying to solve this problem, craft a value proposition for each of the problem scenarios you\u2019ve already defined. Also include the current alternative so you know what you\u2019re competing against. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Does the solution appear to be better than the alternative? How do you know? Do you need to test the value proposition to know for sure? Craft value hypotheses for each value proposition that should be tested. You can state a value hypothesis as \u201cIf we do X for Y, they will Z\u201d. Make sure the behavior you want to see is observable. Some value hypotheses can be tested without building any of the proposed solution, others will require a minimum viable product to test--we\u2019ll work on that in the next step. Finally, it\u2019s good practice to summarize the value of your solution in a positioning statement . Fill in the blanks: For [persona A] who need to [problem scenario], our [solution name] is a [product category] that [value proposition]. Unlike [current alternative], our product [key differentiation]. Draft one for each of your primary personas. Now is also the time to develop a Results Chain to clarify assumptions as to why solving this problem will improve the conservation target and identify metrics for evaluating progress.","title":"Step 5: Identify a Solution"},{"location":"metrics-design/step5-identify-a-solution/#step-5-identify-a-solution","text":"This is the fun part! In terms of the design \u2018 double diamond ,\u2019 we are at the central convergence point. Starting from our vision, we\u2019ve diverged to explore the problem space and its stakeholders, and have since converged around a problem worth solving. Now, we get to come up with a solution. Call it brainstorming or ideation or, my personal favorite, spitballing . Just don\u2019t take it too seriously and don\u2019t do it alone. There are a plethora of techniques for working in groups to create solutions. Can you invest in a weeklong design sprint ? If not, try this one day version. Use sketching to help you visualize the solution. Timed idea generation , mind mapping , storyboarding , journey maps are all great options to get a creative buzz on. Reframe your problem scenarios into opportunities by asking \u2018 how might we? \u2019. Think about the problem from multiple angles. Create frameworks to help you see the problem with a different lens. Use many model thinking . Consider both extremes and mainstreams . Try coming up with your worst idea . Look at similar projects or other people\u2019s interventions and describe their challenges, insights, and opportunities . Consider the problem and define success --what would a good solution look like? You\u2019re looking for a solution that is all three: desirable, feasible, viable . Ideally one that also gets people in your group excited and overlaps with your unique strengths . Once you\u2019ve arrived at a potential conceptual solution, it\u2019s time to put it to the test. For the people for whom you\u2019re trying to solve this problem, craft a value proposition for each of the problem scenarios you\u2019ve already defined. Also include the current alternative so you know what you\u2019re competing against. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Does the solution appear to be better than the alternative? How do you know? Do you need to test the value proposition to know for sure? Craft value hypotheses for each value proposition that should be tested. You can state a value hypothesis as \u201cIf we do X for Y, they will Z\u201d. Make sure the behavior you want to see is observable. Some value hypotheses can be tested without building any of the proposed solution, others will require a minimum viable product to test--we\u2019ll work on that in the next step. Finally, it\u2019s good practice to summarize the value of your solution in a positioning statement . Fill in the blanks: For [persona A] who need to [problem scenario], our [solution name] is a [product category] that [value proposition]. Unlike [current alternative], our product [key differentiation]. Draft one for each of your primary personas. Now is also the time to develop a Results Chain to clarify assumptions as to why solving this problem will improve the conservation target and identify metrics for evaluating progress.","title":"Step 5: Identify a Solution"},{"location":"metrics-services/how-we-work/","text":"We've refined our metrics design philosophy to a simple, scalable process to support you in the design and development of your tools and programs. Whether you've identified a new need within an existing project, are setting expectations with a client for a new tool, or simply replacing an old tool with a new one, we can help. Expect to meet at least three times as we work to understand the problem, define a solution, and develop a prototype. Our role can be minimal or more involved, from simply helping you think through the important details to building and supporting the product over the long term. Our process can be scaled to fit your needs large or small, supporting projects including: Building an internal tool Conducting a data analysis Creating a map, data visualization or dashboard Evaluating options for a technology solution Scoping a tool as a product/service for you practice area Developing a prototype prior to working with a technology partner Interfacing with technology partners Building a client-facing tool as a deliverable We'll work with you through the process of scoping , product definition , specification , implementation and adaptation . What's described below is optimized for developing a medium- to large-effort tool, but will be similar, though less involved, for lesser engagements. Scoping \u00b6 Scoping is all about understanding the situation and exploring the problem. We often start by asking you to describe your vision. If its our first time working on this program, we'd like to know your vision for the program, your conservation targets, etc. If we've already engaged, then we'll focus on your vision for the world in which this solution exists. Understanding your vision helps us know what a successful solution will look like. If that's too optimistic for you, we can instead focus on the problems you face, decisions you need to make, or jobs to be done. We'll have you list the problems and needs so we can zero in on the key problems, who is involved, and the overall situation. While you're describing the vision or problem, we'll be listening for three things. First, the people involved. Who are the users and what are their roles? Who are the stakeholders (those affected by the tool)? What are the users' capabilities? The list of people will be refined to a list of users and persona descriptions that describe their motivations and constraints. Second, their problems or needs. Which can be addressed? What are people doing instead? How will our solution improve upon the alternatives? Problems will be catalogued and we'll ultimately seek to identify the problem, or set of problems, that our solution will address first. Finally, context. How will this solution integrate with existing tools or processes? What are the requirements, constraints, and design considerations we must keep in mind? If we come up with questions you don't have an answer for, its helpful to have an expert (whether a true expert or just another member of the project team) available to get clarity before we move on. While we're not committing to a particular solution yet, we will want to work with you to identify priority problems to address by asking 'How Might We?'. This question allows us to transform problems into opportunities and get creative about potential solutions. After this meeting, we'll refine our understanding of what you told us, develop testable hypotheses for people and problems, and craft a research plan. We'll do background research to fill in gaps. We may even interview potential users to confirm our persona hypotheses and problem hypotheses. Product Definition \u00b6 Utilizing our research and what you've told us, we'll develop options for solutions and develop a recommendation. We'll return with a Product Definition and, potentially, a 'Minimum Viable Product' (MVP) - some tangible way for you to understand the recommended solution. The MVP is the minimum build sufficient to test key hypotheses we've developed. It may not even function at all (e.g., a 'wizard of oz' approach), but will simulate the experience of using the tool. It might be as simple as a drawing, a wireframe, or even a small version of what we'll eventually develop. We find it's better to have something tangible for you to react to so that you are fully aware of the look/feel and limitations of any solution. Otherwise you might be imaging a cure all that is impossible! Also, we may not be able to solve all of the problems (or meet all of the needs) all at once, if not, we'll pick an 'anchor problem' and start there. We'll keep track of other problems to be solved in future features. This is your chance to give us direction on the scope and solution. Did we overestimate the users capabilities? Forget a key requirement? It's our job to make sure you fully understand the implications of the proposed solution - who will manage it, what is its expected lifetime, what will it cost, etc. We'll also review roles, timeline, and necessary resources. Specification \u00b6 We'll document the specifications for the solution in our template Specifications Document , create our project plan and roadmap, and develop the solution or support you to get the product built with a technology provider. Before working with a technology provider, we may develop a simple prototype that will allow you to test and interact with the product so you can be very clear about what you want and how you want it to work. Implementation \u00b6 Implementation - the development and deployment of the solution- is simply iteration. We strongly recommend implementing the smallest workable subset of a solution possible. Don't wait for the perfect build out before putting it in the hands of your users. Also, don't overlook the importance of outreach to and education of both users and stakeholders. We can help you navigate the cycle of building and testing that is central to solution implementation. Adaptation \u00b6 Almost every tool requires some revision in the first year. We can provide support for the first year and recommend a structure process for adapting the product as needed after a full year of use. The adaptation process should, of course, be scaled to the size of the tool. Why have we developed this process? \u00b6 A common component of the delivery of metrics services is the design of tools. We have developed over the past few years a unique perspective and approach to developing tools to support metrics projects. Our tools can include a technology component (such as a scripting language like R or Python), but many do not. The design and development of tools, whether technological or not, is a unique skillset developed within the Metrics service line over the past few years. We have built out processes and products to facilitate the efficient development of tools that can be used both internally and externally to improve EI services and internal processes. In addition, some EI staff not explicitly included in the Metrics service line have also been developing skills and experience in overlapping areas (e.g., human-centered design). Our aim is to build EI's capacity for effective tool development by promoting the processes and products we have while incorporating the learning of other EI staff in those products and processes. We are not pretending to be a tech company when we are not, nor develop a competency in the development of software or hardware. EI will continue to work with technology providers to deliver robust technology solutions. Instead, this is an effort to describe and promote, internally and externally, a subset of the Metrics service line's skillset that is unique to Metrics service line experts. Tool design is central to this, but it can also include data analytics and visualization as stand-alone services. We also explore the opportunity and requirements to deliver Metrics services outside the typical program design and implementation package to new clients/markets, to existing clients, and to internal initiatives. The Metrics service line can support an effort as small as designing a new budget template to as large as developing an entire information management system for a large agency. We leverage our skills in consulting, design, development, and deployment created through past internal and external work. We work to improve EI's internal practices, expand our staff's understanding of what is possible, and deliver robust, user-friendly tools to clients. Additionally, we seek to increase the capacity of all EI staff to develop lasting solutions.","title":"How We Work"},{"location":"metrics-services/how-we-work/#scoping","text":"Scoping is all about understanding the situation and exploring the problem. We often start by asking you to describe your vision. If its our first time working on this program, we'd like to know your vision for the program, your conservation targets, etc. If we've already engaged, then we'll focus on your vision for the world in which this solution exists. Understanding your vision helps us know what a successful solution will look like. If that's too optimistic for you, we can instead focus on the problems you face, decisions you need to make, or jobs to be done. We'll have you list the problems and needs so we can zero in on the key problems, who is involved, and the overall situation. While you're describing the vision or problem, we'll be listening for three things. First, the people involved. Who are the users and what are their roles? Who are the stakeholders (those affected by the tool)? What are the users' capabilities? The list of people will be refined to a list of users and persona descriptions that describe their motivations and constraints. Second, their problems or needs. Which can be addressed? What are people doing instead? How will our solution improve upon the alternatives? Problems will be catalogued and we'll ultimately seek to identify the problem, or set of problems, that our solution will address first. Finally, context. How will this solution integrate with existing tools or processes? What are the requirements, constraints, and design considerations we must keep in mind? If we come up with questions you don't have an answer for, its helpful to have an expert (whether a true expert or just another member of the project team) available to get clarity before we move on. While we're not committing to a particular solution yet, we will want to work with you to identify priority problems to address by asking 'How Might We?'. This question allows us to transform problems into opportunities and get creative about potential solutions. After this meeting, we'll refine our understanding of what you told us, develop testable hypotheses for people and problems, and craft a research plan. We'll do background research to fill in gaps. We may even interview potential users to confirm our persona hypotheses and problem hypotheses.","title":"Scoping"},{"location":"metrics-services/how-we-work/#product-definition","text":"Utilizing our research and what you've told us, we'll develop options for solutions and develop a recommendation. We'll return with a Product Definition and, potentially, a 'Minimum Viable Product' (MVP) - some tangible way for you to understand the recommended solution. The MVP is the minimum build sufficient to test key hypotheses we've developed. It may not even function at all (e.g., a 'wizard of oz' approach), but will simulate the experience of using the tool. It might be as simple as a drawing, a wireframe, or even a small version of what we'll eventually develop. We find it's better to have something tangible for you to react to so that you are fully aware of the look/feel and limitations of any solution. Otherwise you might be imaging a cure all that is impossible! Also, we may not be able to solve all of the problems (or meet all of the needs) all at once, if not, we'll pick an 'anchor problem' and start there. We'll keep track of other problems to be solved in future features. This is your chance to give us direction on the scope and solution. Did we overestimate the users capabilities? Forget a key requirement? It's our job to make sure you fully understand the implications of the proposed solution - who will manage it, what is its expected lifetime, what will it cost, etc. We'll also review roles, timeline, and necessary resources.","title":"Product Definition"},{"location":"metrics-services/how-we-work/#specification","text":"We'll document the specifications for the solution in our template Specifications Document , create our project plan and roadmap, and develop the solution or support you to get the product built with a technology provider. Before working with a technology provider, we may develop a simple prototype that will allow you to test and interact with the product so you can be very clear about what you want and how you want it to work.","title":"Specification"},{"location":"metrics-services/how-we-work/#implementation","text":"Implementation - the development and deployment of the solution- is simply iteration. We strongly recommend implementing the smallest workable subset of a solution possible. Don't wait for the perfect build out before putting it in the hands of your users. Also, don't overlook the importance of outreach to and education of both users and stakeholders. We can help you navigate the cycle of building and testing that is central to solution implementation.","title":"Implementation"},{"location":"metrics-services/how-we-work/#adaptation","text":"Almost every tool requires some revision in the first year. We can provide support for the first year and recommend a structure process for adapting the product as needed after a full year of use. The adaptation process should, of course, be scaled to the size of the tool.","title":"Adaptation"},{"location":"metrics-services/how-we-work/#why-have-we-developed-this-process","text":"A common component of the delivery of metrics services is the design of tools. We have developed over the past few years a unique perspective and approach to developing tools to support metrics projects. Our tools can include a technology component (such as a scripting language like R or Python), but many do not. The design and development of tools, whether technological or not, is a unique skillset developed within the Metrics service line over the past few years. We have built out processes and products to facilitate the efficient development of tools that can be used both internally and externally to improve EI services and internal processes. In addition, some EI staff not explicitly included in the Metrics service line have also been developing skills and experience in overlapping areas (e.g., human-centered design). Our aim is to build EI's capacity for effective tool development by promoting the processes and products we have while incorporating the learning of other EI staff in those products and processes. We are not pretending to be a tech company when we are not, nor develop a competency in the development of software or hardware. EI will continue to work with technology providers to deliver robust technology solutions. Instead, this is an effort to describe and promote, internally and externally, a subset of the Metrics service line's skillset that is unique to Metrics service line experts. Tool design is central to this, but it can also include data analytics and visualization as stand-alone services. We also explore the opportunity and requirements to deliver Metrics services outside the typical program design and implementation package to new clients/markets, to existing clients, and to internal initiatives. The Metrics service line can support an effort as small as designing a new budget template to as large as developing an entire information management system for a large agency. We leverage our skills in consulting, design, development, and deployment created through past internal and external work. We work to improve EI's internal practices, expand our staff's understanding of what is possible, and deliver robust, user-friendly tools to clients. Additionally, we seek to increase the capacity of all EI staff to develop lasting solutions.","title":"Why have we developed this process?"},{"location":"metrics-services/product-definition/","text":"Product Definition \u00b6 An editable version of the Product Definition is available here . Download a copy before continuing. Product definition is the process of documenting the proposed solution in broad terms. The purpose of a product definition is to align expectations, formulate broad concepts, clarify roles, and establish a development timeline. In the next step, Product Specification , you will have a chance to provide more specifics. At the product definition stage, you still have considerable flexibility in how you will deliver the final solution. Use the guidance below to complete the product definition. The product definition for this website is available here as an example. Solution Description \u00b6 Provide a 1-2 sentence description of the proposed solution. If you'd like, include a positioning statement: For [User Name], who wants/needs to [statement of need or opportunity], this [product category] will [primary value proposition]. Unlike [next best alternative], this tool [primary difference]. Goal & Objectives \u00b6 Goal \u00b6 The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully. Objectives \u00b6 Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives Users \u00b6 Primary \u00b6 Primary users are the subset of your boundary partners who will directly interact with the tool or solution. Think of these like your customers. You might want to group users with personas . For each primary user identified, complete the table below. If you've already completed the scoping exercise , you will have a starting point. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Add a row to the table for each problem scenario. Secondary \u00b6 Secondary users don't directly use the tool or solution, but are affected by it. They may provide data inputs or make decisions from the outputs. Repeat the process you went through for primary users. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Uses \u00b6 Uses describe the conditions under which the tool is used and the purpose for using the tool in that case. They should be fairly broad and together encompass the list of uses you envision for the tool or solution. If you're familiar with user stories, you can think of a use case as an 'epic' user story. During product specification you will enumerate more specific user stories to motivate development of features. However, if it's helpful, you can frame use cases like user stories as \"As user X, I want to Y so I can Z.\" This helps you think through each use case (Y) as specific to a user (X) who is motivated to achieve some outcome (Z). Product Sketch \u00b6 The six panels in this section allow you to get visual with how you expect the solution to work in practice. Visualizing can really help you avoid pitfalls and identify necessary features. Don't feel restricted to this six-panel format, feel free to get creative. Storyboards and Journey Maps are both great options for this sketch. Jot down any insights you had. You can move this section to the back if breaks up the flow of the document too much. Design Principles & Constraints \u00b6 List design principles and constraints. Consider integration, hosting, support, adaptive management and deployment but don't get into details yet. Your client may have constraints related to software licensing, budget, etc. Listing them here will make sure you don't come back with a solution that isn't feasible. Conditions of Satisfaction \u00b6 Set expectations for the tool by clarifying how you will know this tool is successful. Approach \u00b6 Provide a general description of the proposed work plan. Discuss roles, timelines, and resources required.","title":"Product Definition"},{"location":"metrics-services/product-definition/#product-definition","text":"An editable version of the Product Definition is available here . Download a copy before continuing. Product definition is the process of documenting the proposed solution in broad terms. The purpose of a product definition is to align expectations, formulate broad concepts, clarify roles, and establish a development timeline. In the next step, Product Specification , you will have a chance to provide more specifics. At the product definition stage, you still have considerable flexibility in how you will deliver the final solution. Use the guidance below to complete the product definition. The product definition for this website is available here as an example.","title":"Product Definition"},{"location":"metrics-services/product-definition/#solution-description","text":"Provide a 1-2 sentence description of the proposed solution. If you'd like, include a positioning statement: For [User Name], who wants/needs to [statement of need or opportunity], this [product category] will [primary value proposition]. Unlike [next best alternative], this tool [primary difference].","title":"Solution Description"},{"location":"metrics-services/product-definition/#goal-objectives","text":"","title":"Goal &amp; Objectives"},{"location":"metrics-services/product-definition/#goal","text":"The goal statement should describe the desired impact of the project. It should relate to your conservation targets. It should be observable. Optionally, it can be measurable and time-limited. The goal is not \"to implement the solution\", the goal is what happens if the solution is implemented successfully.","title":"Goal"},{"location":"metrics-services/product-definition/#objectives","text":"Objectives describe a desired outcome for the project Objectives are not an outline, list of features, are set of use cases Try to have at least two and no more than five objectives","title":"Objectives"},{"location":"metrics-services/product-definition/#users","text":"","title":"Users"},{"location":"metrics-services/product-definition/#primary","text":"Primary users are the subset of your boundary partners who will directly interact with the tool or solution. Think of these like your customers. You might want to group users with personas . For each primary user identified, complete the table below. If you've already completed the scoping exercise , you will have a starting point. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better? Add a row to the table for each problem scenario.","title":"Primary"},{"location":"metrics-services/product-definition/#secondary","text":"Secondary users don't directly use the tool or solution, but are affected by it. They may provide data inputs or make decisions from the outputs. Repeat the process you went through for primary users. Problem Scenarios Alternatives Value Propositions What is the problem, need, or job to be done? What are they doing now? Why is your solution better?","title":"Secondary"},{"location":"metrics-services/product-definition/#uses","text":"Uses describe the conditions under which the tool is used and the purpose for using the tool in that case. They should be fairly broad and together encompass the list of uses you envision for the tool or solution. If you're familiar with user stories, you can think of a use case as an 'epic' user story. During product specification you will enumerate more specific user stories to motivate development of features. However, if it's helpful, you can frame use cases like user stories as \"As user X, I want to Y so I can Z.\" This helps you think through each use case (Y) as specific to a user (X) who is motivated to achieve some outcome (Z).","title":"Uses"},{"location":"metrics-services/product-definition/#product-sketch","text":"The six panels in this section allow you to get visual with how you expect the solution to work in practice. Visualizing can really help you avoid pitfalls and identify necessary features. Don't feel restricted to this six-panel format, feel free to get creative. Storyboards and Journey Maps are both great options for this sketch. Jot down any insights you had. You can move this section to the back if breaks up the flow of the document too much.","title":"Product Sketch"},{"location":"metrics-services/product-definition/#design-principles-constraints","text":"List design principles and constraints. Consider integration, hosting, support, adaptive management and deployment but don't get into details yet. Your client may have constraints related to software licensing, budget, etc. Listing them here will make sure you don't come back with a solution that isn't feasible.","title":"Design Principles &amp; Constraints"},{"location":"metrics-services/product-definition/#conditions-of-satisfaction","text":"Set expectations for the tool by clarifying how you will know this tool is successful.","title":"Conditions of Satisfaction"},{"location":"metrics-services/product-definition/#approach","text":"Provide a general description of the proposed work plan. Discuss roles, timelines, and resources required.","title":"Approach"},{"location":"metrics-services/scoping/","text":"Scoping \u00b6 This scoping exercise will help you better understand the problem context and kick-start the process of identifying solutions. This is best done together in person, if possible, and typically requires at least an hour but can take half a day or more, depending on how invested you are in building out the Situation Model (feel free to sub in a systems map or journey map if preferred). If you've already developed some of these elements, great! Bring along a copy and start with a quick review. Find a large whiteboard and divide it up into sections as illustrated below (don't label each box as large as shown below, you'll need room to write). You can also find an editable version of this scoping canvas here . Instructions for filling in each section are provided below. Instructions \u00b6 Work through the sections in approximately this order, but don't hesitate to jump around if that's where the conversation flows. Scope \u00b6 A project\u2019s scope defines what the project intends to affect. \u201cPlace-based\u201d projects have a geographic scope and include efforts to conserve or effectively manage ecoregions, priority areas, or protected areas. \u201cThematic-based\u201d projects include efforts to address specific conservation targets, threats, opportunities, or enabling conditions and generally have a corresponding thematic scope. Thematic-based projects may also define a geographic scope that spatially describes a project area and might reference specific elements of biodiversity or a specific threat. (CMP 2013). I recommend erring on the inclusive side here. You will continue to get more specific throughout this process, for now try to represent the entire problem space. The scope should include the scope for the program, not just the specific tactic, tool, or feature you're considering. Vision \u00b6 In addition to defining the scope, it is also necessary to decide on a clear and common vision \u2013 a description of the desired state or ultimate condition that you are working to achieve. Your vision can be summarized in a vision statement, which meets the criteria of being relatively general, visionary, and brief. (CMP 2013) It's important to be agnostic about the solution within the vision statement. The vision is not for everyone to be using the tool your thinking about developing. Just describe the long-term outcomes that you're trying to achieve in sufficient detail that if you time-traveled to a time in which your strategy was successful, you'd know you were there. See Step 1: Start at the End of our metrics design philosophy for help if you get stuck. Situation Model & Interventions \u00b6 The Situation Model consists of one or more conservation targets, threats, and drivers. Interventions should be mapped where appropriate. See Step 2: Map It of our metrics design philosophy for inspiration. Conservation Targets \u00b6 Conservation targets are specific species or ecological systems/habitats that are chosen to represent and encompass the full suite of biodiversity in the project area for place-based conservation or the focus of a thematic program (CMP 2013). If you've already been through the process of developing a situation model, you should have a list of conservation targets to draw from. You may also already know which target is the focus of today's effort. For expediency, you can focus on the conservation target you are, well, targeting. If you're not sure, include them all. Place the conservation targets on the far right side of the Situation Model diagram. Threats \u00b6 Direct threats are primarily human activities that immediately degrade a conservation target (e.g., unsustainable fishing, unsustainable hunting, oil drilling, construction of roads, industrial wastewater, or introduction of exotic invasive species), but they can be natural phenomena altered by human activities (e.g., increase in extreme storm events or increased evaporation due to global climate change) or in rare cases, natural phenomena whose impact is increased by other human activities (e.g., a potential tsunami that threatens the last remaining population of an Asian rhino). (CMP 2013) Feel free to list all threats, but what we really need is the threat that is mapped to the key intervention point that strategy will attack. If you haven't chosen a key intervention point yet, go ahead and build out the entire thing. If you have, just build out the relevant parts. List threats vertically to the left of the conservation target . Drivers \u00b6 From the Conservation Measures Partnership, drivers are: key factors that drive the direct threats and ultimately influence your conservation targets. These include indirect threats (also known as root causes and drivers), opportunities, and enabling conditions (CMP 2013). If you already have a situation model, just copy over the relevant bits. Otherwise, list the key drivers and make connections where there are relationships. Place drivers in boxes, mapped to their relevant threats, and group like drivers. Draw lines to connect related drivers. Leave a bit of room for interventions. Interventions \u00b6 Finally, map the primary existing interventions (e.g., strategies or tactics employed by boundary partners or strategic partners in your problem space). These are typically captured in hexagonal polygons. Connect them to the relevant threats or drivers. Take a step back and determine which driver you would like to intervene on. This is your key intervention point. Circle the key intervention point(s) (or leverage point or pain point, etc.). This is where your group has chosen to focus, either with the entire program or with this specific tactic, tool, or feature. Stakeholders \u00b6 Stakeholders include everyone that is involved in the problem space or might be affected by your selected intervention. That may be a lot of people, which is why we'll focus in on Boundary Partners here. List stakeholders and underline boundary partners. Boundary partners are people or organizations you can directly influence. Stakeholders you can't directly influence, but can work with directly, are called Strategic Partners . You might put a star next to their name to distinguish them. Feel free to group stakeholders to maintain a manageable list, but at the same time identify an actual human being who would fit in that group. If a boundary partner is described as 'permitting staff at state agencies', find a real human being in a permitting department at a state in your geographic scope as an example. If you can't do that, your boundary partner probably doesn't actually exist. See Step 3: Develop Empathy of our metrics design philosophy for a lot more detail on this process. Problems \u00b6 If you're coming from the Open Standards for the Practice of Conservation, everything so far has been very familiar. This is where we take a bit of a departure. Instead of jumping straight to listing strategies, we're going to think about how we can solve problems or create opportunities for the people whose behavior we need to change to achieve our conservation goal. Our strategy will be shaped by our understanding of these problems (but we actually won't define a specific strategy until the Product Definition step). Identify the most important boundary partner in your list. Ask yourself, what do we need them to do (or to stop doing) to achieve our conservation goal? In other words, what behaviors do we need them to exhibit in order that our vision is achieved? You might frame these as behaviors you'd 'expect to see', 'like to see', and 'love to see'. Now, write down the problems (or jobs to be done) that your boundary partners face in doing what you need them to do . Maybe they don't know where to get technical information, they don't have time to do it, they can't identify the priority areas to work, etc. If you - when you - talk to them about your proposed solution, they should relate to these problem statements - if not, these are not real problems (or jobs to be done) and solving them won't change their behavior. See Step 4: Define the Problem for helpful guidance on this prompt. Alternatives \u00b6 From the list of problems you've identified, have any of these problems been solved already? You may seek to solve them more expediently, which is great, but knowing what the alternatives are will allow you to evaluate your proposed solution against a real baseline. List the currently existing alternative solutions. How Might We? \u00b6 Now you can start to brainstorm solutions. \"How Might We?\" is a way of framing design questions that help you convert your problem statements into opportunities. If, for example, your boundary partners don't know where to find key information, how might we get that information to them? List the 'How Might We?' questions that address the problems you think will be most impactful for your boundary partners. Again, you'll take the time to craft your solutions during the Product Definition step. This will just give you a head start. Considerations/Constraints \u00b6 As your thinking about problems and potential solutions, you'll probably identify a few constraints on what you're able to do to solve the problem, or just some things to keep in mind going forward. Jot down any considerations and/or constraints so you don't forget. Research Questions \u00b6 This space is for recording questions that you don't have the answer to...yet. It might help to have an expert available during the scoping meeting so you don't get hung up on a question, but for those you can't address during the scoping meeting, write down the research questions and plan to get it answered before developing the Product Definition. References \u00b6 CMP 2013","title":"Scoping"},{"location":"metrics-services/scoping/#scoping","text":"This scoping exercise will help you better understand the problem context and kick-start the process of identifying solutions. This is best done together in person, if possible, and typically requires at least an hour but can take half a day or more, depending on how invested you are in building out the Situation Model (feel free to sub in a systems map or journey map if preferred). If you've already developed some of these elements, great! Bring along a copy and start with a quick review. Find a large whiteboard and divide it up into sections as illustrated below (don't label each box as large as shown below, you'll need room to write). You can also find an editable version of this scoping canvas here . Instructions for filling in each section are provided below.","title":"Scoping"},{"location":"metrics-services/scoping/#instructions","text":"Work through the sections in approximately this order, but don't hesitate to jump around if that's where the conversation flows.","title":"Instructions"},{"location":"metrics-services/scoping/#scope","text":"A project\u2019s scope defines what the project intends to affect. \u201cPlace-based\u201d projects have a geographic scope and include efforts to conserve or effectively manage ecoregions, priority areas, or protected areas. \u201cThematic-based\u201d projects include efforts to address specific conservation targets, threats, opportunities, or enabling conditions and generally have a corresponding thematic scope. Thematic-based projects may also define a geographic scope that spatially describes a project area and might reference specific elements of biodiversity or a specific threat. (CMP 2013). I recommend erring on the inclusive side here. You will continue to get more specific throughout this process, for now try to represent the entire problem space. The scope should include the scope for the program, not just the specific tactic, tool, or feature you're considering.","title":"Scope"},{"location":"metrics-services/scoping/#vision","text":"In addition to defining the scope, it is also necessary to decide on a clear and common vision \u2013 a description of the desired state or ultimate condition that you are working to achieve. Your vision can be summarized in a vision statement, which meets the criteria of being relatively general, visionary, and brief. (CMP 2013) It's important to be agnostic about the solution within the vision statement. The vision is not for everyone to be using the tool your thinking about developing. Just describe the long-term outcomes that you're trying to achieve in sufficient detail that if you time-traveled to a time in which your strategy was successful, you'd know you were there. See Step 1: Start at the End of our metrics design philosophy for help if you get stuck.","title":"Vision"},{"location":"metrics-services/scoping/#situation-model-interventions","text":"The Situation Model consists of one or more conservation targets, threats, and drivers. Interventions should be mapped where appropriate. See Step 2: Map It of our metrics design philosophy for inspiration.","title":"Situation Model &amp; Interventions"},{"location":"metrics-services/scoping/#conservation-targets","text":"Conservation targets are specific species or ecological systems/habitats that are chosen to represent and encompass the full suite of biodiversity in the project area for place-based conservation or the focus of a thematic program (CMP 2013). If you've already been through the process of developing a situation model, you should have a list of conservation targets to draw from. You may also already know which target is the focus of today's effort. For expediency, you can focus on the conservation target you are, well, targeting. If you're not sure, include them all. Place the conservation targets on the far right side of the Situation Model diagram.","title":"Conservation Targets"},{"location":"metrics-services/scoping/#threats","text":"Direct threats are primarily human activities that immediately degrade a conservation target (e.g., unsustainable fishing, unsustainable hunting, oil drilling, construction of roads, industrial wastewater, or introduction of exotic invasive species), but they can be natural phenomena altered by human activities (e.g., increase in extreme storm events or increased evaporation due to global climate change) or in rare cases, natural phenomena whose impact is increased by other human activities (e.g., a potential tsunami that threatens the last remaining population of an Asian rhino). (CMP 2013) Feel free to list all threats, but what we really need is the threat that is mapped to the key intervention point that strategy will attack. If you haven't chosen a key intervention point yet, go ahead and build out the entire thing. If you have, just build out the relevant parts. List threats vertically to the left of the conservation target .","title":"Threats"},{"location":"metrics-services/scoping/#drivers","text":"From the Conservation Measures Partnership, drivers are: key factors that drive the direct threats and ultimately influence your conservation targets. These include indirect threats (also known as root causes and drivers), opportunities, and enabling conditions (CMP 2013). If you already have a situation model, just copy over the relevant bits. Otherwise, list the key drivers and make connections where there are relationships. Place drivers in boxes, mapped to their relevant threats, and group like drivers. Draw lines to connect related drivers. Leave a bit of room for interventions.","title":"Drivers"},{"location":"metrics-services/scoping/#interventions","text":"Finally, map the primary existing interventions (e.g., strategies or tactics employed by boundary partners or strategic partners in your problem space). These are typically captured in hexagonal polygons. Connect them to the relevant threats or drivers. Take a step back and determine which driver you would like to intervene on. This is your key intervention point. Circle the key intervention point(s) (or leverage point or pain point, etc.). This is where your group has chosen to focus, either with the entire program or with this specific tactic, tool, or feature.","title":"Interventions"},{"location":"metrics-services/scoping/#stakeholders","text":"Stakeholders include everyone that is involved in the problem space or might be affected by your selected intervention. That may be a lot of people, which is why we'll focus in on Boundary Partners here. List stakeholders and underline boundary partners. Boundary partners are people or organizations you can directly influence. Stakeholders you can't directly influence, but can work with directly, are called Strategic Partners . You might put a star next to their name to distinguish them. Feel free to group stakeholders to maintain a manageable list, but at the same time identify an actual human being who would fit in that group. If a boundary partner is described as 'permitting staff at state agencies', find a real human being in a permitting department at a state in your geographic scope as an example. If you can't do that, your boundary partner probably doesn't actually exist. See Step 3: Develop Empathy of our metrics design philosophy for a lot more detail on this process.","title":"Stakeholders"},{"location":"metrics-services/scoping/#problems","text":"If you're coming from the Open Standards for the Practice of Conservation, everything so far has been very familiar. This is where we take a bit of a departure. Instead of jumping straight to listing strategies, we're going to think about how we can solve problems or create opportunities for the people whose behavior we need to change to achieve our conservation goal. Our strategy will be shaped by our understanding of these problems (but we actually won't define a specific strategy until the Product Definition step). Identify the most important boundary partner in your list. Ask yourself, what do we need them to do (or to stop doing) to achieve our conservation goal? In other words, what behaviors do we need them to exhibit in order that our vision is achieved? You might frame these as behaviors you'd 'expect to see', 'like to see', and 'love to see'. Now, write down the problems (or jobs to be done) that your boundary partners face in doing what you need them to do . Maybe they don't know where to get technical information, they don't have time to do it, they can't identify the priority areas to work, etc. If you - when you - talk to them about your proposed solution, they should relate to these problem statements - if not, these are not real problems (or jobs to be done) and solving them won't change their behavior. See Step 4: Define the Problem for helpful guidance on this prompt.","title":"Problems"},{"location":"metrics-services/scoping/#alternatives","text":"From the list of problems you've identified, have any of these problems been solved already? You may seek to solve them more expediently, which is great, but knowing what the alternatives are will allow you to evaluate your proposed solution against a real baseline. List the currently existing alternative solutions.","title":"Alternatives"},{"location":"metrics-services/scoping/#how-might-we","text":"Now you can start to brainstorm solutions. \"How Might We?\" is a way of framing design questions that help you convert your problem statements into opportunities. If, for example, your boundary partners don't know where to find key information, how might we get that information to them? List the 'How Might We?' questions that address the problems you think will be most impactful for your boundary partners. Again, you'll take the time to craft your solutions during the Product Definition step. This will just give you a head start.","title":"How Might We?"},{"location":"metrics-services/scoping/#considerationsconstraints","text":"As your thinking about problems and potential solutions, you'll probably identify a few constraints on what you're able to do to solve the problem, or just some things to keep in mind going forward. Jot down any considerations and/or constraints so you don't forget.","title":"Considerations/Constraints"},{"location":"metrics-services/scoping/#research-questions","text":"This space is for recording questions that you don't have the answer to...yet. It might help to have an expert available during the scoping meeting so you don't get hung up on a question, but for those you can't address during the scoping meeting, write down the research questions and plan to get it answered before developing the Product Definition.","title":"Research Questions"},{"location":"metrics-services/scoping/#references","text":"CMP 2013","title":"References"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/","text":"Machine Learning Tutorial \u00b6 This tutorial is intended to illustrate a typical workflow for machine learning to solve a land use and land cover classification problem. Land Use and Land Cover classifications are used to identify the dominant land cover or land use type in an area. We use the Naive Bayes and Random Forest classifiers, as implemented within scikit-learn library. This tutorial borrows heavily from the very helpful tutorial developed by Chris Holden and updated by Patrick Gray. Also used: rasterio , geopandas , numpy , pandas , shapely , and matplotlib . The Challenge \u00b6 Our client required a rapid approach for evaluating the benefits of conservation projects to Mule Deer. We proposed using Ecological State and Transition Models (STMs) as the basis for the evaluation. The NRCS has developed STMs for the dominant ecological sites within the region, however only a subset of the region was mapped. We use vegetation data (provided by the Rangelands App ) and other environmental variables to predict STM for the unmapped areas of the range. Import Statements \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import rasterio from rasterio.mask import mask from rasterio.plot import show from rasterio.plot import show_hist from rasterio.windows import Window from rasterio.plot import reshape_as_raster , reshape_as_image import geopandas as gpd import numpy as np from shapely.geometry import mapping from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn import preprocessing from sklearn.pipeline import make_pipeline from sklearn.utils import resample import matplotlib.pyplot as plt # ipywidgets is used to create a progress bar from ipywidgets import IntProgress from IPython.display import display 1 % matplotlib inline Import Data \u00b6 We'll be using supervised classification techniques. We'll need both labels and predictor features to train the models. Our labels will come from the mapped STM derived from NRCS data. To begin, we'll use vegetation data as predictors, including cover estimates for trees, shrubs, perennial grasses and forbs, and bare ground. 1 2 3 4 5 # CHANGE TO VEG_COVER_PATH # Read in features and training data train_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\train_clip_utm.tif' data = rasterio . open ( train_path ) data . crs # Check the projection, all features must share a projection 1 CRS . from_dict ( init = 'epsg:26913' ) 1 2 3 labels_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\labels.shp' labels = gpd . read_file ( labels_path ) labels . crs 1 { 'init' : 'epsg:26913' } 1 len ( labels ) 1 6136 We have 6,136 labeled polygons, represented within a geopandas dataframe, while our predictor features are represented in a rasterio raster. Both share the same projection. Explore Data and Preparation Steps \u00b6 To train the classifiers, we'll need to associate our vector data (labels as polygons) with our raster pixels (predictor features). We'll accomplish this with the rasterio mask function. The mask function will essentially clip (or mask) our raster with each polygon. First, we'll want to extract the geometry of each feature in the labels shapefile to GeoJSON format. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # this generates a list of shapely geometries geoms = labels . geometry . values # let's grab a single shapely geometry to check geometry = geoms [ 0 ] print ( \"This is a shapely polygon\" ) print ( type ( geometry )) print ( geometry ) # transform to GeoJSON format (note 'mapping' is in the shapely namespace) feature = [ mapping ( geometry )] # can also do this using polygon.__geo_interface__ print ( \"This is the same polygon in GeoJSON format\" ) print ( type ( feature )) print ( feature ) 1 2 3 4 5 6 This is a shapely polygon < class 'shapely.geometry.polygon.Polygon' > POLYGON (( 155090 . 8039999995 4317864 . 1029 , 155095 . 9642000003 4317847 . 2246 , 155096 . 9452999998 4317767 . 406400001 , 155079 . 9972999999 4317764 . 4924 , 155013 . 6201999998 4317772 . 8774 , 154946 . 0262000002 4317762 . 761399999 , 154906 . 4253000002 4317765 . 351500001 , 154878 . 2922999999 4317781 . 742900001 , 154874 . 5252 4317788 . 509099999 , 154877 . 2857999997 4317829 . 973099999 , 154890 . 6249000002 4317873 . 230799999 , 154892 . 2852999996 4317898 . 202400001 , 154873 . 1852000002 4318020 . 8462 , 154866 . 6454999996 4318032 . 8059 , 154861 . 3509 4318094 . 8554 , 154880 . 1355999997 4318314 . 824999999 , 154876 . 2439000001 4318350 . 6874 , 154879 . 5504999999 4318400 . 631100001 , 154884 . 4243000001 4318410 . 842 , 154890 . 0175999999 4318527 . 3519 , 154897 . 2582 4318573 . 0209 , 154890 . 3213 4318594 . 527100001 , 154894 . 7226999998 4318598 . 269300001 , 154913 . 1912000002 4318593 . 0416 , 154946 . 1224999996 4318553 . 260600001 , 154952 . 6897 4318463 . 052999999 , 154967 . 4232000001 4318401 . 3926 , 154951 . 2766000004 4318220 . 3751 , 154975 . 3370000003 4318141 . 542099999 , 155019 . 5067999996 4318081 . 9695 , 155041 . 7379000001 4318038 . 8718 , 155041 . 0279000001 4317965 . 691199999 , 155054 . 3926999997 4317914 . 6457 , 155080 . 8075000001 4317871 . 2906 , 155090 . 8039999995 4317864 . 1029 )) This is the same polygon in GeoJSON format < class 'list' > [ { 'type' : 'Polygon' , 'coordinates' : ((( 155090 . 80399999954 , 4317864 . 1029 ), ( 155095 . 96420000028 , 4317847 . 2246 ), ( 155096 . 9452999998 , 4317767 . 406400001 ), ( 155079 . 99729999993 , 4317764 . 4924 ), ( 155013 . 62019999977 , 4317772 . 8774 ), ( 154946 . 0262000002 , 4317762 . 761399999 ), ( 154906 . 42530000024 , 4317765 . 351500001 ), ( 154878 . 29229999986 , 4317781 . 742900001 ), ( 154874 . 52520000003 , 4317788 . 509099999 ), ( 154877 . 28579999972 , 4317829 . 973099999 ), ( 154890 . 62490000017 , 4317873 . 230799999 ), ( 154892 . 28529999964 , 4317898 . 202400001 ), ( 154873 . 18520000018 , 4318020 . 8462000005 ), ( 154866 . 64549999963 , 4318032 . 8059 ), ( 154861 . 35089999996 , 4318094 . 8554 ), ( 154880 . 1355999997 , 4318314 . 824999999 ), ( 154876 . 24390000012 , 4318350 . 6874 ), ( 154879 . 5504999999 , 4318400 . 631100001 ), ( 154884 . 42430000007 , 4318410 . 842 ), ( 154890 . 0175999999 , 4318527 . 3519 ), ( 154897 . 25820000004 , 4318573 . 0209 ), ( 154890 . 32129999995 , 4318594 . 5271000005 ), ( 154894 . 7226999998 , 4318598 . 269300001 ), ( 154913 . 19120000023 , 4318593 . 0416 ), ( 154946 . 1224999996 , 4318553 . 260600001 ), ( 154952 . 6897 , 4318463 . 052999999 ), ( 154967 . 42320000008 , 4318401 . 3926 ), ( 154951 . 27660000045 , 4318220 . 3751 ), ( 154975 . 3370000003 , 4318141 . 542099999 ), ( 155019 . 50679999962 , 4318081 . 9695 ), ( 155041 . 73790000007 , 4318038 . 8718 ), ( 155041 . 0279000001 , 4317965 . 691199999 ), ( 155054 . 39269999973 , 4317914 . 6457 ), ( 155080 . 8075000001 , 4317871 . 2906 ), ( 155090 . 80399999954 , 4317864 . 1029 )),) } ] Now let's extract the raster values within each polygon using the rasterio mask() function . 1 out_image , out_transform = mask ( data , feature , crop = True ) 1 out_image . shape 1 ( 6 , 32 , 10 ) 1 show ( out_image [ 0 ]) 1 < matplotlib . axes . _subplots . AxesSubplot at 0 xbea5c5b70 > As you can see, the features raster was clipped to a single polygon. There are 6 bands and 32x10 pixels. We'll repeat this process for all 6,136 polygons to build our dataset. We'll also need to clean this raster up a bit before we use it in training. We'll explore all of this next. But first, we'll be doing a lot of memory intensive work so we'll close the dataset for now. 1 data . close () Build the Training Data for sckit-learn \u00b6 We'll repeat the above process for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels. Note that the column 'MuleDeer_1' in the labels geodataframe has the label we're after. TODO: change to column with label instead of numeric. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 %% time # Lets create a progress bar as this step can take some time p_bar = IntProgress ( min = 0 , max = len ( geoms ), description = 'Processing' ) display ( p_bar ) # Set up arrays X = np . array ([], dtype = np . int8 ) . reshape ( 0 , 6 ) # pixels for training y = np . array ([], dtype = np . string_ ) # labels for training # extract the raster values within the polygon with rasterio . open ( train_path ) as src : band_count = src . count for index , geom in enumerate ( geoms ): # Note geoms was created above feature = [ mapping ( geom )] # the mask function returns an array of the raster pixels within this feature out_image , out_transform = mask ( src , feature , crop = True ) # eliminate all the pixels with 0 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image [:, ~ np . all ( out_image == 0 , axis = 0 )] # eliminate all the pixels with 255 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image_trimmed [:, ~ np . all ( out_image_trimmed == 255 , axis = 0 )] # reshape the array to [pixel count, bands] out_image_reshaped = out_image_trimmed . reshape ( - 1 , band_count ) # append the labels to the y array y = np . append ( y , [ labels [ \"MuleDeer_1\" ][ index ]] * out_image_reshaped . shape [ 0 ]) # ??? # stack the pixels onto the pixel array X = np . vstack (( X , out_image_reshaped )) # increment the progress bar p_bar . value += 1 1 2 3 4 IntProgress ( value = 0 , description = 'Processing' , max = 6136 ) Wall time : 25 min 33 s Save the output \u00b6 We'll save the output as a numpy array to avoid the long process of rebuilding the features in the future. This will also allow us to share this analysis with others without them needing access to the input data. If this were not a tutorial, I might have ended the notebook here and started a new one for the remainder of the analysis. I've commented out the load statements below, uncomment to load in the saved features and labels. 1 2 3 # Save features and labels np . save ( 'E:/lulc-features.npy' , X ) np . save ( 'E:/lulc-labels.npy' , y ) 1 2 3 # Load in data X = np . load ( 'E:/lulc-features.npy' ) # fill in path y = np . load ( 'E:/lulc-labels.npy' ) Splitting the data for testing \u00b6 In order to evaluate the accuracy of our model, we'll reserve a subset of the data for testing. The train_test_split function allows us to quickly and randomly subset our data for this purpose. 1 2 # split out 30% of data for testing. Random state set for reproducibility. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) Pairing y with X \u00b6 Now that we have the image we want to classify (X_train) and the land cover labels (y_train), let's check to make sure they match in size so we can feed them to our models. 1 2 3 4 5 6 7 st_names = np . unique ( y_train ) print ( 'The training data include {n} classes: {classes} \\n ' . format ( n = st_names . size , classes = st_names )) # We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels print ( 'Our X matrix is sized: {sz}' . format ( sz = X_train . shape )) print ( 'Our y array is sized: {sz}' . format ( sz = y_train . shape )) 1 2 3 4 5 The training data include 5 classes : [ ' Encroached Shrub ' ' Loamy Bottom ' ' P-J ' ' Perennial Shrub ' ' Wet/Salt Meadow ' ] Our X matrix is sized : ( 4755679 , 6 ) Our y array is sized : ( 4755679 , ) That looks good. We have 5 classes (i.e., our STM names); 6 predictor features (i.e., the 6 bands in our X matrix, now flattened; and both the X and y array are the same length. We'll treat these vegetation cover values as spectral signatures, and plot each to make sure they're actually separable since all we're going by in this classification is pixel values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 fig , ax = plt . subplots ( 1 , 3 , figsize = [ 20 , 8 ]) # bands are numbered 1 through 6 following GDAL convention band_count = np . arange ( 1 , 7 ) classes = np . unique ( y_train ) for class_type in classes : band_intensity = np . mean ( X_train [ y_train == class_type , :], axis = 0 ) ax [ 0 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 1 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 2 ] . plot ( band_count , band_intensity , label = class_type ) #plot them as lines # Add some axis labels # Add some axis labels ax [ 0 ] . set_xlabel ( 'Band #' ) ax [ 0 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_xlabel ( 'Band #' ) ax [ 2 ] . set_ylabel ( 'Reflectance Value' ) ax [ 2 ] . set_xlabel ( 'Band #' ) ax [ 1 ] . legend ( loc = \"upper right\" ) # Add a title ax [ 0 ] . set_title ( 'Band Intensities Full Overview' ) ax [ 1 ] . set_title ( 'Band Intensities Lower Ref Subset' ) ax [ 2 ] . set_title ( 'Band Intensities Higher Ref Subset' ) plt . show () Looks like each class will be easily separable. This will be a helper function to convert class labels into indices so we're predicting to integers instead of strings. TODO: switch labels and numbers 1 2 3 4 5 6 7 def str_class_to_int ( class_array ): class_array [ class_array == 'P-J' ] = 1 class_array [ class_array == 'Perennial Shrub' ] = 2 class_array [ class_array == 'Encroached Shrub' ] = 3 class_array [ class_array == 'Loamy Bottom' ] = 4 class_array [ class_array == 'Wet/Salt Meadow' ] = 5 return ( class_array . astype ( int )) Training the Classifier \u00b6 Now that we have our X matrix of feature inputs (the vegetation cover bands) and our y array (the labels), we can train our model. Visit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn . 1 2 gnb = GaussianNB () gnb . fit ( X_train , y_train ) 1 GaussianNB ( priors = None , var_smoothing = 1 e - 09 ) It's that simple to train a classifier in sckit-learn . The hard part is often validation and interpretation. Validation \u00b6 To see how well our classifier worked, we could use the test data we partioned earlier. However, we may want to adjust the model if our results are not as accurate as we'd like. This could lead to overfitting by 'leaking' information from the test set into our training of the model. Overfitting will hurt the performance of our model on predicting novel data, and will lead to inflated accuracy metrics. So how do we evaluate our model at this stage? Cross-validation . There are a few options for cross-validation, but for our purposes k-fold cross validation will work. 1 2 # 5-fold cross validation scores = cross_val_score ( gnb , X_train , y_train , cv = 5 ) scores stores the results of computing the score 5 consecutive times (with different splits each time) 1 scores 1 array ([ 0 . 59125447 , 0 . 59230899 , 0 . 59061017 , 0 . 59067851 , 0 . 59257414 ]) The mean score and the 95% confidence interval of the score estimate are hence given by: 1 print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.59 (+/- 0.002 ) Improving Model Accuracy \u00b6 Standardizing Values \u00b6 The accuracy is not as good as we'd like. How can we improve the accuracy of the model? One option is to standardize the data so that each of the features are similar in magnitude. This avoids some higher values from overwhelming lower values. It is often necessary to complete this step, depending on the model used. sklearn provides a preprocessing module that facilitate this scaling. 1 2 3 # Hide warnings for converting ints to floats (or save X, y as float64 type) import warnings warnings . filterwarnings ( \"ignore\" ) 1 2 gnb = make_pipeline ( preprocessing . StandardScaler (), GaussianNB ()) cross_val_score ( gnb , X_train , y_train , cv = 5 ) 1 array ([ 0 . 59125447 , 0 . 59230899 , 0 . 59061017 , 0 . 59067851 , 0 . 59257414 ]) The accuracy is exactly the same! This is because the Gaussian Naive Bayes is robust to scaling. In essence Gaussian Naive Bayes performs standardization internally . Balancing Classes \u00b6 Unbalanced classes can also impact the accuracy of a model. Imagine you have data on a rare disease that only 0.01% of people have. A simple model that predicts everyone does not have the disease would be right 99.99% of the time! To get a model that can actually predict when people do have the disease, you'll need to address the imbalanced classes. Again, this is an issue depending on the model used. We'll try a different type of model later in the tutorial that is more robust to imbalanced data (Random Forests). To balance the data, you can either down-sample the over-represented classes or up-sample the under-represented classes. How unbalanced are the classes now? \u00b6 Let's quickly plot the number of each label so we know how unbalanced the data are. 1 2 3 import pandas as pd df_plot = pd . DataFrame ( y_train ) df_plot [ 0 ] . value_counts () . plot ( kind = 'bar' ) 1 < matplotlib . axes . _subplots . AxesSubplot at 0 x22899b1eda0 > We'll try up-sampling first so we don't reduce the number of data points too much. How much to upsample? We'll resample each of the less represented features with replacement to get the number of features contained in the most represented class. 1 df_plot [ 0 ] . value_counts () 1 2 3 4 5 6 P - J 2877442 Perennial Shrub 1388305 Encroached Shrub 375820 Loamy Bottom 85357 Wet / Salt Meadow 28755 Name : 0 , dtype : int64 This will be easier with a pandas DataFrame, so let's convert our data to a DataFrame. 1 2 3 4 5 6 df = pd . concat ( [ pd . DataFrame ( y_train , columns = [ 'label' ]), pd . DataFrame ( X_train )], axis = 1 ) . set_index ( 'label' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 label P-J 18 19 12 12 11 13 P-J 9 9 7 8 7 13 P-J 18 24 21 22 20 20 Perennial Shrub 14 18 16 12 14 13 P-J 4 5 9 9 7 7 1 2 3 4 df_shrub = resample ( df . loc [ 'Perennial Shrub' ], replace = True , n_samples = df . index . value_counts () . max ()) df_shrub . shape [ 0 ] 1 2877442 Now we have the same number of Perennial Shrub classes as P-J classes, our dominant class type. Let's repeat for the remaining features. We'll loop through our labels and concatenate the results to the most represented class so that this step is robust to changes in which features we explore. 1 df . index . value_counts () . max () 1 2877442 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Start by subsetting the most represented class max_class = df . index . value_counts () . idxmax () df_upsampled = df . loc [ max_class ] labels = list ( df . index . unique ()) labels . remove ( max_class ) # For each label, resample the features to balance classes and append # to the empty dataframe for label in labels : df_temp = resample ( df . loc [ label ], replace = True , n_samples = df . index . value_counts () . max () ) df_upsampled = pd . concat ([ df_upsampled , df_temp ], axis = 0 ) df_upsampled . index . value_counts () 1 2 3 4 5 6 P - J 2877442 Encroached Shrub 2877442 Perennial Shrub 2877442 Wet / Salt Meadow 2877442 Loamy Bottom 2877442 Name : label , dtype : int64 Now, we can split out our X and y data again and re-train the model on the more balanced classes. 1 2 X_train_upsampled = df_upsampled . reset_index () . drop ( 'label' , axis = 1 ) y_train_upsampled = df_upsampled . index . values 1 2 3 4 # 5-fold cross validation scores = cross_val_score ( gnb , X_train_upsampled , y_train_upsampled , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.24 (+/- 0.001 ) Our accuracy took a bit of a nosedive, as now we're probably over-representing those less common classes on the landscape. For the purposes of our model, it may actually be better to simply ignore those more rare classes instead of over-representing them. We could try downsampling instead, but it will likely not help our accuracy very much. Instead, let's try a different model, Random Forests. Alternative Model: Random Forests \u00b6 Random Forests is robust to unscaled and unbalanced data, making it a good option for this classification problem out-of-the-box. It's not a bad idea to scale the data as we did earlier, but since our data is well scaled (percent cover data from 0 - 100%), we'll skip this step. Acknowledgements to this tutorial used in developing this section. 1 2 3 4 5 6 7 8 %% time from sklearn.ensemble import RandomForestClassifier # Initialize our model with 10 estimators to limit processing time rfc = RandomForestClassifier ( n_estimators = 10 , random_state = 0 ) # 5-fold cross validation scores = cross_val_score ( rfc , X_train , y_train , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 2 Accuracy : 0.58 (+/- 0.001 ) Wall time : 3 h 13 min 13 s Our accuracy (57%) is slightly less than our accuracy with the unbalanced classes using Naive Bayes (59%) but comparable. Confusion Matrix \u00b6 We can visualize how well we're classifying each class (and where the model is getting confused) using a confusion matrix . The code for the confusion matrix is copied from the linked documentation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels def plot_confusion_matrix ( y_true , y_pred , classes , normalize = False , title = None , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if not title : if normalize : title = 'Normalized confusion matrix' else : title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix ( y_true , y_pred ) # Only use the labels that appear in the data classes = unique_labels ( y_true , y_pred ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) fig , ax = plt . subplots ( figsize = ( 15 , 10 )) im = ax . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) ax . figure . colorbar ( im , ax = ax ) # We want to show all ticks... ax . set ( xticks = np . arange ( cm . shape [ 1 ]), yticks = np . arange ( cm . shape [ 0 ]), # ... and label them with the respective list entries xticklabels = classes , yticklabels = classes , title = title , ylabel = 'True label' , xlabel = 'Predicted label' ) # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 45 , ha = \"right\" , rotation_mode = \"anchor\" ) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i in range ( cm . shape [ 0 ]): for j in range ( cm . shape [ 1 ]): ax . text ( j , i , format ( cm [ i , j ], fmt ), ha = \"center\" , va = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) fig . tight_layout () return ax 1 2 # Use the model to predict on the X_train dataset y_pred = gnb . fit ( X_train , y_train ) . predict ( X_train ) 1 2 class_names = unique_labels ( y_train , y_pred ) np . set_printoptions ( precision = 2 ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[ 0 . 00 e + 00 0 . 00 e + 00 8 . 77 e - 01 1 . 22 e - 01 2 . 90 e - 04 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 13 e - 01 1 . 84 e - 01 2 . 91 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 86 e - 01 1 . 14 e - 01 8 . 17 e - 05 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 01 e - 01 1 . 89 e - 01 9 . 71 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 7 . 46 e - 01 2 . 42 e - 01 1 . 17 e - 02 ]] The confusion matrix indicates that the model is catogorizing most pixels as P-J and some as Perennial Shrub, with just a few Wet Meadow predictions. It ignores Encroached Shrub and Loamy Bottom. Focusing on the final row of the matrix, you can see that the Wet/Salt Meadow is being categorized as P-J 75% of the time, Perennial Shrub 24% of the time, and its correct label only 1% of the time. 1 y_pred_rfc = rfc . fit ( X_train , y_pred ) . predict ( X_train ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred_rfc , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[ 0 . 00 e + 00 0 . 00 e + 00 8 . 77 e - 01 1 . 22 e - 01 2 . 87 e - 04 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 13 e - 01 1 . 84 e - 01 2 . 85 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 86 e - 01 1 . 14 e - 01 8 . 06 e - 05 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 01 e - 01 1 . 89 e - 01 9 . 69 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 7 . 46 e - 01 2 . 42 e - 01 1 . 17 e - 02 ]] Interestingly, the confusion matrix for both the Naive Bayes and Random Forests model is the same. This may signal that we've done about as well as we could with these features. We could incorporate additional features through the same process as we went through before, considering topography, soils, precipitation, and spatial measures using distances or moving windows. We should also carefully consider feature selection to optimize the bias-variance tradeoffs. Feature engingeering and feature selection is beyond the scope of this tutorial. A classification report will provide reportable metrics of model accuracy to allow us document the performance of the model. It reports precision, recall and the F1 score. Precision is the ability of the model to avoid false positives. Recall is the ability to identify the feature correctly (notice it is equivalent to where the diagonal axis of the confusion matrix). The F1 score is a harmonic mean of precision and recall. 1 2 from sklearn.metrics import classification_report print ( classification_report ( y_train , y_pred , target_names = class_names )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 C : \\ Users \\ Erik \\ Anaconda3 \\ lib \\ site - packages \\ sklearn \\ metrics \\ classification . py : 1143 : UndefinedMetricWarning : Precision and F - score are ill - defined and being set to 0 . 0 in labels with no predicted samples . 'precision' , 'predicted' , average , warn_for ) precision recall f1 - score support Encroached Shrub 0 . 00 0 . 00 0 . 00 375820 Loamy Bottom 0 . 00 0 . 00 0 . 00 85357 P - J 0 . 62 0 . 89 0 . 73 2877442 Perennial Shrub 0 . 40 0 . 19 0 . 26 1388305 Wet / Salt Meadow 0 . 02 0 . 01 0 . 02 28755 micro avg 0 . 59 0 . 59 0 . 59 4755679 macro avg 0 . 21 0 . 22 0 . 20 4755679 weighted avg 0 . 49 0 . 59 0 . 52 4755679 \u200b Predicting on the Image \u00b6 With our classifier fit, we can now proceed by trying to classify the entire image. 1 2 3 4 5 6 7 8 9 range_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\range_clip_utm.tif' with rasterio . open ( range_path ) as src : profile = src . profile # the src profile will be used to save the output later img = src . read () # Take our full iamge and reshape into long 2d array (nrow * ncol, nband) for classification print ( img . shape ) reshaped_img = reshape_as_image ( img ) print ( reshaped_img . shape ) 1 2 ( 6 , 9412 , 7341 ) ( 9412 , 7341 , 6 ) Now we can predict for each pixel in our image. 1 2 3 4 class_prediction = gnb . predict ( reshaped_img . reshape ( - 1 , 6 )) # Reshape our classification map back into a 2d matrix so we can visualize it class_prediction = class_prediction . reshape ( reshaped_img [:, :, 0 ] . shape ) Because we used labels as strings we will want to convert them to numpy array with integers using the helper function we made earlier. 1 2 3 # This function converts the class prediction to ints from strings because it was originally created as a string # See template notebook for more class_prediction = str_class_to_int ( class_prediction ) Visualize the Results \u00b6 First we'll make a colormap so we can adjust the colors of each class to more logical colors. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def color_stretch ( image , index ): colors = image [:, :, index ] . astype ( np . float64 ) for b in range ( colors . shape [ 2 ]): colors [:, :, b ] = rasterio . plot . adjust_band ( colors [:, :, b ]) return colors # find the highest pixel value in the prediction image n = int ( np . max ( class_prediction )) # next setup a colormap for our map colors = dict (( ( 1 , ( 34 , 139 , 34 , 255 )), # Forest Green - PJ ( 2 , ( 139 , 69 , 19 , 255 )), # Brown - Perennial Shrub ( 3 , ( 48 , 156 , 214 , 255 )), # Blue - Encroached Shrub ( 4 , ( 244 , 164 , 96 , 255 )), # Tan - Loamy Bottom ( 5 , ( 206 , 224 , 196 , 255 )), # Lime - Grass Meadow )) # Put 0 - 255 as float 0 - 1 for k in colors : v = colors [ k ] _v = [ _v / 255.0 for _v in v ] colors [ k ] = _v index_colors = [ colors [ key ] if key in colors else ( 255 , 255 , 255 , 0 ) for key in range ( 0 , n + 1 )] cmap = plt . matplotlib . colors . ListedColormap ( index_colors , 'Classification' , n + 1 ) 1 show ( class_prediction ) 1 < matplotlib . axes . _subplots . AxesSubplot at 0 xbeb7d2278 > 1 2 3 4 5 6 7 8 9 with rasterio . Env (): profile . update ( dtype = rasterio . uint8 , count = 1 , compress = 'lzw' ) with rasterio . open ( r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\example.tif' , 'w' , ** profile ) as dst : dst . write ( class_prediction . astype ( rasterio . uint8 ), 1 ) Improving our Model Accuracy \u00b6 In the following sections (TBD), we'll improve upon our model, evaluate accuracy, and explore different classifiers * Normalize values * Split into training and testing data * Balance classes in training data * Introduce additional features (elevation, precip, soil temperature, aspect, moving window stats, imagery) * Explore different classifiers * Feature Importance * Parameter tuning * Correlations Evaluation Accuracy 1","title":"Machine Learning Tutorial"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#machine-learning-tutorial","text":"This tutorial is intended to illustrate a typical workflow for machine learning to solve a land use and land cover classification problem. Land Use and Land Cover classifications are used to identify the dominant land cover or land use type in an area. We use the Naive Bayes and Random Forest classifiers, as implemented within scikit-learn library. This tutorial borrows heavily from the very helpful tutorial developed by Chris Holden and updated by Patrick Gray. Also used: rasterio , geopandas , numpy , pandas , shapely , and matplotlib .","title":"Machine Learning Tutorial"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#the-challenge","text":"Our client required a rapid approach for evaluating the benefits of conservation projects to Mule Deer. We proposed using Ecological State and Transition Models (STMs) as the basis for the evaluation. The NRCS has developed STMs for the dominant ecological sites within the region, however only a subset of the region was mapped. We use vegetation data (provided by the Rangelands App ) and other environmental variables to predict STM for the unmapped areas of the range.","title":"The Challenge"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#import-statements","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import rasterio from rasterio.mask import mask from rasterio.plot import show from rasterio.plot import show_hist from rasterio.windows import Window from rasterio.plot import reshape_as_raster , reshape_as_image import geopandas as gpd import numpy as np from shapely.geometry import mapping from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn import preprocessing from sklearn.pipeline import make_pipeline from sklearn.utils import resample import matplotlib.pyplot as plt # ipywidgets is used to create a progress bar from ipywidgets import IntProgress from IPython.display import display 1 % matplotlib inline","title":"Import Statements"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#import-data","text":"We'll be using supervised classification techniques. We'll need both labels and predictor features to train the models. Our labels will come from the mapped STM derived from NRCS data. To begin, we'll use vegetation data as predictors, including cover estimates for trees, shrubs, perennial grasses and forbs, and bare ground. 1 2 3 4 5 # CHANGE TO VEG_COVER_PATH # Read in features and training data train_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\train_clip_utm.tif' data = rasterio . open ( train_path ) data . crs # Check the projection, all features must share a projection 1 CRS . from_dict ( init = 'epsg:26913' ) 1 2 3 labels_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\labels.shp' labels = gpd . read_file ( labels_path ) labels . crs 1 { 'init' : 'epsg:26913' } 1 len ( labels ) 1 6136 We have 6,136 labeled polygons, represented within a geopandas dataframe, while our predictor features are represented in a rasterio raster. Both share the same projection.","title":"Import Data"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#explore-data-and-preparation-steps","text":"To train the classifiers, we'll need to associate our vector data (labels as polygons) with our raster pixels (predictor features). We'll accomplish this with the rasterio mask function. The mask function will essentially clip (or mask) our raster with each polygon. First, we'll want to extract the geometry of each feature in the labels shapefile to GeoJSON format. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # this generates a list of shapely geometries geoms = labels . geometry . values # let's grab a single shapely geometry to check geometry = geoms [ 0 ] print ( \"This is a shapely polygon\" ) print ( type ( geometry )) print ( geometry ) # transform to GeoJSON format (note 'mapping' is in the shapely namespace) feature = [ mapping ( geometry )] # can also do this using polygon.__geo_interface__ print ( \"This is the same polygon in GeoJSON format\" ) print ( type ( feature )) print ( feature ) 1 2 3 4 5 6 This is a shapely polygon < class 'shapely.geometry.polygon.Polygon' > POLYGON (( 155090 . 8039999995 4317864 . 1029 , 155095 . 9642000003 4317847 . 2246 , 155096 . 9452999998 4317767 . 406400001 , 155079 . 9972999999 4317764 . 4924 , 155013 . 6201999998 4317772 . 8774 , 154946 . 0262000002 4317762 . 761399999 , 154906 . 4253000002 4317765 . 351500001 , 154878 . 2922999999 4317781 . 742900001 , 154874 . 5252 4317788 . 509099999 , 154877 . 2857999997 4317829 . 973099999 , 154890 . 6249000002 4317873 . 230799999 , 154892 . 2852999996 4317898 . 202400001 , 154873 . 1852000002 4318020 . 8462 , 154866 . 6454999996 4318032 . 8059 , 154861 . 3509 4318094 . 8554 , 154880 . 1355999997 4318314 . 824999999 , 154876 . 2439000001 4318350 . 6874 , 154879 . 5504999999 4318400 . 631100001 , 154884 . 4243000001 4318410 . 842 , 154890 . 0175999999 4318527 . 3519 , 154897 . 2582 4318573 . 0209 , 154890 . 3213 4318594 . 527100001 , 154894 . 7226999998 4318598 . 269300001 , 154913 . 1912000002 4318593 . 0416 , 154946 . 1224999996 4318553 . 260600001 , 154952 . 6897 4318463 . 052999999 , 154967 . 4232000001 4318401 . 3926 , 154951 . 2766000004 4318220 . 3751 , 154975 . 3370000003 4318141 . 542099999 , 155019 . 5067999996 4318081 . 9695 , 155041 . 7379000001 4318038 . 8718 , 155041 . 0279000001 4317965 . 691199999 , 155054 . 3926999997 4317914 . 6457 , 155080 . 8075000001 4317871 . 2906 , 155090 . 8039999995 4317864 . 1029 )) This is the same polygon in GeoJSON format < class 'list' > [ { 'type' : 'Polygon' , 'coordinates' : ((( 155090 . 80399999954 , 4317864 . 1029 ), ( 155095 . 96420000028 , 4317847 . 2246 ), ( 155096 . 9452999998 , 4317767 . 406400001 ), ( 155079 . 99729999993 , 4317764 . 4924 ), ( 155013 . 62019999977 , 4317772 . 8774 ), ( 154946 . 0262000002 , 4317762 . 761399999 ), ( 154906 . 42530000024 , 4317765 . 351500001 ), ( 154878 . 29229999986 , 4317781 . 742900001 ), ( 154874 . 52520000003 , 4317788 . 509099999 ), ( 154877 . 28579999972 , 4317829 . 973099999 ), ( 154890 . 62490000017 , 4317873 . 230799999 ), ( 154892 . 28529999964 , 4317898 . 202400001 ), ( 154873 . 18520000018 , 4318020 . 8462000005 ), ( 154866 . 64549999963 , 4318032 . 8059 ), ( 154861 . 35089999996 , 4318094 . 8554 ), ( 154880 . 1355999997 , 4318314 . 824999999 ), ( 154876 . 24390000012 , 4318350 . 6874 ), ( 154879 . 5504999999 , 4318400 . 631100001 ), ( 154884 . 42430000007 , 4318410 . 842 ), ( 154890 . 0175999999 , 4318527 . 3519 ), ( 154897 . 25820000004 , 4318573 . 0209 ), ( 154890 . 32129999995 , 4318594 . 5271000005 ), ( 154894 . 7226999998 , 4318598 . 269300001 ), ( 154913 . 19120000023 , 4318593 . 0416 ), ( 154946 . 1224999996 , 4318553 . 260600001 ), ( 154952 . 6897 , 4318463 . 052999999 ), ( 154967 . 42320000008 , 4318401 . 3926 ), ( 154951 . 27660000045 , 4318220 . 3751 ), ( 154975 . 3370000003 , 4318141 . 542099999 ), ( 155019 . 50679999962 , 4318081 . 9695 ), ( 155041 . 73790000007 , 4318038 . 8718 ), ( 155041 . 0279000001 , 4317965 . 691199999 ), ( 155054 . 39269999973 , 4317914 . 6457 ), ( 155080 . 8075000001 , 4317871 . 2906 ), ( 155090 . 80399999954 , 4317864 . 1029 )),) } ] Now let's extract the raster values within each polygon using the rasterio mask() function . 1 out_image , out_transform = mask ( data , feature , crop = True ) 1 out_image . shape 1 ( 6 , 32 , 10 ) 1 show ( out_image [ 0 ]) 1 < matplotlib . axes . _subplots . AxesSubplot at 0 xbea5c5b70 > As you can see, the features raster was clipped to a single polygon. There are 6 bands and 32x10 pixels. We'll repeat this process for all 6,136 polygons to build our dataset. We'll also need to clean this raster up a bit before we use it in training. We'll explore all of this next. But first, we'll be doing a lot of memory intensive work so we'll close the dataset for now. 1 data . close ()","title":"Explore Data and Preparation Steps"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#build-the-training-data-for-sckit-learn","text":"We'll repeat the above process for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels. Note that the column 'MuleDeer_1' in the labels geodataframe has the label we're after. TODO: change to column with label instead of numeric. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 %% time # Lets create a progress bar as this step can take some time p_bar = IntProgress ( min = 0 , max = len ( geoms ), description = 'Processing' ) display ( p_bar ) # Set up arrays X = np . array ([], dtype = np . int8 ) . reshape ( 0 , 6 ) # pixels for training y = np . array ([], dtype = np . string_ ) # labels for training # extract the raster values within the polygon with rasterio . open ( train_path ) as src : band_count = src . count for index , geom in enumerate ( geoms ): # Note geoms was created above feature = [ mapping ( geom )] # the mask function returns an array of the raster pixels within this feature out_image , out_transform = mask ( src , feature , crop = True ) # eliminate all the pixels with 0 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image [:, ~ np . all ( out_image == 0 , axis = 0 )] # eliminate all the pixels with 255 values for all bands - AKA not actually part of the shapefile out_image_trimmed = out_image_trimmed [:, ~ np . all ( out_image_trimmed == 255 , axis = 0 )] # reshape the array to [pixel count, bands] out_image_reshaped = out_image_trimmed . reshape ( - 1 , band_count ) # append the labels to the y array y = np . append ( y , [ labels [ \"MuleDeer_1\" ][ index ]] * out_image_reshaped . shape [ 0 ]) # ??? # stack the pixels onto the pixel array X = np . vstack (( X , out_image_reshaped )) # increment the progress bar p_bar . value += 1 1 2 3 4 IntProgress ( value = 0 , description = 'Processing' , max = 6136 ) Wall time : 25 min 33 s","title":"Build the Training Data for sckit-learn"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#save-the-output","text":"We'll save the output as a numpy array to avoid the long process of rebuilding the features in the future. This will also allow us to share this analysis with others without them needing access to the input data. If this were not a tutorial, I might have ended the notebook here and started a new one for the remainder of the analysis. I've commented out the load statements below, uncomment to load in the saved features and labels. 1 2 3 # Save features and labels np . save ( 'E:/lulc-features.npy' , X ) np . save ( 'E:/lulc-labels.npy' , y ) 1 2 3 # Load in data X = np . load ( 'E:/lulc-features.npy' ) # fill in path y = np . load ( 'E:/lulc-labels.npy' )","title":"Save the output"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#splitting-the-data-for-testing","text":"In order to evaluate the accuracy of our model, we'll reserve a subset of the data for testing. The train_test_split function allows us to quickly and randomly subset our data for this purpose. 1 2 # split out 30% of data for testing. Random state set for reproducibility. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 )","title":"Splitting the data for testing"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#pairing-y-with-x","text":"Now that we have the image we want to classify (X_train) and the land cover labels (y_train), let's check to make sure they match in size so we can feed them to our models. 1 2 3 4 5 6 7 st_names = np . unique ( y_train ) print ( 'The training data include {n} classes: {classes} \\n ' . format ( n = st_names . size , classes = st_names )) # We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels print ( 'Our X matrix is sized: {sz}' . format ( sz = X_train . shape )) print ( 'Our y array is sized: {sz}' . format ( sz = y_train . shape )) 1 2 3 4 5 The training data include 5 classes : [ ' Encroached Shrub ' ' Loamy Bottom ' ' P-J ' ' Perennial Shrub ' ' Wet/Salt Meadow ' ] Our X matrix is sized : ( 4755679 , 6 ) Our y array is sized : ( 4755679 , ) That looks good. We have 5 classes (i.e., our STM names); 6 predictor features (i.e., the 6 bands in our X matrix, now flattened; and both the X and y array are the same length. We'll treat these vegetation cover values as spectral signatures, and plot each to make sure they're actually separable since all we're going by in this classification is pixel values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 fig , ax = plt . subplots ( 1 , 3 , figsize = [ 20 , 8 ]) # bands are numbered 1 through 6 following GDAL convention band_count = np . arange ( 1 , 7 ) classes = np . unique ( y_train ) for class_type in classes : band_intensity = np . mean ( X_train [ y_train == class_type , :], axis = 0 ) ax [ 0 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 1 ] . plot ( band_count , band_intensity , label = class_type ) ax [ 2 ] . plot ( band_count , band_intensity , label = class_type ) #plot them as lines # Add some axis labels # Add some axis labels ax [ 0 ] . set_xlabel ( 'Band #' ) ax [ 0 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_ylabel ( 'Reflectance Value' ) ax [ 1 ] . set_xlabel ( 'Band #' ) ax [ 2 ] . set_ylabel ( 'Reflectance Value' ) ax [ 2 ] . set_xlabel ( 'Band #' ) ax [ 1 ] . legend ( loc = \"upper right\" ) # Add a title ax [ 0 ] . set_title ( 'Band Intensities Full Overview' ) ax [ 1 ] . set_title ( 'Band Intensities Lower Ref Subset' ) ax [ 2 ] . set_title ( 'Band Intensities Higher Ref Subset' ) plt . show () Looks like each class will be easily separable. This will be a helper function to convert class labels into indices so we're predicting to integers instead of strings. TODO: switch labels and numbers 1 2 3 4 5 6 7 def str_class_to_int ( class_array ): class_array [ class_array == 'P-J' ] = 1 class_array [ class_array == 'Perennial Shrub' ] = 2 class_array [ class_array == 'Encroached Shrub' ] = 3 class_array [ class_array == 'Loamy Bottom' ] = 4 class_array [ class_array == 'Wet/Salt Meadow' ] = 5 return ( class_array . astype ( int ))","title":"Pairing y with X"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#training-the-classifier","text":"Now that we have our X matrix of feature inputs (the vegetation cover bands) and our y array (the labels), we can train our model. Visit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn . 1 2 gnb = GaussianNB () gnb . fit ( X_train , y_train ) 1 GaussianNB ( priors = None , var_smoothing = 1 e - 09 ) It's that simple to train a classifier in sckit-learn . The hard part is often validation and interpretation.","title":"Training the Classifier"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#validation","text":"To see how well our classifier worked, we could use the test data we partioned earlier. However, we may want to adjust the model if our results are not as accurate as we'd like. This could lead to overfitting by 'leaking' information from the test set into our training of the model. Overfitting will hurt the performance of our model on predicting novel data, and will lead to inflated accuracy metrics. So how do we evaluate our model at this stage? Cross-validation . There are a few options for cross-validation, but for our purposes k-fold cross validation will work. 1 2 # 5-fold cross validation scores = cross_val_score ( gnb , X_train , y_train , cv = 5 ) scores stores the results of computing the score 5 consecutive times (with different splits each time) 1 scores 1 array ([ 0 . 59125447 , 0 . 59230899 , 0 . 59061017 , 0 . 59067851 , 0 . 59257414 ]) The mean score and the 95% confidence interval of the score estimate are hence given by: 1 print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.59 (+/- 0.002 )","title":"Validation"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#improving-model-accuracy","text":"","title":"Improving Model Accuracy"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#standardizing-values","text":"The accuracy is not as good as we'd like. How can we improve the accuracy of the model? One option is to standardize the data so that each of the features are similar in magnitude. This avoids some higher values from overwhelming lower values. It is often necessary to complete this step, depending on the model used. sklearn provides a preprocessing module that facilitate this scaling. 1 2 3 # Hide warnings for converting ints to floats (or save X, y as float64 type) import warnings warnings . filterwarnings ( \"ignore\" ) 1 2 gnb = make_pipeline ( preprocessing . StandardScaler (), GaussianNB ()) cross_val_score ( gnb , X_train , y_train , cv = 5 ) 1 array ([ 0 . 59125447 , 0 . 59230899 , 0 . 59061017 , 0 . 59067851 , 0 . 59257414 ]) The accuracy is exactly the same! This is because the Gaussian Naive Bayes is robust to scaling. In essence Gaussian Naive Bayes performs standardization internally .","title":"Standardizing Values"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#balancing-classes","text":"Unbalanced classes can also impact the accuracy of a model. Imagine you have data on a rare disease that only 0.01% of people have. A simple model that predicts everyone does not have the disease would be right 99.99% of the time! To get a model that can actually predict when people do have the disease, you'll need to address the imbalanced classes. Again, this is an issue depending on the model used. We'll try a different type of model later in the tutorial that is more robust to imbalanced data (Random Forests). To balance the data, you can either down-sample the over-represented classes or up-sample the under-represented classes.","title":"Balancing Classes"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#how-unbalanced-are-the-classes-now","text":"Let's quickly plot the number of each label so we know how unbalanced the data are. 1 2 3 import pandas as pd df_plot = pd . DataFrame ( y_train ) df_plot [ 0 ] . value_counts () . plot ( kind = 'bar' ) 1 < matplotlib . axes . _subplots . AxesSubplot at 0 x22899b1eda0 > We'll try up-sampling first so we don't reduce the number of data points too much. How much to upsample? We'll resample each of the less represented features with replacement to get the number of features contained in the most represented class. 1 df_plot [ 0 ] . value_counts () 1 2 3 4 5 6 P - J 2877442 Perennial Shrub 1388305 Encroached Shrub 375820 Loamy Bottom 85357 Wet / Salt Meadow 28755 Name : 0 , dtype : int64 This will be easier with a pandas DataFrame, so let's convert our data to a DataFrame. 1 2 3 4 5 6 df = pd . concat ( [ pd . DataFrame ( y_train , columns = [ 'label' ]), pd . DataFrame ( X_train )], axis = 1 ) . set_index ( 'label' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 label P-J 18 19 12 12 11 13 P-J 9 9 7 8 7 13 P-J 18 24 21 22 20 20 Perennial Shrub 14 18 16 12 14 13 P-J 4 5 9 9 7 7 1 2 3 4 df_shrub = resample ( df . loc [ 'Perennial Shrub' ], replace = True , n_samples = df . index . value_counts () . max ()) df_shrub . shape [ 0 ] 1 2877442 Now we have the same number of Perennial Shrub classes as P-J classes, our dominant class type. Let's repeat for the remaining features. We'll loop through our labels and concatenate the results to the most represented class so that this step is robust to changes in which features we explore. 1 df . index . value_counts () . max () 1 2877442 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Start by subsetting the most represented class max_class = df . index . value_counts () . idxmax () df_upsampled = df . loc [ max_class ] labels = list ( df . index . unique ()) labels . remove ( max_class ) # For each label, resample the features to balance classes and append # to the empty dataframe for label in labels : df_temp = resample ( df . loc [ label ], replace = True , n_samples = df . index . value_counts () . max () ) df_upsampled = pd . concat ([ df_upsampled , df_temp ], axis = 0 ) df_upsampled . index . value_counts () 1 2 3 4 5 6 P - J 2877442 Encroached Shrub 2877442 Perennial Shrub 2877442 Wet / Salt Meadow 2877442 Loamy Bottom 2877442 Name : label , dtype : int64 Now, we can split out our X and y data again and re-train the model on the more balanced classes. 1 2 X_train_upsampled = df_upsampled . reset_index () . drop ( 'label' , axis = 1 ) y_train_upsampled = df_upsampled . index . values 1 2 3 4 # 5-fold cross validation scores = cross_val_score ( gnb , X_train_upsampled , y_train_upsampled , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 Accuracy : 0.24 (+/- 0.001 ) Our accuracy took a bit of a nosedive, as now we're probably over-representing those less common classes on the landscape. For the purposes of our model, it may actually be better to simply ignore those more rare classes instead of over-representing them. We could try downsampling instead, but it will likely not help our accuracy very much. Instead, let's try a different model, Random Forests.","title":"How unbalanced are the classes now?"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#alternative-model-random-forests","text":"Random Forests is robust to unscaled and unbalanced data, making it a good option for this classification problem out-of-the-box. It's not a bad idea to scale the data as we did earlier, but since our data is well scaled (percent cover data from 0 - 100%), we'll skip this step. Acknowledgements to this tutorial used in developing this section. 1 2 3 4 5 6 7 8 %% time from sklearn.ensemble import RandomForestClassifier # Initialize our model with 10 estimators to limit processing time rfc = RandomForestClassifier ( n_estimators = 10 , random_state = 0 ) # 5-fold cross validation scores = cross_val_score ( rfc , X_train , y_train , cv = 5 ) print ( \"Accuracy: %0.2f (+/- %0.3f )\" % ( scores . mean (), scores . std () * 2 )) 1 2 Accuracy : 0.58 (+/- 0.001 ) Wall time : 3 h 13 min 13 s Our accuracy (57%) is slightly less than our accuracy with the unbalanced classes using Naive Bayes (59%) but comparable.","title":"Alternative Model: Random Forests"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#confusion-matrix","text":"We can visualize how well we're classifying each class (and where the model is getting confused) using a confusion matrix . The code for the confusion matrix is copied from the linked documentation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels def plot_confusion_matrix ( y_true , y_pred , classes , normalize = False , title = None , cmap = plt . cm . Blues ): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if not title : if normalize : title = 'Normalized confusion matrix' else : title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix ( y_true , y_pred ) # Only use the labels that appear in the data classes = unique_labels ( y_true , y_pred ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] print ( \"Normalized confusion matrix\" ) else : print ( 'Confusion matrix, without normalization' ) print ( cm ) fig , ax = plt . subplots ( figsize = ( 15 , 10 )) im = ax . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) ax . figure . colorbar ( im , ax = ax ) # We want to show all ticks... ax . set ( xticks = np . arange ( cm . shape [ 1 ]), yticks = np . arange ( cm . shape [ 0 ]), # ... and label them with the respective list entries xticklabels = classes , yticklabels = classes , title = title , ylabel = 'True label' , xlabel = 'Predicted label' ) # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 45 , ha = \"right\" , rotation_mode = \"anchor\" ) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm . max () / 2. for i in range ( cm . shape [ 0 ]): for j in range ( cm . shape [ 1 ]): ax . text ( j , i , format ( cm [ i , j ], fmt ), ha = \"center\" , va = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) fig . tight_layout () return ax 1 2 # Use the model to predict on the X_train dataset y_pred = gnb . fit ( X_train , y_train ) . predict ( X_train ) 1 2 class_names = unique_labels ( y_train , y_pred ) np . set_printoptions ( precision = 2 ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[ 0 . 00 e + 00 0 . 00 e + 00 8 . 77 e - 01 1 . 22 e - 01 2 . 90 e - 04 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 13 e - 01 1 . 84 e - 01 2 . 91 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 86 e - 01 1 . 14 e - 01 8 . 17 e - 05 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 01 e - 01 1 . 89 e - 01 9 . 71 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 7 . 46 e - 01 2 . 42 e - 01 1 . 17 e - 02 ]] The confusion matrix indicates that the model is catogorizing most pixels as P-J and some as Perennial Shrub, with just a few Wet Meadow predictions. It ignores Encroached Shrub and Loamy Bottom. Focusing on the final row of the matrix, you can see that the Wet/Salt Meadow is being categorized as P-J 75% of the time, Perennial Shrub 24% of the time, and its correct label only 1% of the time. 1 y_pred_rfc = rfc . fit ( X_train , y_pred ) . predict ( X_train ) 1 2 3 4 5 # Plot normalized confusion matrix plot_confusion_matrix ( y_train , y_pred_rfc , classes = class_names , normalize = True , title = 'Normalized confusion matrix' ) plt . show () 1 2 3 4 5 6 Normalized confusion matrix [[ 0 . 00 e + 00 0 . 00 e + 00 8 . 77 e - 01 1 . 22 e - 01 2 . 87 e - 04 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 13 e - 01 1 . 84 e - 01 2 . 85 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 86 e - 01 1 . 14 e - 01 8 . 06 e - 05 ] [ 0 . 00 e + 00 0 . 00 e + 00 8 . 01 e - 01 1 . 89 e - 01 9 . 69 e - 03 ] [ 0 . 00 e + 00 0 . 00 e + 00 7 . 46 e - 01 2 . 42 e - 01 1 . 17 e - 02 ]] Interestingly, the confusion matrix for both the Naive Bayes and Random Forests model is the same. This may signal that we've done about as well as we could with these features. We could incorporate additional features through the same process as we went through before, considering topography, soils, precipitation, and spatial measures using distances or moving windows. We should also carefully consider feature selection to optimize the bias-variance tradeoffs. Feature engingeering and feature selection is beyond the scope of this tutorial. A classification report will provide reportable metrics of model accuracy to allow us document the performance of the model. It reports precision, recall and the F1 score. Precision is the ability of the model to avoid false positives. Recall is the ability to identify the feature correctly (notice it is equivalent to where the diagonal axis of the confusion matrix). The F1 score is a harmonic mean of precision and recall. 1 2 from sklearn.metrics import classification_report print ( classification_report ( y_train , y_pred , target_names = class_names )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 C : \\ Users \\ Erik \\ Anaconda3 \\ lib \\ site - packages \\ sklearn \\ metrics \\ classification . py : 1143 : UndefinedMetricWarning : Precision and F - score are ill - defined and being set to 0 . 0 in labels with no predicted samples . 'precision' , 'predicted' , average , warn_for ) precision recall f1 - score support Encroached Shrub 0 . 00 0 . 00 0 . 00 375820 Loamy Bottom 0 . 00 0 . 00 0 . 00 85357 P - J 0 . 62 0 . 89 0 . 73 2877442 Perennial Shrub 0 . 40 0 . 19 0 . 26 1388305 Wet / Salt Meadow 0 . 02 0 . 01 0 . 02 28755 micro avg 0 . 59 0 . 59 0 . 59 4755679 macro avg 0 . 21 0 . 22 0 . 20 4755679 weighted avg 0 . 49 0 . 59 0 . 52 4755679 \u200b","title":"Confusion Matrix"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#predicting-on-the-image","text":"With our classifier fit, we can now proceed by trying to classify the entire image. 1 2 3 4 5 6 7 8 9 range_path = r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\range_clip_utm.tif' with rasterio . open ( range_path ) as src : profile = src . profile # the src profile will be used to save the output later img = src . read () # Take our full iamge and reshape into long 2d array (nrow * ncol, nband) for classification print ( img . shape ) reshaped_img = reshape_as_image ( img ) print ( reshaped_img . shape ) 1 2 ( 6 , 9412 , 7341 ) ( 9412 , 7341 , 6 ) Now we can predict for each pixel in our image. 1 2 3 4 class_prediction = gnb . predict ( reshaped_img . reshape ( - 1 , 6 )) # Reshape our classification map back into a 2d matrix so we can visualize it class_prediction = class_prediction . reshape ( reshaped_img [:, :, 0 ] . shape ) Because we used labels as strings we will want to convert them to numpy array with integers using the helper function we made earlier. 1 2 3 # This function converts the class prediction to ints from strings because it was originally created as a string # See template notebook for more class_prediction = str_class_to_int ( class_prediction )","title":"Predicting on the Image"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#visualize-the-results","text":"First we'll make a colormap so we can adjust the colors of each class to more logical colors. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def color_stretch ( image , index ): colors = image [:, :, index ] . astype ( np . float64 ) for b in range ( colors . shape [ 2 ]): colors [:, :, b ] = rasterio . plot . adjust_band ( colors [:, :, b ]) return colors # find the highest pixel value in the prediction image n = int ( np . max ( class_prediction )) # next setup a colormap for our map colors = dict (( ( 1 , ( 34 , 139 , 34 , 255 )), # Forest Green - PJ ( 2 , ( 139 , 69 , 19 , 255 )), # Brown - Perennial Shrub ( 3 , ( 48 , 156 , 214 , 255 )), # Blue - Encroached Shrub ( 4 , ( 244 , 164 , 96 , 255 )), # Tan - Loamy Bottom ( 5 , ( 206 , 224 , 196 , 255 )), # Lime - Grass Meadow )) # Put 0 - 255 as float 0 - 1 for k in colors : v = colors [ k ] _v = [ _v / 255.0 for _v in v ] colors [ k ] = _v index_colors = [ colors [ key ] if key in colors else ( 255 , 255 , 255 , 0 ) for key in range ( 0 , n + 1 )] cmap = plt . matplotlib . colors . ListedColormap ( index_colors , 'Classification' , n + 1 ) 1 show ( class_prediction ) 1 < matplotlib . axes . _subplots . AxesSubplot at 0 xbeb7d2278 > 1 2 3 4 5 6 7 8 9 with rasterio . Env (): profile . update ( dtype = rasterio . uint8 , count = 1 , compress = 'lzw' ) with rasterio . open ( r 'D:\\ArcGIS\\Colorado\\General\\Rangelands_App\\for-analysis\\example.tif' , 'w' , ** profile ) as dst : dst . write ( class_prediction . astype ( rasterio . uint8 ), 1 )","title":"Visualize the Results"},{"location":"portfolio/machine-learning-tutorial/machine-learning-tutorial/#improving-our-model-accuracy","text":"In the following sections (TBD), we'll improve upon our model, evaluate accuracy, and explore different classifiers * Normalize values * Split into training and testing data * Balance classes in training data * Introduce additional features (elevation, precip, soil temperature, aspect, moving window stats, imagery) * Explore different classifiers * Feature Importance * Parameter tuning * Correlations Evaluation Accuracy 1","title":"Improving our Model Accuracy"},{"location":"project-organization/project-planning/","text":"Getting Started \u00b6 Before beginning any project, start by thinking it through. What technologies are required? What existing frameworks or packages are available? How will the project be deployed? Who are the users? Is the project worth the effort? How long will it take? How will the project be supported? These are the high-level questions that will shape the direction and scope of the project. Use the EI Data Driven Product - Product Definition to get started. Next, think about the general approach. What will the architecture of the project be? Which specific packages will be used? Should you set up a virtual environment? What will the user interface look like? How will the backend be managed? What testing approach will be used? What conventions will be used for file and folder naming, code style, etc.? Draft a Tool Specifications document if warranted (i.e., for large, billable projects). See the draft Tool Specifications outline in the EI Data Driven Product - Product Definition. You should have a clear plan in writing before starting with the first line of code. Example Project Plan \u00b6 Here's an example project plan for this project: Solution Description \u00b6 A web-based wiki for capturing and organizing information important to developing metrics products with a focus on performance-driven conservation programs. Goal & Objectives \u00b6 Goal \u00b6 Create a single source for documenting and sharing EI's approach to metrics product development and best practices for current and future staff. Objectives \u00b6 Compile all existing resources for data product development and create single platform for accumulating new resources. Present information in user-friendly format that balances instructional content with requirements for illustrating in-line code. Require staff focused on product development to work with technologies that will be used in deploying EI data products. Users \u00b6 Primary \u00b6 EI's metrics staff and other technical staff (e.g., Erik, Kristen, Maso) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Metrics staff have developed standard processes and must learn new technologies frequently to deliver metrics products. Until now, this information has been scattered in multiple locations or lost entirely. New information that is learned or \u200b Value Propositions: This product will provide a common location for storing information and allow metrics staff to quickly access said information and re-familiarize themselves with important information quickly. Secondary \u00b6 Non-technical staff working with metrics staff to develop a data product (e.g., Kelsey) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Non-technical staff are unaware of the capabilities available to them from metrics service line staff or are uncertain as to how to utilize those capabilities for their projects, both internal and external. \u200b Value Propositions: This product will include overviews of available services and technologies, as well as project examples, that non-technical staff can peruse at their leisure. Metrics staff can also use the wiki to introduce options and explain our engagement process. Use Cases \u00b6 User Stories \u00b6 As the metrics service line lead, I want to ______ so I can ______: Direct staff to a common resource so I can limit the time needed to train new staff on commonly used technologies and best practices. Encourage metrics staff to work with technologies like git and the command line so I can better integrate them into our workflows. As a metrics/technical staff, I want to ______ so I can ______: Quickly relearn previously used technologies so I can employ them in new projects with minimal spin up time. Communicate to non-metrics staff the options they have for a specific product, visualization or analysis so I can more quickly scope their projects. Capture and share new technologies, ideas, and other information as I discover it so I can reference it later and promote its use. As a non-metrics EI staff, I want to ______ so I can ______: Better understand the capabilities of EI staff to support on technology-based products so I can pitch them to clients or develop them for internal uses. Better understand the process for working with metrics service line staff so I can assess the feasibility of the product I'm considering. Product Sketch \u00b6 [Story Board, what did you learn from storyboarding?] Design Principles & Constraints \u00b6 (list 'em. Consider integration, hosting, support, adaptive management, deployment but don't get into details yet) Conditions of Satisfaction \u00b6 (How will we know this tool is successful?) Approach \u00b6 (Roles, Timeline, Resources required) As you can see, lots to be done! The folder order is roughly the prioritization for these pages. Thus, my approach will be to work through these pages in roughly this order. One objective for this project is to provide a place to store new information as it becomes available, so I'll create the above folders initially as markdown files of the same name within a 'tbd' folder where I can store links and other references as I come across them. I've also built some of the pages above in other formats (Evernote, Google Docs, Jupyter Notebooks, etc.) so I can now pull everything together into one place. Optional: Situation Model \u00b6 Optional: Results Chain \u00b6 \u00b6 (Move to Specs) \u00b6 Technologies: \u00b6 MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE Git - version control Github - Repository Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc. Architecture \u00b6 The MkDocs package will create the basic architecture when creating the project . After creating the project, a mkdocs.yml file will be created. A docs folder will also be created with an index.md file within it. The index.md file manages the site outline; the mkdocs.yml file manages the settings. I'll add a README.md file in the root folder that will show up on the Github repo page. Files and folders can be created within the docs folder to create the project pages. Here's the file structure proposed within the root folder; the folder structure will mirror the site outline: mkdocs.yml # describes how the site is organized, enables features README.md # instructions for accessing and publishing docs/ index.md # home page how-we-work/ consulting-process.md program-requirements.md conservation-design/ project-planning/ file-organization-and-naming.md project-planning.md specifications-outline.md skills-and-training.md git/ installing-git.md initializing-git.md using-git.md development/ virtual-environments.md IDEs.md data-science-workflow.md deployment/ deployment-overview.md jupyter.md heroku.md linux.md aws.md docker.md data-management/ database-overview.md data-science/ workflow-overview.md data-exploration.md data-analysis.md data-visualization.md (e.g., inline exploratory) spatial-analysis/ earth-observation.md google-earth-engine.md gdal.md arcpy.md land-use-land-cover.md dashboards/ visualization/ (e.g. report quality) packages/ dash pandas seaborn folium sqlite rasterio Other Considerations \u00b6 Keep track of concerns and other considerations as you go and revisit the specifications periodically to ensure the best approach has been taken. How will this tech stack allow for illustrating using code? Can code be run within the deployment environment, or will static code blocks and outputs be needed? How often should links to a Jupyter Notebook, for example, be used as opposed to illustrating static code? Is there a good way to surface content for non-technical staff that are interested in these services or are asked by the metrics staff to, for example, complete a product definition? Or should the users be limited to technical staff only?","title":"Project Planning"},{"location":"project-organization/project-planning/#getting-started","text":"Before beginning any project, start by thinking it through. What technologies are required? What existing frameworks or packages are available? How will the project be deployed? Who are the users? Is the project worth the effort? How long will it take? How will the project be supported? These are the high-level questions that will shape the direction and scope of the project. Use the EI Data Driven Product - Product Definition to get started. Next, think about the general approach. What will the architecture of the project be? Which specific packages will be used? Should you set up a virtual environment? What will the user interface look like? How will the backend be managed? What testing approach will be used? What conventions will be used for file and folder naming, code style, etc.? Draft a Tool Specifications document if warranted (i.e., for large, billable projects). See the draft Tool Specifications outline in the EI Data Driven Product - Product Definition. You should have a clear plan in writing before starting with the first line of code.","title":"Getting Started"},{"location":"project-organization/project-planning/#example-project-plan","text":"Here's an example project plan for this project:","title":"Example Project Plan"},{"location":"project-organization/project-planning/#solution-description","text":"A web-based wiki for capturing and organizing information important to developing metrics products with a focus on performance-driven conservation programs.","title":"Solution Description"},{"location":"project-organization/project-planning/#goal-objectives","text":"","title":"Goal &amp; Objectives"},{"location":"project-organization/project-planning/#goal","text":"Create a single source for documenting and sharing EI's approach to metrics product development and best practices for current and future staff.","title":"Goal"},{"location":"project-organization/project-planning/#objectives","text":"Compile all existing resources for data product development and create single platform for accumulating new resources. Present information in user-friendly format that balances instructional content with requirements for illustrating in-line code. Require staff focused on product development to work with technologies that will be used in deploying EI data products.","title":"Objectives"},{"location":"project-organization/project-planning/#users","text":"","title":"Users"},{"location":"project-organization/project-planning/#primary","text":"EI's metrics staff and other technical staff (e.g., Erik, Kristen, Maso) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Metrics staff have developed standard processes and must learn new technologies frequently to deliver metrics products. Until now, this information has been scattered in multiple locations or lost entirely. New information that is learned or \u200b Value Propositions: This product will provide a common location for storing information and allow metrics staff to quickly access said information and re-familiarize themselves with important information quickly.","title":"Primary"},{"location":"project-organization/project-planning/#secondary","text":"Non-technical staff working with metrics staff to develop a data product (e.g., Kelsey) Problem Scenarios Alternatives Value Propositions \u200b Problem Scenarios: Non-technical staff are unaware of the capabilities available to them from metrics service line staff or are uncertain as to how to utilize those capabilities for their projects, both internal and external. \u200b Value Propositions: This product will include overviews of available services and technologies, as well as project examples, that non-technical staff can peruse at their leisure. Metrics staff can also use the wiki to introduce options and explain our engagement process.","title":"Secondary"},{"location":"project-organization/project-planning/#use-cases","text":"","title":"Use Cases"},{"location":"project-organization/project-planning/#user-stories","text":"As the metrics service line lead, I want to ______ so I can ______: Direct staff to a common resource so I can limit the time needed to train new staff on commonly used technologies and best practices. Encourage metrics staff to work with technologies like git and the command line so I can better integrate them into our workflows. As a metrics/technical staff, I want to ______ so I can ______: Quickly relearn previously used technologies so I can employ them in new projects with minimal spin up time. Communicate to non-metrics staff the options they have for a specific product, visualization or analysis so I can more quickly scope their projects. Capture and share new technologies, ideas, and other information as I discover it so I can reference it later and promote its use. As a non-metrics EI staff, I want to ______ so I can ______: Better understand the capabilities of EI staff to support on technology-based products so I can pitch them to clients or develop them for internal uses. Better understand the process for working with metrics service line staff so I can assess the feasibility of the product I'm considering.","title":"User Stories"},{"location":"project-organization/project-planning/#product-sketch","text":"[Story Board, what did you learn from storyboarding?]","title":"Product Sketch"},{"location":"project-organization/project-planning/#design-principles-constraints","text":"(list 'em. Consider integration, hosting, support, adaptive management, deployment but don't get into details yet)","title":"Design Principles &amp; Constraints"},{"location":"project-organization/project-planning/#conditions-of-satisfaction","text":"(How will we know this tool is successful?)","title":"Conditions of Satisfaction"},{"location":"project-organization/project-planning/#approach","text":"(Roles, Timeline, Resources required) As you can see, lots to be done! The folder order is roughly the prioritization for these pages. Thus, my approach will be to work through these pages in roughly this order. One objective for this project is to provide a place to store new information as it becomes available, so I'll create the above folders initially as markdown files of the same name within a 'tbd' folder where I can store links and other references as I come across them. I've also built some of the pages above in other formats (Evernote, Google Docs, Jupyter Notebooks, etc.) so I can now pull everything together into one place.","title":"Approach"},{"location":"project-organization/project-planning/#optional-situation-model","text":"","title":"Optional: Situation Model"},{"location":"project-organization/project-planning/#optional-results-chain","text":"","title":"Optional: Results Chain"},{"location":"project-organization/project-planning/#move-to-specs","text":"","title":"(Move to Specs)"},{"location":"project-organization/project-planning/#technologies","text":"MkDocs - static site generator that requires Markdown Markdown - markup text language Typora - Markdown editor VS Code - IDE Git - version control Github - Repository Github Project Pages - Deployment (gh-pages branch); see MkDocs deployment documentation . Screen2Gif - a screen recording app that saves outputs as gifs (for video instruction) YouTube - custom videos for instruction, etc.","title":"Technologies:"},{"location":"project-organization/project-planning/#architecture","text":"The MkDocs package will create the basic architecture when creating the project . After creating the project, a mkdocs.yml file will be created. A docs folder will also be created with an index.md file within it. The index.md file manages the site outline; the mkdocs.yml file manages the settings. I'll add a README.md file in the root folder that will show up on the Github repo page. Files and folders can be created within the docs folder to create the project pages. Here's the file structure proposed within the root folder; the folder structure will mirror the site outline: mkdocs.yml # describes how the site is organized, enables features README.md # instructions for accessing and publishing docs/ index.md # home page how-we-work/ consulting-process.md program-requirements.md conservation-design/ project-planning/ file-organization-and-naming.md project-planning.md specifications-outline.md skills-and-training.md git/ installing-git.md initializing-git.md using-git.md development/ virtual-environments.md IDEs.md data-science-workflow.md deployment/ deployment-overview.md jupyter.md heroku.md linux.md aws.md docker.md data-management/ database-overview.md data-science/ workflow-overview.md data-exploration.md data-analysis.md data-visualization.md (e.g., inline exploratory) spatial-analysis/ earth-observation.md google-earth-engine.md gdal.md arcpy.md land-use-land-cover.md dashboards/ visualization/ (e.g. report quality) packages/ dash pandas seaborn folium sqlite rasterio","title":"Architecture"},{"location":"project-organization/project-planning/#other-considerations","text":"Keep track of concerns and other considerations as you go and revisit the specifications periodically to ensure the best approach has been taken. How will this tech stack allow for illustrating using code? Can code be run within the deployment environment, or will static code blocks and outputs be needed? How often should links to a Jupyter Notebook, for example, be used as opposed to illustrating static code? Is there a good way to surface content for non-technical staff that are interested in these services or are asked by the metrics staff to, for example, complete a product definition? Or should the users be limited to technical staff only?","title":"Other Considerations"}]}